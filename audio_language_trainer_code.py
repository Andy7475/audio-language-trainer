# -*- coding: utf-8 -*-
"""swedish_project_ready.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18eQ6XpY0NmLCFpFwwDzK-AIy4L2xPnNT
"""

!pip install --upgrade google-cloud-aiplatform google-cloud-texttospeech pydub pysnooper anthropic --quiet

"""# Import

"""

from google.colab import auth
auth.authenticate_user()

#!gcloud auth login

project_id = 'swedish-course'
!gcloud projects describe 'swedish-course' --format='get(projectNumber)'
!gcloud config set project {project_id}

#project number is 307643465852
#my gmail is andylaing5@gmail.com

!gcloud auth application-default login

from google.cloud import translate_v2 as translate
from google.colab import files
import re
import vertexai
from vertexai.language_models import TextGenerationModel
import itertools
from google.cloud import texttospeech
import IPython.display as ipd
from pydub import AudioSegment
import io
import networkx as nx
import random
import pysnooper
import matplotlib.pyplot as plt
import spacy
import os
import pandas as pd
import numpy as np
import math
from ipywidgets import interact, FloatSlider, IntSlider
from typing_extensions import Literal
import altair as alt
from dataclasses import dataclass
alt.data_transformers.disable_max_rows()
import uuid
import requests
from anthropic import AnthropicVertex
import json

translate_client = translate.Client()
vertexai.init(project=project_id)
TARGET_LANGUAGE = "sv"

"""Working on a graph approach to words

# Functions
"""

def generate_lesson(input_text):
    prompt = f"""
You are a language tutor helping students learn Swedish. Based on the following upcoming translations the learner will work on, please create a mini lesson or cultural snippet to help them understand Swedish grammar patterns, or cultural context. Avoid just expanding or repeating the translations, but look for
an underlying grammatical rule or cultural significance. The output should be in SSML format, using <lang> and <voice> tags for Swedish words and phrases.
Don't leave an isolated full-stop after a </voice></lang> tag. Ensure every swedish utterance is surrounded by both <voice><lang> tags - otherwise
the English voice will say it incorrectly.

When using Swedish words or phrases, make sure to wrap them with the <lang xml:lang="sv-SE"> tag to specify the Swedish language and the <voice name="sv-SE-Standard-D"> tag to use the Swedish voice. Here's an example:

The word for hello is <lang xml:lang="sv-SE"><voice name="sv-SE-Standard-D">Hej.</voice></lang> - note that there is NO full-stop after the </lang> tag, but
it is within the tag (otherwise the English voice says 'dot' when it's transcribed)

Upcoming translations:
{input_text}

Mini lesson or cultural snippet in SSML format:
<speak>
"""

    # Use the LLM (e.g., Opus) to generate the lesson based on the prompt
    lesson = opus_generate(prompt)

    # Extract the SSML portion from the generated lesson
    ssml_output = lesson.split("<speak>")[1].split("</speak>")[0]

    def remove_trailing_fullstops(text):
      pattern = r'(</lang>)\.\s*'
      return re.sub(pattern, r'\1 ', text)
    ssml_output = remove_trailing_fullstops(ssml_output)
    return f"<speak>{ssml_output}</speak>"

def opus_generate(prompt:str):
    MODEL = "claude-3-opus@20240229"
    REGION = "us-east5"
    PROJECT_ID = project_id  # Assuming you have the project ID stored in a variable called project_id

    client = AnthropicVertex(region=REGION, project_id=PROJECT_ID)

    message = client.messages.create(
        max_tokens=1024,
        messages=[
            {
                "role": "user",
                "content": prompt,
            }
        ],
        model=MODEL,
    )
    response_json = message.model_dump_json(indent=2)

    response = json.loads(response_json)
    return response["content"][0]["text"]
SPEECH_MODELS = {"sv" :{"personA" : "sv-SE-Wavenet-D", "personB" : "sv-SE-Wavenet-C"},
                 "ta" :{"personA" : "ta-IN-Wavenet-D", "personB" : "ta-IN-Wavenet-C"},
                 "ru" :{"personA" : "ru-RU-Wavenet-D", "personB" : "ru-RU-Wavenet-C"},
                 "en" : {"personA" : "en-GB-Neural2-B"}}

def synthesize_text_from_ssml(ssml_text, language="en", model_name="en-GB-Neural2-B", speaking_rate=1.0) -> AudioSegment:
    """Synthesizes speech from the input SSML string, returns the audio_content."""
    LANGUAGE_MAPPINGS = {
        "sv": "sv-SE",
        "en": "en-GB",
        "ru": "ru-RU",
        "ta": "ta-IN"
    }

    client = texttospeech.TextToSpeechClient()

    input_text = texttospeech.SynthesisInput(ssml=ssml_text)

    voice = texttospeech.VoiceSelectionParams(
        language_code=LANGUAGE_MAPPINGS[language],
        name=model_name,
        # ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,
    )

    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3,
        speaking_rate=speaking_rate
    )

    response = client.synthesize_speech(
        request={"input": input_text, "voice": voice, "audio_config": audio_config}
    )

    return AudioSegment.from_file(io.BytesIO(response.audio_content), format="mp3")

def correct_grammar_sub_phrase(sub_phrase:dict)->tuple[str,str]:
  """returns the corrected (phrase, context) of a subphrase as strings. This might generate odd sounding
  phrases for every small fragments, as even though they are odd they can still be gramatically correct"""
  phrase_position_in_context = sub_phrase["context"].index(sub_phrase["phrase"][0])
  phrase_length = len(sub_phrase["phrase"])
  context = " ".join(sub_phrase["context"])
  corrected_context = correct_grammar(context)
  corrected_phrase = corrected_context.split(" ")[phrase_position_in_context:phrase_position_in_context + phrase_length]
  corrected_phrase_string = " ".join(corrected_phrase)
  #step one
  #find index of phrase words in context,

  return (corrected_phrase_string, corrected_context)







def list_voices():
    """Lists the available voices."""
    client = texttospeech.TextToSpeechClient()

    # Performs the list voices request
    voices = client.list_voices()

    for voice in voices.voices:
        # Display the voice's name. Example: tpc-vocoded
        print(f"Name: {voice.name}")

        # Display the supported language codes for this voice. Example: "en-US"
        for language_code in voice.language_codes:
            print(f"Supported language: {language_code}")

        ssml_gender = texttospeech.SsmlVoiceGender(voice.ssml_gender)

        # Display the SSML Voice Gender
        print(f"SSML Voice Gender: {ssml_gender.name}")

        # Display the natural sample rate hertz for this voice. Example: 24000
        print(f"Natural Sample Rate Hertz: {voice.natural_sample_rate_hertz}\n")

def string_before_brackets(string:str)->str:
  '''you (would you like) -> returns you'''
  pattern = r"^(.*) \("
  matches = re.search(pattern, string)
  if matches:
    return matches.group(1)
  else:
    return None

def string_to_sub_phrase(string:str)->dict:
  """Takes a string and turns back to sub_phrase with phrase and context keys. Note the values are strings NOT tuples here"""
  phrase = string_before_brackets(string)
  context = string_within_brackets(string)
  return {"phrase" : phrase, "context" : context}

def string_within_brackets(string:str)->str:
  """ you (would you like) -> return 'would you like' """
  pattern = r"\((.+)\)"
  matches = re.search(pattern, string)
  if matches:
    return matches.group(1)
  else:
    return None

def add_equals_signs_around_string(sub_string:str, string:str)->str:
  '''you, would you like -> would = you = like'''

  repl = f"= {sub_string} ="
  pattern = f"(?<!\w){sub_string}(?!\w)"
  modified_string = re.sub(pattern=pattern, string=string, repl=repl)
  return modified_string

def get_string_within_equals_signs(string:str)->str:
  """would = you = like -> you"""
  pattern = r"= (.+) ="
  matches = re.search(pattern, string)
  if matches:
    return matches.group(1)
  else:
    return None

{'phrase': ('a',), 'context': ('have', 'a')}

def is_sub_phrase(phrase:dict)->bool:
  """word (some words) -> true; some words -> false"""
  if ("context" in phrase) & (len(phrase["phrase"]) < 3):
    return True
  else:
    return False

def extract_sub_phrase_for_translation(phrase:str, context:str)->str:
  '''so we can pass the translation in with some context.
  for you (would you like) -> returns would = you = like'''
  modified_string = add_equals_signs_around_string(phrase, context)
  return modified_string

#@pysnooper.snoop()
def translate_sub_phrase(phrase:str, context:str, lang="sv")->str:
  """give a sub_phrase with context you (you would like), returns the
  translated word 'you' but within the wider context you would like"""
  modified_phrase = extract_sub_phrase_for_translation(phrase, context)
  translated = translate_from_english(modified_phrase, target=lang)
  translated_sub_phrase = get_string_within_equals_signs(translated)
  return translated_sub_phrase

def translate_from_english(text, target="sv"):
  result = translate.Client().translate(text, target_language=target, source_language="en")
  return result["translatedText"]

def remove_eos(phrase:dict)->dict:
  '''removes the <EOS> marker'''
  for key, value in phrase.items():
    value = (word for word in value if word != "<EOS>")
    phrase[key] = tuple(value)
  return phrase

def generate_text(prompt, temperature: float = .1):
    """Ideation example with a Large Language Model"""

    # TODO developer - override these parameters as needed:
    parameters = {
        "temperature": temperature,
        "max_output_tokens": 256,
        "top_p": .8,
        "top_k": 40,
    }

    model = TextGenerationModel.from_pretrained("text-unicorn@001")
    response = model.predict(
        prompt,
        **parameters,
    )
    return response.text

def plot_graph(graph):
    # Create a new figure and axis
    fig, ax = plt.subplots(figsize=(3, 3))

    # Draw the graph
    pos = nx.spring_layout(graph)
    nx.draw_networkx_nodes(graph, pos, node_size=250, node_color='lightblue', ax=ax)

    # Get the line widths based on the 'times_crossed' attribute
    edges = graph.edges(data=True)
    line_widths = [edge[2].get('times_crossed', 1)+1 for edge in edges]

    nx.draw_networkx_edges(graph, pos, edge_color='gray', arrows=True, ax=ax, width=line_widths)

    # Draw node labels
    labels = {node: node for node in graph.nodes()}
    nx.draw_networkx_labels(graph, pos, labels, font_size=8, ax=ax)

    # Remove axis
    ax.axis('off')

    # Display the plot
    plt.tight_layout()
    plt.show()

def plot_edge_count(graph):
  nodes = list(graph.nodes())

  # Create an empty DataFrame with nodes as row and column labels
  df = pd.DataFrame(0, index=nodes, columns=nodes)

  # Iterate over the edges and populate the DataFrame
  for source, target, data in graph.edges(data=True):
      df.at[source, target] = data['times_crossed']

  # Remove rows and columns with all zero values
  df = df.loc[:, (df != 0).any(axis=0)]
  df = df.loc[(df != 0).any(axis=1), :]

  # Replace zero values with empty string


  # Define a function to set the background color based on the value
  def highlight_cell(value, max_val):
      if pd.isna(value) or value == '':
          return ''
      else:
          intensity = (value / max_val) * 0.8 + 0.2  # Adjust the color intensity
          return f'background-color: rgba(0, 0, 255, {intensity}); color: white;'

  # Get the maximum value in the DataFrame
  max_val = df.max().max()
  df = df.replace(0, '')
  # Apply the styling to the DataFrame
  styled_df = df.style.applymap(lambda x: highlight_cell(x, max_val))

  # Set the display options to show all rows and columns
  pd.set_option('display.max_rows', None)
  pd.set_option('display.max_columns', None)

  # Print the resulting styled DataFrame
  return styled_df

def concat_audio_segments(audio_segments:list[AudioSegment], gap_ms=100)->AudioSegment:
  """Joins audio segments together with a tiny gap between each one in ms
  Returns a single joined up audio segment"""
  gap_audio = AudioSegment.silent(duration=gap_ms)
  audio_with_gap = [audio_seg + gap_audio for audio_seg in audio_segments]
  return sum(audio_with_gap)


@dataclass
class Phrase:
  original_phrase : dict
  translated : bool = False
  corrected_english: str = None
  faulty : bool = False #if the phrase is nonsensical
  target_language: str = "sv"
  phrase : str = None
  context : str = None

  def __repr__(self):

    repr_string = f"'{self.phrase}'"
    if self.context:
      repr_string += f"('{self.context}')"
    if self.translated:
      repr_string += f" -> '{self.translated_phrase}'"
    # if self.corrected_english:
    #   repr_string += f" | [grammar corrected]"
    # if self.faulty:
    #   repr_string += f" ** FAULTY **"
    return repr_string

  def __post_init__(self):
    self.hash = tuple(self.original_phrase.items())
    self.times_seen = 0
    self.phrase = " ".join(self.original_phrase["phrase"])
    if "context" in self.original_phrase:
      self.context = " ".join(self.original_phrase["context"])
    else:
      self.context = None
    if is_sub_phrase(self.original_phrase):
      self.is_sub_phrase = True
    else:
      self.is_sub_phrase = False

  def __eq__(self, other):
        if isinstance(other, Phrase):
            return self.hash == other.hash
        return False

  def __len__(self):
    return len(self.phrase.split(" ")) #number of words in the phrase

  def increment_times_seen(self):
    self.times_seen += 1

  def correct_phrase_english(self):
    '''correct the phrases english grammar'''
    if self.is_sub_phrase:
      self.phrase, self.context = correct_grammar_sub_phrase(self.original_phrase)
      if all([self.phrase, self.context]):
        self.corrected_english = True
      else:
        self.faulty = True
        return
    else:
      self.phrase = correct_grammar(self.phrase)
      if self.phrase:
        self.corrected_english = True
      else:
        self.faulty = True
        return

  def translate_corrected_english(self):
    if self.translated | self.faulty:
      return
    elif self.corrected_english == False:
      print("phrases have not been corrected yet, run phrase.correct_phrase_english()")
      return
    else:
      if self.is_sub_phrase:
        self.translated_phrase = translate_sub_phrase(self.phrase, self.context, lang=self.target_language)
        self.translated = True
      else:
        self.translated_phrase = translate_from_english(self.phrase, target=self.target_language)
        self.translated = True

  def generate_audio(self):
    #we want slow and fast versions of translated phrase
    #single phrase versions of phrase and context (english)
    if self.faulty:
      return
    if self.phrase:
      self.english_phrase_audio = synthesize_text(self.phrase,
                                           language="en",
                                           model_name = SPEECH_MODELS["en"]["personA"])
    else:
      print("empty phrase")
      return
    if self.context:
      self.english_context_audio = synthesize_text(self.context,
                                              language="en",
                                              model_name = SPEECH_MODELS["en"]["personA"])
    if self.translated_phrase:
      self.translated_audio_slow = synthesize_text(self.translated_phrase,
                                         language=self.target_language,
                                         model_name = SPEECH_MODELS[self.target_language]["personA"],
                                        speaking_rate = 0.7)
      self.translated_audio_fast = synthesize_text(self.translated_phrase,
                                         language=self.target_language,
                                         model_name = SPEECH_MODELS[self.target_language]["personA"],
                                        speaking_rate = 1.0)

@dataclass
class Phrases:
  original_phrases: list #the original output from the language graph
  target_language: str
  def __post_init__(self):
    self.create_phrase_objects()
  def create_phrase_objects(self): #there will be fewer of these
    self.phrase_hashes = set()
    self.phrase_objects = []
    for phrase in self.original_phrases:
      single_phrase = Phrase(phrase, target_language=self.target_language)
      if single_phrase.hash in self.phrase_hashes:
        #already been seen do not duplicate
        continue
      else:
        self.phrase_objects.append(single_phrase)
        self.phrase_hashes |= {single_phrase.hash}

  def correct_phrases(self):
    for phrase_object in self.phrase_objects:
      phrase_object.correct_phrase_english()

  def translate_phrases(self):
    for phrase_object in self.phrase_objects:
      phrase_object.translate_corrected_english()

  def get_phrase_object(self, phrase):
    temp_phrase = Phrase(phrase)
    loc = self.phrase_objects.index(temp_phrase)
    return self.phrase_objects[loc]

  def generate_audio(self):
    for po in self.phrase_objects:
      po.generate_audio()




class LanguageGraph():
  def __init__(self, starting_word_pick_probability:float=0.1):
    self.min_times_seen_for_long_term = 200
    self.nlp = spacy.load('en_core_web_sm')
    self.threshold_repeats = 3 #min times to repeat a word
    self.pick_decay_factor = 40 #controls how probability of picking word decreases given use
    self.recall_decay_rate = 40*1e-3 #controls how things are forgotton
    self.strength_constant = 0.05 #how much repetition reinforces memory
    self.initial_strength = 1.0
    self.alpha = 0.45 # another variable for repeititon reinforced memory
    self.graph = nx.DiGraph()
    self.pos_list_for_enrich = ["PRON", "ADV", "ADJ", "CONJ", "DET", "PUNCT", "VERB", "NOUN", "NUM" ]
    self.current_phrase = ("",)
    self.current_max_phrase_length = 1
    self.learner_max_phrase_length = 3
    self.total_phrases_generated = 0
    self.phrases_heard = {} #this will be phase tuple as key and value is count
    self.target_phrases = set()
    self.target_phrases_not_yet_heard = set()
    self.starting_word_pick_probability = starting_word_pick_probability
    self.df = pd.DataFrame() #for storing_probs etc
    self.focus_index = -1 #start of going forwards by default
    self.max_phrase_repeats = 5
    self.FROM_SUBPHRASE = False
    self.EOS_tokens = ["<EOS>", "?"]
    self.punct_to_EOS = [".", "!"] #convert these to <EOS>


  def __add__(self, other):
    """the first term in the add statement should the 'main' language graph, with new vocab being added with the
    second term"""

    self.target_phrases |= other.target_phrases
    self.target_phrases_not_yet_heard |= other.target_phrases_not_yet_heard
    self.input_phrases.extend(other.input_phrases)
    # Create a new graph to store the merged result
    merged_graph = nx.DiGraph()

    # Add nodes from the first graph and their attributes
    merged_graph.add_nodes_from(self.graph.nodes(data=True))

    # Add nodes from the other graph and update attributes with maximum values
    for node, attributes in other.graph.nodes(data=True):
      if merged_graph.has_node(node):
          # Update node attributes with maximum values
        for attr, value in attributes.items():
          merged_graph.nodes[node][attr] = max(merged_graph.nodes[node].get(attr, 0), value)
      else:
          # Add the node and its attributes to the merged graph
          merged_graph.add_node(node, **attributes)

    # Add edges from the first graph and their attributes
    merged_graph.add_edges_from(self.graph.edges(data=True))

    # Add edges from the other graph and update attributes with maximum values
    for u, v, attributes in other.graph.edges(data=True):
      if merged_graph.has_edge(u, v):
            # Update edge attributes with maximum values
        for attr, value in attributes.items():
          merged_graph.edges[u, v][attr] = max(merged_graph.edges[u, v].get(attr, 0), value)
      else:
            # Add the edge and its attributes to the merged graph
        merged_graph.add_edge(u, v, **attributes)

    # Set the merged graph as the new graph of the current object
    self.graph = merged_graph

    return self

  def create_graph_from_text(self, input_text:list[tuple], custom_stop_words=[",", ";"]):
    """From a list of tuples like [('personA', 'hello'), ('personB', 'bye')]"""
    self.lemmas_pos = []
    self.max_target_phrase_length = 0
    self.input_phrases = input_text

    utterances = []
    for person, utterance in input_text:
      doc = self.nlp(utterance.lower())
      utterance = tuple()
      for sent in doc.sents:
        sentence_pos = []
        phrase = tuple()

        for token in sent:
            lemma = token.lemma_
            pos = token.pos_
            text = token.text
            if lemma in custom_stop_words:
              continue

            if pos == "PUNCT":
              if lemma in self.punct_to_EOS:
                word = "<EOS>"
              else:
                word = text
            elif pos in ["PRON"]: #so I and me get written separately
              word = text
            else:
              word = text #used to be lemmas but removing this now as too hard to correct sentence fragments with lots of lemmas
            if word != "<EOS>": #don't have it as part of target phrases
              phrase = phrase + (word, )
              utterance = utterance + (word, )
            sentence_pos.append({word: {"pos": pos}})

        self.target_phrases |= {phrase}
        #phrase_length = len([word for word in phrase if word not in self.EOS_tokens]) #don't include EOS when calculating phrase length
        self.max_target_phrase_length = max(len(phrase), self.max_target_phrase_length)

        self.lemmas_pos.append(sentence_pos)
      self.target_phrases |= {utterance}
    for lemmas_pos_sentence in self.lemmas_pos:
      if len(lemmas_pos_sentence) == 1: #single word utterance
        curr_lemma = list(lemmas_pos_sentence[0].keys())[0]
        curr_pos = lemmas_pos_sentence[0][curr_lemma]["pos"]
        if curr_pos == "PUNCT":
          continue
        self.add_word(curr_lemma, curr_pos)
      else:
        for i in range(len(lemmas_pos_sentence) - 1):
          curr_lemma = list(lemmas_pos_sentence[i].keys())[0]
          curr_pos = lemmas_pos_sentence[i][curr_lemma]["pos"]
          if curr_pos == "PUNCT":
            #print("punct lemma ", curr_lemma)
            continue

          next_lemma = list(lemmas_pos_sentence[i + 1].keys())[0]
          next_pos = lemmas_pos_sentence[i + 1][next_lemma]["pos"]
          if (next_pos == "PUNCT"): #end the sentence
            #print("punct lemma ", next_lemma)
            if next_lemma in self.punct_to_EOS:
              next_lemma = "<EOS>"
            elif next_lemma in ["?"]: #punctuation to keep
              next_lemma = next_lemma
            else:
              continue

          self.add_word(curr_lemma, curr_pos)
          self.add_word(next_lemma, next_pos)
          self.add_word_sequence(curr_lemma, next_lemma)
    self.target_phrases_not_yet_heard = self.target_phrases.copy()

  def get_word_pos(self, word:str):
    return self.graph.nodes[word]["pos"]

  def get_max_phrase_length(self, word:str)->int:
    return self.graph.nodes[word]["max_phrase_length"]

  def get_index_of_smallest_max_phrase_length_word(self) -> int:
    smallest_max_length = min(self.get_max_phrase_length(word) for word in self.current_phrase)
    smallest_words = [i for i, word in enumerate(self.current_phrase) if self.get_max_phrase_length(word) == smallest_max_length]

    if len(smallest_words) == 1:
        return smallest_words[0]
    else:
        if self.focus_index == 0:
            return min(smallest_words)
        elif self.focus_index == -1:
            return max(smallest_words)
        else:
            raise ValueError(f"Invalid value: {self.focus_index}. Expected 'prepend' or 'append'.")

  def get_valid_sub_phrase_from_smallest_word(self, smallest_word_index)->tuple:
    smallest_word = self.current_phrase[smallest_word_index]
    max_length_smallest_word = self.get_max_phrase_length(smallest_word)
    if self.focus_index == -1:
      end = min((smallest_word_index + max_length_smallest_word), self.current_phrase_length())
      beginning = end - max_length_smallest_word
      sub_phrase = self.current_phrase[beginning:end]
    elif self.focus_index == 0:
      beginning = max(0, (smallest_word_index - max_length_smallest_word + 1) )
      end = beginning + max_length_smallest_word
      sub_phrase = self.current_phrase[beginning:end] #pos+1 - length : pos +
    else:
      raise ValueError("expect method to be -1 or 0 only")
    return sub_phrase
  #@pysnooper.snoop(depth=1, watch="self.current_phrase")
  def get_sub_phrase(self)->tuple:
    #this will be function
    smallest_word_index = self.get_index_of_smallest_max_phrase_length_word()
    smallest_word = self.current_phrase[smallest_word_index]

    #we want to return sub-phrases with words we have just met, so where they have been used in fewer than 4 word sentences
    #and the current sentence is too big
    if (self.get_max_phrase_length(smallest_word) < self.current_phrase_length()) & (self.get_max_phrase_length(smallest_word) <= 3):
      sub_phrase = self.get_valid_sub_phrase_from_smallest_word(smallest_word_index)
      return sub_phrase
    else:
      return self.current_phrase

  def can_say_whole_phrase(self)->bool:
    '''If the whole phrase can be said, i.e. every word's max_phrase_length is at least as big as the phrase length'''
    #print(f"our current_phrase length is {phrase_length}")

    #if a single noun it's fine
    for word in self.current_phrase:
      if self.graph.nodes[word]['max_phrase_length'] < self.current_phrase_length():
        #print(f"the word {word} means we can't say the whole phrase as it's length is {self.graph.nodes[word]['max_phrase_length']}")
        return False
    return True

  def add_word(self, word, pos):
    if not self.graph.has_node(word):
      self.graph.add_node(word, pos=pos, times_seen=0, last_seen=0, strength=0, recall_probability=0, max_phrase_length=1, probability_of_being_picked=self.starting_word_pick_probability)
    if word in self.EOS_tokens: #EOS tokens should not impact learning phrases
      self.graph.nodes[word]["max_phrase_length"] = 100

  def add_word_sequence(self, word1, word2):
    if not self.graph.has_edge(word1, word2):
      self.graph.add_edge(word1, word2, times_crossed=0)

  def update_graph_scores(self, phrase:tuple):
    """based on current phrase, updates the score"""

    for node in self.graph.nodes():
      if node in phrase:
        self.update_node_attributes_in_phrase(node)
      else:
        self.update_node_attributes_not_in_phrase(node)
    self.update_probability_being_picked()

  def update_probability_being_picked(self):
    """key to phrase selection behaviour are the variables that control the probability of picking a word"""
    THRESHOLD_REPEAT_DECAY = 20 #this will slighly put in favour words that haven't been seen that much
    for node in self.graph.nodes:
      times_seen = self.graph.nodes[node]['times_seen']
      recall_probability = self.graph.nodes[node]['recall_probability']
      strength = self.graph.nodes[node]['strength']

      if times_seen == 0:
        continue
      else:
        # Calculate the base probability using recall_probability
        base_prob = 1 - recall_probability

        # Adjust the probability based on times_seen
        if times_seen <= self.threshold_repeats:
            # If times_seen is below the threshold, keep the probability high
            adjusted_prob = 1.0 - (times_seen/THRESHOLD_REPEAT_DECAY)
        else:
            # If times_seen is above the threshold, gradually decrease the probability
            adjusted_prob = base_prob * math.exp(-(times_seen - self.threshold_repeats) / self.pick_decay_factor)

        # Ensure the probability is within a valid range [min_prob, max_prob]
        probability = max(0.01, min(adjusted_prob, 1.0))
      if node in self.EOS_tokens:
        probability = self.starting_word_pick_probability

      self.graph.nodes[node]['probability_of_being_picked'] = probability

  def update_node_attributes_in_phrase(self, node):
    self.graph.nodes[node]['times_seen'] += 1
    self.graph.nodes[node]['last_seen'] = 0  # You can use actual timestamps if needed
    self.graph.nodes[node]['strength'] = self.update_strength_of_word(node, self.graph.nodes[node]['times_seen'])
    #self.graph.nodes[node]['max_phrase_length'] = min(max(self.graph.nodes[node]['max_phrase_length'], len(self.current_phrase)+1), self.max_target_phrase_length)
    self.graph.nodes[node]['max_phrase_length'] += 1 #goes to 2 after first use, then takes about 4 more phrases before used in 3 length phrase
    self.graph.nodes[node]['recall_probability'] = self.recall_probability(self.graph.nodes[node]['last_seen'], self.graph.nodes[node]['strength'])

  def update_node_attributes_not_in_phrase(self, node):
    if self.graph.nodes[node]['times_seen'] != 0:
      self.graph.nodes[node]['last_seen'] += 1
      self.graph.nodes[node]['recall_probability'] = self.recall_probability(self.graph.nodes[node]['last_seen'], self.graph.nodes[node]['strength'])

  def update_edge_attributes(self, node1, node2):
    try:
      self.graph[node1][node2]['times_crossed'] += 1
    except KeyError:
      self.graph[node2][node1]['times_crossed'] += 1

  def update_strength_of_word(self, node, times_seen:int):
    """
    Update the strength of a word's memory trace based on the number of repetitions.

    Args:
        strength (float): The current strength of the word's memory trace.
        time_Seen (int): The total number of times the word has been repeated.
        c (float): A constant that determines the rate of diminishing returns.

    Returns:
        float: The updated strength of the word's memory trace.
    """
    new_strength =  1.0 + (self.strength_constant/(times_seen+1))*(times_seen**(1+self.alpha))
    return new_strength

  def recall_probability(self, last_seen, strength):
    """
    Calculate the recall probability of a word based on the exponential strength decay model.

    Args:
        last_seen (int): The number of intervening events or time elapsed since the last exposure to the word.
        strength (float): The initial strength of the word's memory trace.
    Returns:
        float: The recall probability of the word.
    """
    return min(1.0, strength * math.exp(-(self.recall_decay_rate/strength) * last_seen))

  def create_dataframe(self):
    """creates a dataframe of the data"""
    results = []
    for node, attributes in self.graph.nodes(data=True):
      node_data = {'Node': node, **attributes}
      results.append(node_data)
    self.df = pd.DataFrame(results)

  def __repr__(self):
    self.create_dataframe()
    return self.df.to_string(index=False)

  #@pysnooper.snoop()
  def select_starting_node(self, excluded_words:tuple=tuple()):
    # Add a small constant to the betweenness centrality scores
    #epsilon = 1e-6
    #betweenness_centrality = {node: score + epsilon for node, score in self.betweenness_centrality.items()}

    nodes = list(self.graph.nodes())
    nodes = [node for node in nodes if node not in excluded_words and node not in self.EOS_tokens]
    probabilities = [self.graph.nodes[node]["probability_of_being_picked"] for node in nodes]

    dice_roll = random.random() #we need to remove possibilites with low probs
    nodes_to_keep = []
    node_probabilities_to_keep = []
    for i, node in enumerate(nodes):
      if probabilities[i] > dice_roll:
        nodes_to_keep.append(node)
        node_probabilities_to_keep.append(probabilities[i])

    if (len(nodes_to_keep) > 0):
      nodes = [word for word in nodes if word in nodes_to_keep]
      probabilities = node_probabilities_to_keep

    starting_node = random.choices(nodes, probabilities)[0]
    #print(list(zip(nodes, probabilities)))
    return starting_node
  #@pysnooper.snoop(depth=2)
  def enrich_graph(self):
    """loops though the POS list and tries to add additional nodes based on existing POS links, e.g. if nouns link to verbs, then
    to enrich a verb node it will find another noun in the graph and add an edge."""

    #enrich one of each VERB, NOUN,
    for pos in self.pos_list_for_enrich:
      node_to_enrich = self.select_node_to_enrich(pos)
      if len(node_to_enrich) == 0:
        continue
      self.enrich_node_with_new_edge(node_to_enrich)

  def enrich_node_with_new_edge(self, node):
    """Given an edge (going) > (home), if you want to enrich verbs e.g. node=(going), it first finds successor nodes,
    here this would be 'home', this function finds other nouns that are poorly connected perhaps 'work' and
    adds this as a new edge, so we will get (going) > (work), if the node (work) was not very well connected"""

    # Find the outgoing neighbor of the node
    successors = list(self.graph.successors(node))
    if successors:
      target = random.choice(successors)
      target_pos = self.graph.nodes[target]['pos']

      # Find other potential target nodes in the graph with the same POS as the original
      new_target_nodes = [n for n in self.graph.nodes() if self.graph.nodes[n]['pos'] == target_pos and n not in successors]
      # Calculate weights based on the number of outgoing edges so we prioritise poorly connected new source nodes

      # Add edges from the new source to the target node to randomly selected nodes with the same POS (weighted)
      if new_target_nodes:
        weights = [1 / (len(list(self.graph.predecessors(n))) + 1) for n in new_target_nodes]
        new_target_node = random.choices(new_target_nodes, weights=weights)[0]
        self.add_word_sequence(node, new_target_node)
        print(f"adding edge ({node})->({new_target_node})")
      else:
        print("no enrichment as no other source nodes")


  def select_node_to_enrich(self, pos:str)->str:
    """returns a node to enrich based on it having not very many successors"""
    # Get all nodes with the specified POS and finds the one with fewest successors (most poorly connected)
    pos_nodes = [n for n in self.graph.nodes() if self.graph.nodes[n]['pos'] == pos]
    if len(pos_nodes) == 0:
      return []

    # Calculate weights based on the number of outgoing edges
    weights = [1 / (len(list(self.graph.successors(n))) + 1) for n in pos_nodes]

    # Randomly select a node based on weights
    return random.choices(pos_nodes, weights=weights)[0]

  def choose_next_word(self, current_word):
    edge_strength = 0.4
    successors = list(self.graph.successors(current_word))
    predecessors = list(self.graph.predecessors(current_word))
    if self.focus_index == -1:
      neighbors = successors
    else:
      neighbors = predecessors
    neighbors = [word for word in neighbors if word not in self.current_phrase]
    if len(neighbors) == 0:
      return None

    node_probabilities = [self.graph.nodes[node]['probability_of_being_picked'] for node in neighbors]

    times_crossed = [self.graph.edges[current_word, node]['times_crossed'] if self.focus_index == -1 else self.graph.edges[node, current_word]['times_crossed'] for node in neighbors]
    max_times_crossed = max(max(times_crossed), 1)
    adjusted_times_crossed = [max_times_crossed - times for times in times_crossed]

    adjusted_probabilities = [prob * (1 + edge_strength * (adj_times / max_times_crossed)) for prob, adj_times in zip(node_probabilities, adjusted_times_crossed)]

    total_prob = sum(adjusted_probabilities)
    normalized_probabilities = [prob / total_prob for prob in adjusted_probabilities]

    next_word = random.choices(neighbors, normalized_probabilities)[0]
    #print(f"next word is {next_word}")
    return next_word

  def any_stopping_conditions(self)->bool:
    '''Checks to see if we should stop iterating phrases'''

    if len(self.target_phrases_not_yet_heard) == 0:
      return True
    if self.total_phrases_generated > 500:
      print("phrase limit reached")
      return True
    if self.all_words_heard_at_least_n_times(10):
      return True
    else:
      return False

  def update_phrases_heard(self, phrase:tuple):
    '''updates global info on phrases heard'''
    self.total_phrases_generated += 1
    self.update_graph_scores(phrase)
    if phrase in self.phrases_heard:
      self.phrases_heard[phrase] += 1
    else:
      self.phrases_heard[phrase] = 1
              #avoid overly repeated phrases
    self.update_learner_max_phrase_length()


  def all_words_heard_at_least_n_times(self, n:int)->bool:
    '''Returns true if all words heard at least n times'''

    for word in self.graph.nodes():
      if word in self.EOS_tokens:
        continue
      if self.graph.nodes[word]['times_seen'] < n:
        return False
    return True

  def update_learner_max_phrase_length(self):
    '''learners gradually get exposed to bigger phrases'''
    #every 15 phrases we increment learner max_phrase_length? Also if we have heard every words n times, then the new phrase length can be n+1
    #this may need some tweaking with larger vocabs
    if self.all_words_heard_at_least_n_times(self.learner_max_phrase_length * 1.5):
      self.learner_max_phrase_length += 1
    else:
      self.learner_max_phrase_length += int(self.total_phrases_generated % 20 == 0)
      #print("max phrase length", self.learner_max_phrase_length)
  def current_phrase_length(self)->int:
    '''returns the actual current phrase length ignoring punctuation'''

    return len(set(self.current_phrase).difference(set(self.EOS_tokens)))

  #@pysnooper.snoop(watch=('self.current_phrase', 'starting_word', 'sub_phase', 'next_word' 'self.focus_index', 'self.learner_max_phrase_length', 'max_phrase_length_for_word'))
  def generate_phrase(self):
    """generates a phrase based on the starting word and its attriubuts which it will get,
    like max phrase length needs to iterate"""
    focus_index_mapping = {0 : "prepend", -1 : "append"}
    excluded_starting_nodes = tuple()
    while self.any_stopping_conditions() == False:

      if self.FROM_SUBPHRASE == False:
        prev_phrase = self.current_phrase
        #beginnings of a fresh phrase
        starting_word = self.select_starting_node(excluded_starting_nodes)
        self.focus_index = -1
        self.current_phrase = (starting_word, )
        #self.focus_index = -1 #which bit of the phrase we are focussed on, we default to the end
        max_phrase_length_for_word = self.graph.nodes[starting_word]['max_phrase_length']
      if self.FROM_SUBPHRASE == True:
        prev_phrase = self.current_phrase
        focus_word = self.current_phrase[self.focus_index]
        max_phrase_length_for_word = self.graph.nodes[focus_word]['max_phrase_length']

      while self.current_phrase_length() < min(max_phrase_length_for_word+1, self.learner_max_phrase_length):
        next_word = self.choose_next_word(self.current_phrase[self.focus_index])
        if next_word is None: #try beginning of the phrase
          self.focus_index = 0
          next_word = self.choose_next_word(self.current_phrase[self.focus_index])
          if next_word is None:
            #print("limit of phrase reached")
            break
        # if next_word in self.EOS_tokens:
        #   self.focus_index=0 #go to beginning of the word
        #   continue

        if focus_index_mapping[self.focus_index] == 'append':
          self.update_edge_attributes(self.current_phrase[self.focus_index], next_word)
          self.current_phrase = self.current_phrase + (next_word,)

        elif focus_index_mapping[self.focus_index] == 'prepend': #action must be prepend
          self.update_edge_attributes(next_word, self.current_phrase[self.focus_index])
          self.current_phrase = (next_word,) + self.current_phrase

      while self.can_say_whole_phrase() == False:
        #print("cannot say whole phrase")
        sub_phrase = self.get_sub_phrase() # add yield here?
        #print(f"our subphrase is {sub_phrase}")
        if len(sub_phrase) == 0:
          break
        self.update_phrases_heard(sub_phrase)
        self.FROM_SUBPHRASE = True
        # allow single nouns
        if ((len(sub_phrase) ==1) & (self.get_word_pos(sub_phrase[0]) == "NOUN")) | (len(sub_phrase) >= 4):
          yield {"phrase" : sub_phrase}
        else:
          yield {"phrase" : sub_phrase, "context" : self.current_phrase}


      #we can now say the whole planned phrase
      if self.phrases_heard.get(self.current_phrase,0) >= self.max_phrase_repeats:
        #print(f'max phrase repeats of {self.current_phrase}')
        excluded_starting_nodes = prev_phrase

      self.update_phrases_heard(self.current_phrase)
      if (self.current_phrase in self.target_phrases_not_yet_heard) & (self.FROM_SUBPHRASE == False):
        #self.starting_word = None
        self.target_phrases_not_yet_heard.discard(self.current_phrase)
        if (self.current_phrase == prev_phrase) & (self.current_phrase_length() > 4): #if we are repeating, then exclude starting nodes on repeat
          excluded_starting_nodes = prev_phrase
        else:
          excluded_starting_nodes = tuple()
      self.FROM_SUBPHRASE = False
      yield {"phrase" : self.current_phrase}
    #now we generate any remaining target_phrases_not_yet_heard
    while len(self.target_phrases_not_yet_heard) > 0:
      self.current_phrase = random.choice(tuple(self.target_phrases_not_yet_heard))
      self.target_phrases_not_yet_heard.discard(self.current_phrase)
      self.update_phrases_heard(self.current_phrase)
      yield {"phrase" : self.current_phrase}

def generate_lessons(phrases, n=5):
    lessons = []
    input_text = ""

    for index, phrase in enumerate(phrases.phrase_objects, start=1):
        input_text += str(phrase) + " | "

        if index % n == 0 or index == len(phrases.phrase_objects):
            input_text = input_text.rstrip(" | ")  # Remove the trailing " | "
            lesson = generate_lesson(input_text)
            lessons.append(lesson)
            input_text = ""  # Reset input_text for the next set of phrases

    return lessons

def generate_dialogue(scenario):
    prompt = f"""
Given the following scenario, generate a short dialogue between two people, named personA and personB. Use simple vocabulary and present tense or simple future tense like "I am going to <verb>". Do not use contractions; spell them out instead, like "do not" instead of "don't".

Format the output as a Python list of tuples, where each tuple contains the speaker (either "personA" or "personB") and their line of dialogue. Here are a couple examples:

Scenario: Asking for directions to the library.
output:
[
    ("personA", "Excuse me, can you tell me how to get to the library?"),
    ("personB", "Sure, go straight for three blocks, then turn right. The library will be on your left."),
    ("personA", "Thank you for your help!"),
    ("personB", "You are welcome. Have a great day!")
]

Scenario: Ordering food at a restaurant.
output:
[
    ("personA", "Hello, I would like to order a burger and fries, please."),
    ("personB", "Great choice! Would you like anything to drink with that?"),
    ("personA", "Yes, I will have a lemonade, please."),
    ("personB", "Alright, your order will be ready in about 10 minutes. Thank you!")
]

Scenario: {scenario}
output:
"""

    generated_text = generate_text(prompt)
    # Extract the dialogue from the generated text
    dialogue_start = generated_text.find("[")
    dialogue_end = generated_text.rfind("]") + 1
    dialogue_text = generated_text[dialogue_start:dialogue_end]
    return eval(dialogue_text)

REPEATED_AUDIO = {
    "the-swedish-for" : synthesize_text("The swedish for"),
    "in-the-context-of" : synthesize_text("in the context of"),
    "is" : synthesize_text("is"),
    "how-do-you-say" : synthesize_text("how do you say")
}

GAP = AudioSegment.silent(duration=100)

def generate_audio_lessons(my_phrases, n):
    audio_segments_lesson = []
    lessons = []
    input_text = ""
    phrase_index = 0
    lesson_insertion_points = [0]

    for phrase in my_phrases.original_phrases:
        po = my_phrases.get_phrase_object(phrase)

        if po.context:
            length_context = len(po.original_phrase["context"])
        else:
            length_context = 0

        if po.faulty == True or po.times_seen > 4:
            continue

        if (len(po) <= 2) & (po.times_seen == 0):
            try:
                if po.is_sub_phrase & (length_context <= 4):
                    print(f"The swedish for {po.phrase} in the context of {po.context} is {po.translated_phrase}")
                    audio_segments_lesson.extend([REPEATED_AUDIO["the-swedish-for"],
                                                   po.english_phrase_audio,
                                                   REPEATED_AUDIO["in-the-context-of"],
                                                   po.english_context_audio,
                                                   REPEATED_AUDIO["is"],
                                                   po.translated_audio_slow,
                                                   GAP,GAP,
                                                   po.translated_audio_fast])
                else:
                    print(f"The swedish for {po.phrase} is {po.translated_phrase}")
                    audio_segments_lesson.extend([REPEATED_AUDIO["the-swedish-for"],
                                                   po.english_phrase_audio,
                                                   REPEATED_AUDIO["is"],
                                                   po.translated_audio_slow,
                                                   GAP,GAP,
                                                   po.translated_audio_fast])
            except:
                continue

        print(f"How do you say {po.phrase}")
        audio_segments_lesson.extend([REPEATED_AUDIO["how-do-you-say"],
                                       po.english_phrase_audio,
                                       GAP*6])

        print(f"swedish: {po.translated_phrase}")
        audio_segments_lesson.extend([GAP,
                                       po.translated_audio_slow,
                                       GAP,GAP,
                                       po.translated_audio_fast,
                                       GAP,GAP,GAP,GAP,GAP])

        input_text += str(po) + " | "
        phrase_index += 1


        if phrase_index % n == 0 or phrase_index == len(my_phrases.original_phrases):
            input_text = input_text.rstrip(" | ")
            lesson_ssml = generate_lesson(input_text)
            try:
              lesson_audio = synthesize_text_from_ssml(lesson_ssml)
              lessons.append(lesson_audio)
              lesson_insertion_points.append(len(audio_segments_lesson))
            except: #if the SSML is invalid
              pass
            input_text = ""

        po.increment_times_seen()

    final_audio_segments = []
    lesson_index = 0
    for index, segment in enumerate(audio_segments_lesson):
        if lesson_index < len(lesson_insertion_points) and index == lesson_insertion_points[lesson_index]:
          if lesson_index >= len(lessons):
            continue
          else:
            final_audio_segments.append(lessons[lesson_index])
            lesson_index += 1
        final_audio_segments.append(segment)

    return final_audio_segments

"""



# Select input conversation"""

input_text_restaurant = [
("personA", "Hello, I'd like to order, please."),
("personB", "Sure, what would you like?"),
("personA", "I'll have the chicken sandwich and a salad."),
("personB", "Great choice. Anything to drink?"),
("personA", "A glass of water, please."),
("personB", "Alright, your order will be ready soon.")
]

input_text_souvenirs = [
("personA", "How much does this keychain cost?"),
("personB", "It's three dollars."),
("personA", "Okay, I'll take two keychains, please."),
("personB", "Great, that will be six dollars in total."),
("personA", "Here you go. Thank you!"),
("personB", "Thank you for your purchase. Have a nice day!")
]

input_text_hotel = [
("personA", "Hello, I'd like to book a room for two nights."),
("personB", "Sure, we have a few rooms available. Would you prefer a single or double room?"),
("personA", "A double room, please."),
("personB", "Alright, and what dates will you be staying?"),
("personA", "From the 15th to the 17th of this month."),
("personB", "Perfect, your reservation is confirmed. Check-in is at 2 PM.")
]

input_text_taxi = [
("personA", "Taxi, Can you take me to the airport, please?"),
("personB", "Of course, hop in. Which terminal?"),
("personA", "Terminal 2, please."),
("personB", "Alright, it should take about 20 minutes to get there."),
("personA", "Great, thank you. How much will it cost?"),
("personB", "It will be around 25 dollars.")
]

# Input text
input_text = [
    ("personA", "Hello, can I have a coffee please?"),
    ("personB", "Yes, of course. What size would you like?"),
    ("personA", "A medium coffee, please. With milk."),
    ("personB", "Anything else?"),
    ("personA", "Yes, I'd like a slice of chocolate cake too."),
    ("personB", "Certainly, That'll be 25 krona")
]

input_text_museum = [ ("personA", "Excuse me, how do I get to the museum?"),
                     ("personB", "Go straight down this street for two blocks, then turn left. The museum will be on your right."),
                      ("personA", "Thank you very much!"),
                      ("personB", "You are welcome. Enjoy your visit!") ]

input_small_test = [
    ("personA", "One mug of coffee."),
    ("personB", "Two cups of Tea?")]

input_add_test = [("personA", "Three pieces of cake")]

input_useful = [
    ("personA", "Hello, how are you?"),
    ("personB", "Good morning, I am OK, what about you?"),
    ("personA", "I am coming to Sweden next week."),
    ("personB", "Great! I will see you later"),
    ("personA", "Do you want to meet for a coffee or a beer?"),
    ("personB", "Yes please"),
    ("personA", "Thank you, Goodbye")
]

input_core = [
    ("personA", "Hello, welcome to the cafe. How are you?"),
    ("personB", "Good morning, I am fine, thank you, what about you?"),
    ("personA", "I am coming to Sweden next week."),
    ("personB", "Great I will see you later"),
    ("personA", "What would you like to order?"),
    ("personB", "I would like that one there"),
    ("personA", "This one?"),
    ("personB", "No, that one. And can I have two or three please"),
    ("personA", "Of course, how do you want to pay? With card or cash?"),
    ("personB", "Card please"),
]

#INPUT_PHRASES = generate_dialogue("Going to the office of PA Consulting and then going for a beer")
#print(INPUT_PHRASES)

#test addition
lg1 = LanguageGraph(starting_word_pick_probability=0.2)
lg2 = LanguageGraph(starting_word_pick_probability=0.2)
lg1.create_graph_from_text(input_small_test)
lg2.create_graph_from_text(input_add_test)
lg1.enrich_graph()
lg1.enrich_graph()
lg1.enrich_graph()

sentences = []
for phrase in lg1.generate_phrase():
  print(phrase)
  sentences.append(phrase)

lg1 = lg1 + lg2
lg1.enrich_graph()
lg1.enrich_graph()
lg1.enrich_graph()

for phrase in lg1.generate_phrase():
  print(phrase)
  sentences.append(phrase)

"""# DANGEROUS DEMO"""

INPUT_PHRASES = generate_dialogue("How to communicate with your cat")
print(INPUT_PHRASES)

lg = LanguageGraph(starting_word_pick_probability=0.2)
lg.create_graph_from_text(INPUT_PHRASES)
lg.enrich_graph()
#lg.enrich_graph()
#lg.enrich_graph()

sentences = []
for phrase in lg.generate_phrase():
  print(phrase)
  sentences.append(phrase)

"""## Build phrase objects from language graph output"""

my_phrases = Phrases(original_phrases = sentences[:10], target_language=TARGET_LANGUAGE)

my_phrases.correct_phrases()
my_phrases.translate_phrases()
my_phrases.generate_audio()
LANGUAGE_GRAPH = lg #change this for demo

"""# Text to speech"""

NUM_LESSONS = 2
n = int((len(my_phrases.original_phrases) * 2) / (NUM_LESSONS + 1)) # 2 grammar lessons
print(n)

#reset the count in case we have looped through before
for phrase in my_phrases.phrase_objects:
  phrase.times_seen = 0
audio_segments_lesson = generate_audio_lessons(my_phrases, n=n)

audio_lesson = concat_audio_segments(audio_segments_lesson)

play_audio(audio_lesson, "one_I_made_earlier.mp3")

"""### translate the target phrases -> audio (Introduction)"""

#translate input_phrases and generate audio
audio_segments = []
for person, phrase in LANGUAGE_GRAPH.input_phrases:
  translated_phrase = translate_from_english(phrase, target=TARGET_LANGUAGE)
  translated_audio = synthesize_text(translated_phrase,
                                     language=TARGET_LANGUAGE,
                                     model_name = SPEECH_MODELS[TARGET_LANGUAGE][person])
  audio_segments.append(translated_audio)

joined_audio_intro_conversation = concat_audio_segments(audio_segments)
faster_intro_conversation = joined_audio_intro_conversation.speedup(playback_speed=1.5)
intro_then_lesson = concat_audio_segments([joined_audio_intro_conversation, audio_lesson, faster_intro_conversation*3])
play_audio(intro_then_lesson, filename="lesson_core.mp3")

"""### archive"""

def update_strength_of_word(initial_strength, times_seen, strength_constant=0.05, alpha=0.45):
    return initial_strength + (strength_constant/(times_seen+1))*(times_seen**(1+alpha))

def recall_probability(last_seen, strength, decay_rate=40*1e-3):
    return min(1.0, strength * math.exp(-(decay_rate/strength) * last_seen))

def update_probability_being_picked(times_seen, recall_probability, threshold_repeats=3, pick_decay_factor=40, min_prob=0.01, max_prob=1.0):
    if times_seen == 0:
        return 0.0
    else:
        base_prob = 1 - recall_probability
        if times_seen <= threshold_repeats:
            adjusted_prob = 1.0
        else:
            adjusted_prob = base_prob * math.exp(-(times_seen - threshold_repeats) / pick_decay_factor)
        return max(min_prob, min(adjusted_prob, max_prob))

def update_figure_recall(initial_strength, strength_constant, recall_decay_rate, times_seen, alpha):
    last_seen = np.arange(0, 200)
    strength = update_strength_of_word(initial_strength, times_seen, strength_constant, alpha)
    recall_prob = [recall_probability(t, strength, recall_decay_rate*1e-3) for t in last_seen]
    #pick_prob = [update_probability_being_picked(t, r, threshold_repeats, decay_factor) for t, r in zip(times_seen, recall_prob)]

    fig, ax = plt.subplots(figsize=(8, 6))
    ax.plot(last_seen, recall_prob, label="Recall Probability", color="blue")
    #ax.plot(times_seen, pick_prob, label="Probability of Being Picked", color="red")
    ax.set_xlabel("Last Seen")
    ax.set_ylabel("Recall Probability")
    ax.set_title("Word Memory Decay give strength")
    ax.legend()
    plt.tight_layout()
    plt.show()

def update_figure(threshold_repeats, pick_decay_factor, last_seen,alpha):
    times_seen = np.arange(0, 50)
    strength = [update_strength_of_word(1, t, strength_constant=0.05, alpha=alpha) for t in times_seen]
    recall_prob = [recall_probability(last_seen, s) for  s in  strength]
    pick_prob = [update_probability_being_picked(t, r, threshold_repeats, pick_decay_factor) for t, r in zip(times_seen, recall_prob)]

    fig, ax = plt.subplots(figsize=(8, 6))
    ax.plot(times_seen, recall_prob, label="Recall Probability", color="blue")
    ax.plot(times_seen, pick_prob, label="Probability of Being Picked", color="red")
    ax.plot(times_seen, strength, label="Strength", color="green")
    ax.set_xlabel("Times Seen")
    ax.set_ylabel("Probability")
    ax.set_title("Word Memory Decay and Probability of Being Picked")
    ax.legend()
    plt.tight_layout()
    plt.show()

interact(
    update_figure_recall,
    initial_strength=IntSlider(min=0, max=10, step=1, value=1.0, description="Initial Strength"),
    strength_constant=FloatSlider(min=0.00, max=1.0, step=0.01, value=0.05, description="Strength Constant"),
    alpha=FloatSlider(min=0.0, max=2.0, step=0.01, value=0.45, description="Alpha"),
    times_seen=IntSlider(min=1., max=300.0, step=5, value=1.0, description="Times Seen"),
    recall_decay_rate=IntSlider(min=0, max=200, step=10, value=40, description="Recall Decay Rate (recall)"),

)

# Create the interactive plot with sliders
interact(
    update_figure,
    last_seen=IntSlider(min=1, max=200, step=5, value=0, description="last_seen"),
    threshold_repeats=IntSlider(min=0, max=10, step=1, value=3, description="Threshold Repeats"),
    pick_decay_factor=FloatSlider(min=0, max=200, step=10, value=40, description="Pick Decay Factor - prob pick"),
    alpha=FloatSlider(min=0.0, max=2.0, step=0.01, value=0.45, description="Alpha"),
)

# build audio lesson
audio_segments_lesson = []
for phrase in my_phrases.original_phrases:
  po = my_phrases.get_phrase_object(phrase)
  #print(po.phrase, len(po))
  if po.context:
    length_context = len(po.original_phrase["context"])
  else:
    length_context = 0
  if (po.faulty == True):
    continue
  if po.times_seen > 4: #limit for the lesson
    continue
  if (len(po) <= 2) & (po.times_seen == 0):
    try: #we want to introduce new words when the phrase length is 2 or smaller
      if po.is_sub_phrase & (length_context <= 4):
        print(f"The swedish for {po.phrase} in the context of {po.context} is {po.translated_phrase}")
        audio_segments_lesson.extend([REPEATED_AUDIO["the-swedish-for"],
                                    po.english_phrase_audio,
                                    REPEATED_AUDIO["in-the-context-of"],
                                    po.english_context_audio,
                                    REPEATED_AUDIO["is"],
                                    po.translated_audio_slow,
                                    GAP,GAP,
                                    po.translated_audio_fast])
      else:
        print(f"The swedish for {po.phrase} is {po.translated_phrase}")
        audio_segments_lesson.extend([REPEATED_AUDIO["the-swedish-for"],
                                    po.english_phrase_audio,
                                    REPEATED_AUDIO["is"],
                                    po.translated_audio_slow,
                                    GAP,GAP,
                                    po.translated_audio_fast])
    except:
      continue
  print(f"How do you say {po.phrase}")
  audio_segments_lesson.extend([REPEATED_AUDIO["how-do-you-say"],
                               po.english_phrase_audio,
                                GAP*6])
  print(f"swedish: {po.translated_phrase}")
  audio_segments_lesson.extend([GAP,
                               po.translated_audio_slow,
                               GAP,GAP,
                               po.translated_audio_fast,
                                GAP,GAP,GAP,GAP,GAP])
  po.increment_times_seen()
#audio_lesson = concat_audio_segments(audio_segments_lesson)