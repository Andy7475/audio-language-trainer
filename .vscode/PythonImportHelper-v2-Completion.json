[
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "sysconfig",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sysconfig",
        "description": "sysconfig",
        "detail": "sysconfig",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "winreg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "winreg",
        "description": "winreg",
        "detail": "winreg",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pycountry",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycountry",
        "description": "pycountry",
        "detail": "pycountry",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "spacy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "spacy",
        "description": "spacy",
        "detail": "spacy",
        "documentation": {}
    },
    {
        "label": "vertexai",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "vertexai",
        "description": "vertexai",
        "detail": "vertexai",
        "documentation": {}
    },
    {
        "label": "AnthropicVertex",
        "importPath": "anthropic",
        "description": "anthropic",
        "isExtraImport": true,
        "detail": "anthropic",
        "documentation": {}
    },
    {
        "label": "AnthropicVertex",
        "importPath": "anthropic",
        "description": "anthropic",
        "isExtraImport": true,
        "detail": "anthropic",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "AudioSegment",
        "importPath": "pydub",
        "description": "pydub",
        "isExtraImport": true,
        "detail": "pydub",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "ImageGenerationModel",
        "importPath": "vertexai.preview.vision_models",
        "description": "vertexai.preview.vision_models",
        "isExtraImport": true,
        "detail": "vertexai.preview.vision_models",
        "documentation": {}
    },
    {
        "label": "ImageGenerationModel",
        "importPath": "vertexai.preview.vision_models",
        "description": "vertexai.preview.vision_models",
        "isExtraImport": true,
        "detail": "vertexai.preview.vision_models",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "VoiceProvider",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "VoiceInfo",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "isExtraImport": true,
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "urllib.parse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "shuffle",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "genanki",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "genanki",
        "description": "genanki",
        "detail": "genanki",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "pysnooper",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pysnooper",
        "description": "pysnooper",
        "detail": "pysnooper",
        "documentation": {}
    },
    {
        "label": "Collection",
        "importPath": "anki.collection",
        "description": "anki.collection",
        "isExtraImport": true,
        "detail": "anki.collection",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "add_audio",
        "importPath": "src.generate",
        "description": "src.generate",
        "isExtraImport": true,
        "detail": "src.generate",
        "documentation": {}
    },
    {
        "label": "add_translations",
        "importPath": "src.generate",
        "description": "src.generate",
        "isExtraImport": true,
        "detail": "src.generate",
        "documentation": {}
    },
    {
        "label": "add_image_paths",
        "importPath": "src.images",
        "description": "src.images",
        "isExtraImport": true,
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "tokenize_text",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_from_english",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "tokenize_text",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_dialogue",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_phrases",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_dialogue",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_from_english",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_phrases",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "tokenize_text",
        "importPath": "src.translation",
        "description": "src.translation",
        "isExtraImport": true,
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "clean_filename",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "create_test_story_dict",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "string_to_large_int",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "clean_filename",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "anthropic_generate",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "extract_json_from_llm_response",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "anthropic_generate",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "clean_filename",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "ok_to_query_api",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "load_json",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "update_vocab_usage",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "sanitize_path_component",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "construct_gcs_path",
        "importPath": "src.utils",
        "description": "src.utils",
        "isExtraImport": true,
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "html",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "html",
        "description": "html",
        "detail": "html",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "IPython.display",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "IPython.display",
        "description": "IPython.display",
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "librosa",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "librosa",
        "description": "librosa",
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "texttospeech",
        "importPath": "google.cloud",
        "description": "google.cloud",
        "isExtraImport": true,
        "detail": "google.cloud",
        "documentation": {}
    },
    {
        "label": "texttospeech_v1",
        "importPath": "google.cloud",
        "description": "google.cloud",
        "isExtraImport": true,
        "detail": "google.cloud",
        "documentation": {}
    },
    {
        "label": "texttospeech",
        "importPath": "google.cloud",
        "description": "google.cloud",
        "isExtraImport": true,
        "detail": "google.cloud",
        "documentation": {}
    },
    {
        "label": "translate_v2",
        "importPath": "google.cloud",
        "description": "google.cloud",
        "isExtraImport": true,
        "detail": "google.cloud",
        "documentation": {}
    },
    {
        "label": "language_v1",
        "importPath": "google.cloud",
        "description": "google.cloud",
        "isExtraImport": true,
        "detail": "google.cloud",
        "documentation": {}
    },
    {
        "label": "storage",
        "importPath": "google.cloud",
        "description": "google.cloud",
        "isExtraImport": true,
        "detail": "google.cloud",
        "documentation": {}
    },
    {
        "label": "MP4",
        "importPath": "mutagen.mp4",
        "description": "mutagen.mp4",
        "isExtraImport": true,
        "detail": "mutagen.mp4",
        "documentation": {}
    },
    {
        "label": "MP4Cover",
        "importPath": "mutagen.mp4",
        "description": "mutagen.mp4",
        "isExtraImport": true,
        "detail": "mutagen.mp4",
        "documentation": {}
    },
    {
        "label": "azure.cognitiveservices.speech",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "azure.cognitiveservices.speech",
        "description": "azure.cognitiveservices.speech",
        "detail": "azure.cognitiveservices.speech",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "SimpleNamespace",
        "importPath": "types",
        "description": "types",
        "isExtraImport": true,
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "Language",
        "importPath": "langcodes",
        "description": "langcodes",
        "isExtraImport": true,
        "detail": "langcodes",
        "documentation": {}
    },
    {
        "label": "LanguageTagError",
        "importPath": "langcodes",
        "description": "langcodes",
        "isExtraImport": true,
        "detail": "langcodes",
        "documentation": {}
    },
    {
        "label": "# async_process_phrases",
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "isExtraImport": true,
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "generate_audio_from_dialogue",
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "isExtraImport": true,
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "generate_normal_and_fast_audio",
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "isExtraImport": true,
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "generate_translated_phrase_audio",
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "isExtraImport": true,
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "create_m4a_with_timed_lyrics",
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "isExtraImport": true,
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "plotly.express",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plotly.express",
        "description": "plotly.express",
        "detail": "plotly.express",
        "documentation": {}
    },
    {
        "label": "plotly.graph_objects",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plotly.graph_objects",
        "description": "plotly.graph_objects",
        "detail": "plotly.graph_objects",
        "documentation": {}
    },
    {
        "label": "make_subplots",
        "importPath": "plotly.subplots",
        "description": "plotly.subplots",
        "isExtraImport": true,
        "detail": "plotly.subplots",
        "documentation": {}
    },
    {
        "label": "Token",
        "importPath": "spacy.tokens",
        "description": "spacy.tokens",
        "isExtraImport": true,
        "detail": "spacy.tokens",
        "documentation": {}
    },
    {
        "label": "anthropic_generate",
        "importPath": "src.dialogue_generation",
        "description": "src.dialogue_generation",
        "isExtraImport": true,
        "detail": "src.dialogue_generation",
        "documentation": {}
    },
    {
        "label": "extract_json_from_llm_response",
        "importPath": "src.dialogue_generation",
        "description": "src.dialogue_generation",
        "isExtraImport": true,
        "detail": "src.dialogue_generation",
        "documentation": {}
    },
    {
        "label": "add_usage_to_words",
        "importPath": "src.dialogue_generation",
        "description": "src.dialogue_generation",
        "isExtraImport": true,
        "detail": "src.dialogue_generation",
        "documentation": {}
    },
    {
        "label": "extract_spacy_lowercase_words",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "extract_substring_matches",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "extract_vocab_and_pos",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "get_verb_and_vocab_lists",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "remove_matching_words",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "get_vocab_dict_from_dialogue",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "extract_substring_matches",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "find_best_card",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "find_candidate_cards",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "get_matching_flashcards_indexed",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "get_vocab_dictionary_from_phrases",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "remove_matching_words",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "create_flashcard_index",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "process_phrase_vocabulary",
        "importPath": "src.nlp",
        "description": "src.nlp",
        "isExtraImport": true,
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "retry",
        "importPath": "google.api_core",
        "description": "google.api_core",
        "isExtraImport": true,
        "detail": "google.api_core",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "generate_wiktionary_links",
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "isExtraImport": true,
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "ANY",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "mock_open",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "MagicMock",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "Tee",
        "kind": 6,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "class Tee:\n    def __init__(self, file):\n        self.f = file\n    def write(self, what):\n        if self.f is not None:\n            try:\n                self.f.write(what.replace(\"\\n\", \"\\r\\n\"))\n            except OSError:\n                pass\n        tee_f.write(what)",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "CopyTo",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def CopyTo(desc, src, dest):\n    import win32api\n    import win32con\n    while 1:\n        try:\n            win32api.CopyFile(src, dest, 0)\n            return\n        except win32api.error as details:\n            if details.winerror == 5:  # access denied - user not admin.\n                raise",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "LoadSystemModule",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def LoadSystemModule(lib_dir, modname):\n    # See if this is a debug build.\n    import importlib.machinery\n    import importlib.util\n    suffix = \"_d\" if \"_d.pyd\" in importlib.machinery.EXTENSION_SUFFIXES else \"\"\n    filename = \"%s%d%d%s.dll\" % (\n        modname,\n        sys.version_info.major,\n        sys.version_info.minor,\n        suffix,",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "SetPyKeyVal",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def SetPyKeyVal(key_name, value_name, value):\n    root_hkey = get_root_hkey()\n    root_key = winreg.OpenKey(root_hkey, root_key_name)\n    try:\n        my_key = winreg.CreateKey(root_key, key_name)\n        try:\n            winreg.SetValueEx(my_key, value_name, 0, winreg.REG_SZ, value)\n            if verbose:\n                print(f\"-> {root_key_name}\\\\{key_name}[{value_name}]={value!r}\")\n        finally:",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "UnsetPyKeyVal",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def UnsetPyKeyVal(key_name, value_name, delete_key=False):\n    root_hkey = get_root_hkey()\n    root_key = winreg.OpenKey(root_hkey, root_key_name)\n    try:\n        my_key = winreg.OpenKey(root_key, key_name, 0, winreg.KEY_SET_VALUE)\n        try:\n            winreg.DeleteValue(my_key, value_name)\n            if verbose:\n                print(f\"-> DELETE {root_key_name}\\\\{key_name}[{value_name}]\")\n        finally:",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterCOMObjects",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterCOMObjects(register=True):\n    import win32com.server.register\n    if register:\n        func = win32com.server.register.RegisterClasses\n    else:\n        func = win32com.server.register.UnregisterClasses\n    flags = {}\n    if not verbose:\n        flags[\"quiet\"] = 1\n    for module, klass_name in com_modules:",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterHelpFile",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterHelpFile(register=True, lib_dir=None):\n    if lib_dir is None:\n        lib_dir = sysconfig.get_paths()[\"platlib\"]\n    if register:\n        # Register the .chm help file.\n        chm_file = os.path.join(lib_dir, \"PyWin32.chm\")\n        if os.path.isfile(chm_file):\n            # This isn't recursive, so if 'Help' doesn't exist, we croak\n            SetPyKeyVal(\"Help\", None, None)\n            SetPyKeyVal(\"Help\\\\Pythonwin Reference\", None, chm_file)",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "RegisterPythonwin",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def RegisterPythonwin(register=True, lib_dir=None):\n    \"\"\"Add (or remove) Pythonwin to context menu for python scripts.\n    ??? Should probably also add Edit command for pys files also.\n    Also need to remove these keys on uninstall, but there's no function\n        like file_created to add registry entries to uninstall log ???\n    \"\"\"\n    import os\n    if lib_dir is None:\n        lib_dir = sysconfig.get_paths()[\"platlib\"]\n    classes_root = get_root_hkey()",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "get_shortcuts_folder",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def get_shortcuts_folder():\n    if get_root_hkey() == winreg.HKEY_LOCAL_MACHINE:\n        try:\n            fldr = get_special_folder_path(\"CSIDL_COMMON_PROGRAMS\")\n        except OSError:\n            # No CSIDL_COMMON_PROGRAMS on this platform\n            fldr = get_special_folder_path(\"CSIDL_PROGRAMS\")\n    else:\n        # non-admin install - always goes in this user's start menu.\n        fldr = get_special_folder_path(\"CSIDL_PROGRAMS\")",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "get_system_dir",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def get_system_dir():\n    import win32api  # we assume this exists.\n    try:\n        import pythoncom\n        import win32process\n        from win32com.shell import shell, shellcon\n        try:\n            if win32process.IsWow64Process():\n                return shell.SHGetSpecialFolderPath(0, shellcon.CSIDL_SYSTEMX86)\n            return shell.SHGetSpecialFolderPath(0, shellcon.CSIDL_SYSTEM)",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "fixup_dbi",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def fixup_dbi():\n    # We used to have a dbi.pyd with our .pyd files, but now have a .py file.\n    # If the user didn't uninstall, they will find the .pyd which will cause\n    # problems - so handle that.\n    import win32api\n    import win32con\n    pyd_name = os.path.join(os.path.dirname(win32api.__file__), \"dbi.pyd\")\n    pyd_d_name = os.path.join(os.path.dirname(win32api.__file__), \"dbi_d.pyd\")\n    py_name = os.path.join(os.path.dirname(win32con.__file__), \"dbi.py\")\n    for this_pyd in (pyd_name, pyd_d_name):",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "install",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def install(lib_dir):\n    import traceback\n    # The .pth file is now installed as a regular file.\n    # Create the .pth file in the site-packages dir, and use only relative paths\n    # We used to write a .pth directly to sys.prefix - clobber it.\n    if os.path.isfile(os.path.join(sys.prefix, \"pywin32.pth\")):\n        os.unlink(os.path.join(sys.prefix, \"pywin32.pth\"))\n    # The .pth may be new and therefore not loaded in this session.\n    # Setup the paths just in case.\n    for name in \"win32 win32\\\\lib Pythonwin\".split():",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "uninstall",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def uninstall(lib_dir):\n    # First ensure our system modules are loaded from pywin32_system, so\n    # we can remove the ones we copied...\n    LoadSystemModule(lib_dir, \"pywintypes\")\n    LoadSystemModule(lib_dir, \"pythoncom\")\n    try:\n        RegisterCOMObjects(False)\n    except Exception as why:\n        print(f\"Failed to unregister COM objects: {why}\")\n    try:",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "verify_destination",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def verify_destination(location):\n    if not os.path.isdir(location):\n        raise argparse.ArgumentTypeError(f'Path \"{location}\" does not exist!')\n    return location\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=\"\"\"A post-install script for the pywin32 extensions.\n    * Typical usage:\n    > python pywin32_postinstall.py -install",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=\"\"\"A post-install script for the pywin32 extensions.\n    * Typical usage:\n    > python pywin32_postinstall.py -install\n    If you installed pywin32 via a .exe installer, this should be run\n    automatically after installation, but if it fails you can run it again.\n    If you installed pywin32 via PIP, you almost certainly need to run this to\n    setup the environment correctly.",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "tee_f",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "tee_f = open(os.path.join(tempfile.gettempdir(), \"pywin32_postinstall.log\"), \"w\")\nclass Tee:\n    def __init__(self, file):\n        self.f = file\n    def write(self, what):\n        if self.f is not None:\n            try:\n                self.f.write(what.replace(\"\\n\", \"\\r\\n\"))\n            except OSError:\n                pass",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "sys.stderr",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "sys.stderr = Tee(sys.stderr)\nsys.stdout = Tee(sys.stdout)\ncom_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "sys.stdout",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "sys.stdout = Tee(sys.stdout)\ncom_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'\nsilent = 0",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "com_modules",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "com_modules = [\n    # module_name,                      class_names\n    (\"win32com.servers.interp\", \"Interpreter\"),\n    (\"win32com.servers.dictionary\", \"DictionaryPolicy\"),\n    (\"win32com.axscript.client.pyscript\", \"PyScript\"),\n]\n# Is this a 'silent' install - ie, avoid all dialogs.\n# Different than 'verbose'\nsilent = 0\n# Verbosity of output messages.",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "silent",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "silent = 0\n# Verbosity of output messages.\nverbose = 1\nroot_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ntry:\n    # When this script is run from inside the bdist_wininst installer,\n    # file_created() and directory_created() are additional builtin\n    # functions which write lines to PythonXX\\pywin32-install.log. This is\n    # a list of actions for the uninstaller, the format is inspired by what\n    # the Wise installer also creates.",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "verbose",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "verbose = 1\nroot_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ntry:\n    # When this script is run from inside the bdist_wininst installer,\n    # file_created() and directory_created() are additional builtin\n    # functions which write lines to PythonXX\\pywin32-install.log. This is\n    # a list of actions for the uninstaller, the format is inspired by what\n    # the Wise installer also creates.\n    file_created  # type: ignore[used-before-def]\n    # 3.10 stopped supporting bdist_wininst, but we can still build them with 3.9.",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "root_key_name",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_postinstall",
        "description": ".venv.Scripts.pywin32_postinstall",
        "peekOfCode": "root_key_name = \"Software\\\\Python\\\\PythonCore\\\\\" + sys.winver\ntry:\n    # When this script is run from inside the bdist_wininst installer,\n    # file_created() and directory_created() are additional builtin\n    # functions which write lines to PythonXX\\pywin32-install.log. This is\n    # a list of actions for the uninstaller, the format is inspired by what\n    # the Wise installer also creates.\n    file_created  # type: ignore[used-before-def]\n    # 3.10 stopped supporting bdist_wininst, but we can still build them with 3.9.\n    # This can be kept until Python 3.9 or exe installers support is dropped.",
        "detail": ".venv.Scripts.pywin32_postinstall",
        "documentation": {}
    },
    {
        "label": "run_test",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "def run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.\n    cmd = [sys.executable, \"-u\", scriptname] + cmdline_extras\n    print(\"--- Running '%s' ---\" % script)\n    sys.stdout.flush()\n    result = subprocess.run(cmd, check=False, cwd=dirname)\n    print(f\"*** Test script '{script}' exited with {result.returncode}\")\n    sys.stdout.flush()\n    if result.returncode:",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "find_and_run",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "def find_and_run(possible_locations, extras):\n    for maybe in possible_locations:\n        if os.path.isfile(maybe):\n            run_test(maybe, extras)\n            break\n    else:\n        raise RuntimeError(\n            \"Failed to locate a test script in one of %s\" % possible_locations\n        )\ndef main():",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "def main():\n    import argparse\n    code_directories = [this_dir] + site_packages\n    parser = argparse.ArgumentParser(\n        description=\"A script to trigger tests in all subprojects of PyWin32.\"\n    )\n    parser.add_argument(\n        \"-no-user-interaction\",\n        default=False,\n        action=\"store_true\",",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "this_dir",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "this_dir = os.path.dirname(__file__)\nsite_packages = [\n    site.getusersitepackages(),\n] + site.getsitepackages()\nfailures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "site_packages",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "site_packages = [\n    site.getusersitepackages(),\n] + site.getsitepackages()\nfailures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "failures",
        "kind": 5,
        "importPath": ".venv.Scripts.pywin32_testall",
        "description": ".venv.Scripts.pywin32_testall",
        "peekOfCode": "failures = []\n# Run a test using subprocess and wait for the result.\n# If we get an returncode != 0, we know that there was an error, but we don't\n# abort immediately - we run as many tests as we can.\ndef run_test(script, cmdline_extras):\n    dirname, scriptname = os.path.split(script)\n    # some tests prefer to be run from their directory.\n    cmd = [sys.executable, \"-u\", scriptname] + cmdline_extras\n    print(\"--- Running '%s' ---\" % script)\n    sys.stdout.flush()",
        "detail": ".venv.Scripts.pywin32_testall",
        "documentation": {}
    },
    {
        "label": "convert_audio_to_base64",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def convert_audio_to_base64(audio_segment: AudioSegment) -> str:\n    \"\"\"Convert an AudioSegment to a base64 encoded string.\"\"\"\n    buffer = io.BytesIO()\n    audio_segment.export(buffer, format=\"mp3\")\n    buffer.seek(0)\n    audio_bytes = buffer.read()\n    return base64.b64encode(audio_bytes).decode(\"utf-8\")\ndef prepare_story_data_for_html(story_data_dict: Dict) -> Dict:\n    \"\"\"Process the story data dictionary to include base64 encoded audio.\"\"\"\n    prepared_data = {}",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "prepare_story_data_for_html",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def prepare_story_data_for_html(story_data_dict: Dict) -> Dict:\n    \"\"\"Process the story data dictionary to include base64 encoded audio.\"\"\"\n    prepared_data = {}\n    for section_name, section_data in story_data_dict.items():\n        prepared_data[section_name] = {\n            \"dialogue\": section_data.get(\"dialogue\", []),\n            \"translated_dialogue\": section_data.get(\"translated_dialogue\", []),\n            \"translated_phrase_list\": section_data.get(\"translated_phrase_list\", []),\n            \"audio_data\": {\"phrases\": [], \"dialogue\": []},\n        }",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "create_html_story",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def create_html_story(\n    story_data_dict: Dict,\n    output_path: str,\n    component_path: str,\n    title: Optional[str] = None,\n    language: str = config.get_language_name(),\n) -> None:\n    \"\"\"\n    Create a standalone HTML file from the story data dictionary.\n    Args:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "generate_story_image",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def generate_story_image(story_plan):\n    \"\"\"\n    Generate an image for a story using Google Cloud Vertex AI's Image Generation API.\n    :param story_plan: A string containing the story plan\n    :param project_id: Your Google Cloud project ID\n    :param location: The location of your Vertex AI endpoint\n    :return: Image data as bytes\n    \"\"\"\n    # Initialize Vertex AI\n    vertexai.init(project=config.PROJECT_ID, location=config.VERTEX_REGION)",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "test_image_reading",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def test_image_reading(image_path):\n    \"\"\"\n    Test reading and basic image properties from a path\n    Args:\n        image_path: Path to the image file\n    Returns:\n        Tuple of (width, height) if successful\n    \"\"\"\n    try:\n        # Open image in binary mode, not text mode",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "add_image_paths",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def add_image_paths(story_dict: Dict[str, Any], image_dir: str) -> Dict[str, Any]:\n    \"\"\"\n    Add image paths to the story dictionary based on the English phrases.\n    Args:\n        story_dict: Dictionary containing story data with translated_phrase_list\n        image_dir: Directory containing the images\n    Returns:\n        Updated dictionary with image_path added for each story part\n    Note:\n        For each story part, expects translated_phrase_list to be a list of tuples",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "add_images_to_phrases",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def add_images_to_phrases(\n    phrases: List[str],\n    output_dir: str,\n    image_format: str = \"png\",\n    imagen_model: Literal[\n        \"imagen-3.0-fast-generate-001\", \"imagen-3.0-generate-001\"\n    ] = \"imagen-3.0-generate-001\",\n    anthropic_model=config.ANTHROPIC_MODEL_NAME,\n) -> Dict:\n    \"\"\"",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "clean_filename",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def clean_filename(phrase: str) -> str:\n    \"\"\"Convert a phrase to a clean filename-safe string.\"\"\"\n    # Convert to lowercase\n    clean = phrase.lower()\n    # Replace any non-alphanumeric characters (except spaces) with empty string\n    clean = re.sub(r\"[^a-z0-9\\s]\", \"\", clean)\n    # Replace spaces with underscores\n    clean = clean.replace(\" \", \"_\")\n    # Remove any double underscores\n    clean = re.sub(r\"_+\", \"_\", clean)",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "string_to_large_int",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def string_to_large_int(s: str) -> int:\n    # Encode the string to bytes\n    encoded = s.encode(\"utf-8\")\n    # Create a SHA-256 hash\n    hash_object = hashlib.sha256(encoded)\n    # Get the hexadecimal representation\n    hex_dig = hash_object.hexdigest()\n    # Take the first 16 characters (64 bits) of the hex string\n    truncated_hex = hex_dig[:16]\n    # Convert hex to integer",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "create_image_generation_prompt",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def create_image_generation_prompt(phrase, anthropic_model: str = None):\n    \"\"\"\n    Create a specific image generation prompt based on a language learning phrase.\n    :param phrase: The language learning phrase to visualize\n    :return: A specific prompt for image generation\n    \"\"\"\n    llm_prompt = f\"\"\"\n    Given the following phrase for language learners: \"{phrase}\"\n    Create a specific, detailed prompt for generating an image that will help learners remember this phrase.\n    Focus on key nouns, verbs, or concepts that can be visually represented.",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "generate_image_deepai",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def generate_image_deepai(\n    prompt: str,\n    width: Union[str, int] = \"512\",\n    height: Union[str, int] = \"512\",\n    model: Literal[\"standard\", \"hd\"] = \"hd\",\n    negative_prompt: Optional[str] = None,\n) -> Image.Image:\n    \"\"\"\n    Generate an image using DeepAI's text2img API and return it as a PIL Image object.\n    Args:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "generate_image_stability",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def generate_image_stability(\n    prompt: str,\n    negative_prompt: str = \"\",\n    style_preset: Optional[\n        Literal[\n            \"3d-model\",\n            \"analog-film\",\n            \"anime\",\n            \"cinematic\",\n            \"comic-book\",",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "generate_image_imagen",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def generate_image_imagen(\n    prompt: str,\n    model: Literal[\n        \"imagen-3.0-fast-generate-001\", \"imagen-3.0-generate-001\"\n    ] = \"imagen-3.0-generate-001\",\n) -> Optional[Image.Image]:\n    \"\"\"\n    Generate an image using the Vertex AI Imagen model with retry logic.\n    Args:\n        prompt: The text prompt to generate the image from",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "resize_image",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def resize_image(generated_image, height=500, width=500):\n    # Get the image bytes directly\n    image_data = generated_image._image_bytes\n    # Convert the image to PIL Image for potential resizing\n    image = Image.open(io.BytesIO(image_data))\n    # Resize the image if it's not 500x500\n    if image.size != (500, 500):\n        image = image.resize((500, 500))\n        # If we resized, convert the resized image back to bytes\n        img_byte_arr = io.BytesIO()",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "create_test_story_dict",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def create_test_story_dict(\n    story_data_dict: Dict[str, Dict],\n    story_parts: int = 2,\n    phrases: int = 2,\n    from_index: int = 0,\n) -> Dict[str, Dict]:\n    \"\"\"\n    Create a smaller version of the story_data_dict for testing purposes.\n    Args:\n    story_data_dict (Dict[str, Dict]): The original story data dictionary.",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "ensure_spacy_model",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def ensure_spacy_model(model_name=\"en_core_web_md\"):\n    try:\n        spacy.load(model_name)\n    except OSError:\n        print(f\"Downloading spaCy model {model_name}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", model_name])\ndef extract_vocab_and_pos(english_phrases: List[str]) -> List[Tuple[str, str]]:\n    \"\"\"Returns the (lemma and POS) for feeding into update_vocab_usage, as a list.\"\"\"\n    # Process vocabulary\n    ensure_spacy_model()",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "extract_vocab_and_pos",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def extract_vocab_and_pos(english_phrases: List[str]) -> List[Tuple[str, str]]:\n    \"\"\"Returns the (lemma and POS) for feeding into update_vocab_usage, as a list.\"\"\"\n    # Process vocabulary\n    ensure_spacy_model()\n    nlp = spacy.load(\"en_core_web_md\")\n    vocab_set = set()\n    excluded_names = {\"sam\", \"alex\"}\n    for phrase in english_phrases:\n        doc = nlp(phrase)\n        for token in doc:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "extract_substring_matches",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def extract_substring_matches(\n    new_phrases: List[str], target_phrases: Set[str]\n) -> Set[str]:\n    # Convert all new phrases to lowercase\n    lowercase_phrases = [phrase.lower() for phrase in new_phrases]\n    # Convert all target phrases to lowercase\n    lowercase_targets = [target.lower() for target in target_phrases]\n    # Initialize a set to store matched substrings\n    matched_substrings = set()\n    # Check each target phrase against each new phrase",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "extract_spacy_lowercase_words",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def extract_spacy_lowercase_words(new_phrases: List[str]) -> Set[str]:\n    # Ensure the spaCy model is loaded\n    nlp = spacy.load(\"en_core_web_sm\")\n    # Initialize an empty set to store unique lowercase words\n    lowercase_words = set()\n    # Process each phrase with spaCy\n    for phrase in new_phrases:\n        doc = nlp(phrase)\n        # Add the lowercase version of each token's text to the set\n        lowercase_words.update(token.text.lower() for token in doc)",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_verb_and_vocab_lists",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def get_verb_and_vocab_lists(used_words: Set[Tuple[str, str]]) -> Dict[str, List[str]]:\n    \"\"\"\n    Separate the input set of (word, POS) tuples into verb and vocabulary lists.\n    Args:\n    used_words (Set[Tuple[str, str]]): A set of tuples containing (word, POS)\n    Returns:\n    Dict[str, List[str]]: A dictionary with 'verbs' and 'vocab' lists\n    \"\"\"\n    verb_list = []\n    vocab_list = []",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "update_vocab_usage",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def update_vocab_usage(used_words: Set[Tuple[str, str]], update_amount: int = 1):\n    \"\"\"Taking a list of (word, word_type) e.g. ('can', 'verbs') we update the vocab_usage\n    list, if the word doesn't exist we add it to list. This is used for sampling vocab for subsequent\n    lessons. words that haven't been used have a higher chance of being sampled.\n    No return statement\"\"\"\n    # Load the current usage\n    vocab_usage = load_json(config.VOCAB_USAGE_PATH)\n    # Update the usage count for each used word\n    for word, pos in used_words:\n        if pos in [\"VERB\", \"AUX\"]:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "convert_defaultdict",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def convert_defaultdict(d):\n    if isinstance(d, defaultdict):\n        d = {k: convert_defaultdict(v) for k, v in d.items()}\n    return d\ndef save_defaultdict(d, filepath):\n    normal_dict = convert_defaultdict(d)\n    save_json(normal_dict, filepath)\ndef filter_longman_words(\n    data: List[Dict], category: Literal[\"S1\", \"S2\", \"S3\", \"W1\", \"W2\", \"W3\"]\n) -> Dict[str, List[str]]:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "save_defaultdict",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def save_defaultdict(d, filepath):\n    normal_dict = convert_defaultdict(d)\n    save_json(normal_dict, filepath)\ndef filter_longman_words(\n    data: List[Dict], category: Literal[\"S1\", \"S2\", \"S3\", \"W1\", \"W2\", \"W3\"]\n) -> Dict[str, List[str]]:\n    \"\"\"This will only work with the specific format of longman data in a nested JSON structure from: https://github.com/healthypackrat/longman-communication-3000.\n    S1 means part of the first 1000 vocab list for speech, W3 means part of the 3000 words (i.e. the third '1000' chunk) for writing\n    \"\"\"\n    s1_words = defaultdict(list)",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "filter_longman_words",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def filter_longman_words(\n    data: List[Dict], category: Literal[\"S1\", \"S2\", \"S3\", \"W1\", \"W2\", \"W3\"]\n) -> Dict[str, List[str]]:\n    \"\"\"This will only work with the specific format of longman data in a nested JSON structure from: https://github.com/healthypackrat/longman-communication-3000.\n    S1 means part of the first 1000 vocab list for speech, W3 means part of the 3000 words (i.e. the third '1000' chunk) for writing\n    \"\"\"\n    s1_words = defaultdict(list)\n    for entry in data:\n        if category in entry.get(\"frequencies\", []):\n            for word_class in entry.get(\"word_classes\", []):",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_longman_verb_vocab_dict",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def get_longman_verb_vocab_dict(\n    longman_file_path, category: Literal[\"S1\", \"S2\", \"S3\", \"W1\", \"W2\", \"W3\"]\n) -> Dict[str, List[str]]:\n    \"\"\"Returns a vocabulary dict with keys 'verbs' and 'vocab' for verbs and all other parts-of-speech. This is now in the\n    same format as the known_vocab_list.json as used in the rest of the code.\"\"\"\n    data = load_json(longman_file_path)\n    category_words = filter_longman_words(data, category=category)\n    words_dict = defaultdict(list)\n    for pos in category_words.keys():\n        if pos in [\"v\", \"auxillary\"]:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "load_json",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def load_json(file_path):\n    with open(file_path, \"r\") as file:\n        return json.load(file)\ndef save_json(data, file_path):\n    with open(file_path, \"w\") as file:\n        json.dump(data, file, indent=2)\n    # print(f\"Data saved to {file_path}\")\ndef get_caller_name():\n    \"\"\"Method 1: Using inspect.stack()\"\"\"\n    # Get the frame 2 levels up (1 would be this function, 2 is the caller)",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "save_json",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def save_json(data, file_path):\n    with open(file_path, \"w\") as file:\n        json.dump(data, file, indent=2)\n    # print(f\"Data saved to {file_path}\")\ndef get_caller_name():\n    \"\"\"Method 1: Using inspect.stack()\"\"\"\n    # Get the frame 2 levels up (1 would be this function, 2 is the caller)\n    caller_frame = inspect.stack()[2]\n    return caller_frame.function\ndef ok_to_query_api() -> bool:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_caller_name",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def get_caller_name():\n    \"\"\"Method 1: Using inspect.stack()\"\"\"\n    # Get the frame 2 levels up (1 would be this function, 2 is the caller)\n    caller_frame = inspect.stack()[2]\n    return caller_frame.function\ndef ok_to_query_api() -> bool:\n    \"\"\"Check if enough time has passed since the last API call.\n    If not enough time has passed, wait for the remaining time.\n    Returns:\n        bool: True when it's ok to proceed with the API call",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "ok_to_query_api",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def ok_to_query_api() -> bool:\n    \"\"\"Check if enough time has passed since the last API call.\n    If not enough time has passed, wait for the remaining time.\n    Returns:\n        bool: True when it's ok to proceed with the API call\n    \"\"\"\n    time_since_last_call = config.get_time_since_last_api_call()\n    if time_since_last_call >= config.API_DELAY_SECONDS:\n        config.update_api_timestamp()\n        return True",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "anthropic_generate",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def anthropic_generate(prompt: str, max_tokens: int = 1024, model: str = None) -> str:\n    \"\"\"given a prompt generates an LLM response. The default model is specified in the config file.\n    Most likely the largest Anthropic model. The region paramater in the config will have to match where that model\n    is available\"\"\"\n    print(\n        f\"Function that called this one: {get_caller_name()}. Sleeping for 20 seconds\"\n    )\n    ok_to_query_api()\n    client = AnthropicVertex(region=config.ANTHROPIC_REGION, project_id=PROJECT_ID)\n    if model is None:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "extract_json_from_llm_response",
        "kind": 2,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "def extract_json_from_llm_response(response):\n    \"\"\"\n    Extract JSON from an LLM response.\n    :param response: String containing the LLM's response\n    :return: Extracted JSON as a Python object, or None if no valid JSON is found\n    \"\"\"\n    # Try to find JSON-like structure in the response\n    json_pattern = (\n        r\"\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}))*\\}\"\n    )",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "PROJECT_ID",
        "kind": 5,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "PROJECT_ID = os.getenv(\"GOOGLE_PROJECT_ID\")\nSTABILITY_API_KEY = os.getenv(\"STABILITY_API_KEY\")\ndef convert_audio_to_base64(audio_segment: AudioSegment) -> str:\n    \"\"\"Convert an AudioSegment to a base64 encoded string.\"\"\"\n    buffer = io.BytesIO()\n    audio_segment.export(buffer, format=\"mp3\")\n    buffer.seek(0)\n    audio_bytes = buffer.read()\n    return base64.b64encode(audio_bytes).decode(\"utf-8\")\ndef prepare_story_data_for_html(story_data_dict: Dict) -> Dict:",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "STABILITY_API_KEY",
        "kind": 5,
        "importPath": "src..ipynb_checkpoints.utils-checkpoint",
        "description": "src..ipynb_checkpoints.utils-checkpoint",
        "peekOfCode": "STABILITY_API_KEY = os.getenv(\"STABILITY_API_KEY\")\ndef convert_audio_to_base64(audio_segment: AudioSegment) -> str:\n    \"\"\"Convert an AudioSegment to a base64 encoded string.\"\"\"\n    buffer = io.BytesIO()\n    audio_segment.export(buffer, format=\"mp3\")\n    buffer.seek(0)\n    audio_bytes = buffer.read()\n    return base64.b64encode(audio_bytes).decode(\"utf-8\")\ndef prepare_story_data_for_html(story_data_dict: Dict) -> Dict:\n    \"\"\"Process the story data dictionary to include base64 encoded audio.\"\"\"",
        "detail": "src..ipynb_checkpoints.utils-checkpoint",
        "documentation": {}
    },
    {
        "label": "AnkiCollectionReader",
        "kind": 6,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "class AnkiCollectionReader:\n    def __init__(self, collection_path: str = None):\n        self.collection_path = os.getenv(\"ANKI_COLLECTION_PATH\", collection_path)\n        if not os.path.exists(self.collection_path):\n            raise FileNotFoundError(\n                f\"Anki collection not found at: {self.collection_path}\"\n            )\n        self.col: Optional[Collection] = None\n    def connect(self):\n        self.col = Collection(self.collection_path)",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "convert_anki_to_story_dict",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def convert_anki_to_story_dict(collection_path: str, deck_name: str) -> Dict[str, Dict]:\n    \"\"\"\n    Read an Anki deck and convert it to the story_data_dict format used by export_anki_with_images\n    Args:\n        collection_path: Path to the .anki2 collection file\n        deck_name: Name of the deck to convert\n    Returns:\n        Dictionary in the story_data_dict format with phrases and audio\n    \"\"\"\n    story_data_dict = defaultdict(lambda: defaultdict(list))",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "extract_audio_filename",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def extract_audio_filename(field_value: str) -> str:\n    \"\"\"Extract the audio filename from an Anki field value containing [sound:] tags\"\"\"\n    match = re.search(r\"\\[sound:(.*?)\\]\", field_value)\n    if match:\n        return match.group(1)\n    return None\ndef get_anki_path() -> str:\n    \"\"\"\n    Get Anki collection path from environment variable with error checking.\n    Returns:",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "get_anki_path",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def get_anki_path() -> str:\n    \"\"\"\n    Get Anki collection path from environment variable with error checking.\n    Returns:\n        str: Path to Anki collection\n    Raises:\n        EnvironmentError: If ANKI_COLLECTION_PATH not set or file doesn't exist\n    \"\"\"\n    load_dotenv()\n    path = os.getenv(\"ANKI_COLLECTION_PATH\")",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "validate_anki_tag",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def validate_anki_tag(tag: str) -> bool:\n    \"\"\"\n    Validate if a tag is acceptable for Anki.\n    Args:\n        tag: String to validate as an Anki tag\n    Returns:\n        bool: True if tag is valid, False otherwise\n    \"\"\"\n    if not tag or \" \" in tag:\n        return False",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "append_tag_to_note",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def append_tag_to_note(existing_tags: str, new_tag: str) -> str:\n    \"\"\"\n    Append a new tag to existing space-separated tags if not already present.\n    Args:\n        existing_tags: Space-separated string of existing tags\n        new_tag: Tag to append\n    Returns:\n        str: Updated space-separated tags string\n    \"\"\"\n    tags = set(existing_tags.split()) if existing_tags else set()",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "add_tag_to_matching_notes",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def add_tag_to_matching_notes(\n    phrases: List[str], tag: str, deck_name: str, collection_path: Optional[str] = None\n) -> Tuple[int, List[str]]:\n    \"\"\"\n    Add tag to notes where EnglishText matches any of the provided phrases.\n    Uses ANKI_COLLECTION_PATH from environment if collection_path not provided.\n    Args:\n        phrases: List of English phrases to match against\n        tag: Tag to add to matching notes\n        deck_name: Name of the deck to process",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "print_deck_info",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def print_deck_info(collection_path: str):\n    with AnkiCollectionReader(collection_path) as reader:\n        # Print all decks\n        print(\"\\nAvailable decks:\")\n        print(\"-\" * 50)\n        for deck_id, deck_name in reader.get_deck_names().items():\n            print(f\"Deck ID: {deck_id}\")\n            print(f\"Deck Name: {deck_name}\")\n            print(\"-\" * 50)\n        # Print media info",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "get_deck_contents",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def get_deck_contents(\n    deck_name: str,\n    collection_path: Optional[str] = None,\n    fields_to_extract: Optional[list[str]] = None,\n    include_stats: bool = True,\n) -> pd.DataFrame:\n    \"\"\"\n    Get contents of a specific Anki deck as a pandas DataFrame.\n    Args:\n        deck_name: Name of the deck to analyze",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "calculate_knowledge_score",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def calculate_knowledge_score(row: pd.Series) -> float:\n    \"\"\"\n    Calculate a knowledge score (0-1) for a single card based on its statistics.\n    Components:\n    - Interval (40%): max 365 days\n    - Ease (20%): range 1300-3100\n    - Success (30%): based on lapses vs reps\n    - Efficiency (10%): interval gained per rep\n    Args:\n        row: Series containing card statistics with columns:",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "add_knowledge_score",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def add_knowledge_score(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Add knowledge score column to DataFrame using apply.\"\"\"\n    result_df = df.copy()\n    result_df[\"knowledge_score\"] = df.apply(calculate_knowledge_score, axis=1)\n    return result_df\ndef export_to_anki_with_images_english(\n    english_phrases: List[str],\n    output_dir: str,\n    image_dir: str,\n    audio_dir: str,",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "export_to_anki_with_images_english",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def export_to_anki_with_images_english(\n    english_phrases: List[str],\n    output_dir: str,\n    image_dir: str,\n    audio_dir: str,\n    story_name: str,\n    deck_name: str = None,\n):\n    \"\"\"\n    Export English learning flashcards to an Anki deck, with images and audio.",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "create_anki_deck_from_english_phrase_list",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def create_anki_deck_from_english_phrase_list(\n    phrase_list: List[str],\n    deck_name: str,\n    anki_filename_prefix: str,\n    image_dir: str,\n    batch_size: int = 50,\n    output_dir=\"../outputs/longman\",\n):\n    \"\"\"Takes a list of english phrases and does: 1) translation 2) text to speech 3) export to Anki deck.\n    To avoid overloading the text-to-speech APIs it will batch up the phrases into smaller decks (*.apkg), but these will all have the same 'deck_id'",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "generate_wiktionary_links_non_english",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def generate_wiktionary_links_non_english(\n    phrase: str, native_language_code: str = \"uk\"\n) -> str:\n    \"\"\"\n    Generate Wiktionary links for native speakers of other languages learning English.\n    Similar format to the original generate_wiktionary_links function.\n    Args:\n        phrase: The English phrase to generate links for\n        native_language_code: The two-letter language code (e.g., 'uk' for Ukrainian)\n    Returns:",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "get_wiktionary_language_name",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def get_wiktionary_language_name(language_name: str) -> str:\n    \"\"\"Map standard language names to Wiktionary header names\"\"\"\n    wiktionary_mapping = {\n        \"Mandarin Chinese\": \"Chinese\",\n        \"Modern Greek\": \"Greek\",\n        \"Standard Arabic\": \"Arabic\",\n        \"Brazilian Portuguese\": \"Portuguese\",\n        # Add more mappings as discovered\n    }\n    return wiktionary_mapping.get(language_name, language_name)",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "find_language_section",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def find_language_section(soup: BeautifulSoup, language_name: str) -> Optional[str]:\n    \"\"\"Try different strategies to find the language section\"\"\"\n    # Try exact match with mapping\n    wiktionary_name = get_wiktionary_language_name(language_name)\n    if section := soup.find(\"h2\", {\"id\": wiktionary_name}):\n        return wiktionary_name\n    # Try words in reverse order (longest to shortest)\n    words = language_name.split()\n    for i in range(len(words), 0, -1):\n        partial_name = \" \".join(words[:i])",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "generate_wiktionary_links",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def generate_wiktionary_links(\n    phrase: str,\n    language_name: str = None,\n    language_code: str = None,\n) -> str:\n    if language_name is None:\n        language_name = config.TARGET_LANGUAGE_NAME\n    if language_code is None:\n        language_code = config.TARGET_LANGUAGE_CODE\n    words = tokenize_text(text=phrase, language_code=language_code)",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "load_template",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def load_template(filename):\n    # print(os.listdir())\n    filename = os.path.join(\"..\", \"src\", filename)\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        return f.read()\ndef export_to_anki_with_images(\n    story_data_dict: Dict[str, Dict],\n    output_dir: str,\n    story_name: str,\n    deck_name: str = None,",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "export_to_anki_with_images",
        "kind": 2,
        "importPath": "src.anki_tools",
        "description": "src.anki_tools",
        "peekOfCode": "def export_to_anki_with_images(\n    story_data_dict: Dict[str, Dict],\n    output_dir: str,\n    story_name: str,\n    deck_name: str = None,\n):\n    \"\"\"\n    Export story data to an Anki deck, including images for each card. Use add_image_paths\n    with story_data_dict first to get image data.\n    The story_name is used as a prefix for the anki file only",
        "detail": "src.anki_tools",
        "documentation": {}
    },
    {
        "label": "generate_translated_phrase_audio",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def generate_translated_phrase_audio(\n    translated_phrases: List[Tuple[str, str]], source_language_audio: bool = False\n) -> List[List[AudioSegment]]:\n    \"\"\"\n    Generate audio for a list of translated phrases.\n    Args:\n        translated_phrases: List of tuples (original_text, translated_text)\n        english_voice_models: Configuration for English TTS voices\n        target_voice_models: Configuration for target language TTS voices\n    Returns:",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "clean_tts_text",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def clean_tts_text(text: str) -> str:\n    \"\"\"\n    Clean and prepare text for TTS processing by:\n    1. Decoding HTML entities\n    2. Handling any special characters or formatting\n    Args:\n        text: Input text that may contain HTML entities or special characters\n    Returns:\n        Cleaned text ready for TTS processing\n    \"\"\"",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "clean_translated_content",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def clean_translated_content(\n    content: Union[str, Tuple[str, str], List[Dict[str, str]]]\n) -> Union[str, Tuple[str, str], List[Dict[str, str]]]:\n    \"\"\"\n    Clean translated content in various formats:\n    - Single string\n    - Tuple of (original, translated)\n    - List of dialogue dictionaries\n    Args:\n        content: Translated content in various formats",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "generate_phrase_english_audio_files",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def generate_phrase_english_audio_files(phrases: List[str], output_dir: str) -> None:\n    \"\"\"\n    Generate slow and normal English-only speed MP3 files for each phrase and save them to output_dir.\n    Args:\n        phrases: List of English phrases to convert to audio\n        output_dir: Directory where the MP3 files will be saved\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    for phrase in tqdm(phrases):",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "setup_ffmpeg",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def setup_ffmpeg():\n    ffmpeg_path = r\"C:\\Program Files\\ffmpeg-7.0-essentials_build\\bin\"\n    if os.path.exists(ffmpeg_path):\n        # Add FFmpeg to the PATH\n        os.environ[\"PATH\"] += os.pathsep + ffmpeg_path\n        print(f\"FFmpeg path added to system PATH: {ffmpeg_path}\")\n    else:\n        print(f\"FFmpeg path not found: {ffmpeg_path}\")\n        print(\"Please check the installation directory.\")\nsetup_ffmpeg()",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "text_to_speech_google",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def text_to_speech_google(\n    text: str,\n    voice_model: VoiceInfo,\n    speaking_rate: float = 1.0,\n    is_ssml: bool = False,\n) -> AudioSegment:\n    \"\"\"\n    Convert text to speech using Google Cloud TTS.\n    Args:\n        text: Text or SSML to convert to speech",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "text_to_speech_azure",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def text_to_speech_azure(\n    text: str,\n    voice_model: VoiceInfo,\n    speaking_rate: float = 1.0,\n    is_ssml: bool = False,\n) -> AudioSegment:\n    \"\"\"\n    Convert text to speech using Azure Speech Service.\n    Args:\n        text: Text or SSML to convert to speech",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "text_to_speech",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def text_to_speech(\n    text: str,\n    config_language: Literal[\"source\", \"target\"] = \"source\",\n    gender: Literal[\"MALE\", \"FEMALE\"] = \"MALE\",\n    speaking_rate: float = 1.0,\n    is_ssml: bool = False,\n) -> AudioSegment:\n    \"\"\"\n    Wrapper that handles diveriting to Azure or Google depending on the settings in the config file\n    for language, which then cause the voice models to be either Azure or Google ones.",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "slow_text_to_speech",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def slow_text_to_speech(\n    text: str,\n    config_language: Literal[\"source\", \"target\"] = \"source\",\n    gender: Literal[\"MALE\", \"FEMALE\"] = \"MALE\",\n    speaking_rate: float = None,\n    word_break_ms: int = None,\n) -> AudioSegment:\n    \"\"\"\n    Generate slowed down text-to-speech audio with breaks between words using SSML.\n    Args:",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "export_audio",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def export_audio(final_audio: AudioSegment, filename: str = None) -> str:\n    \"\"\"\n    Saves the final_audio AudioSegment as an MP3 file.\n    Args:\n        final_audio (AudioSegment): The audio to be saved.\n        filename (str, optional): The filename to save the audio as. If not provided, a UUID will be used.\n    Returns:\n        str: The filename of the saved audio file.\n    \"\"\"\n    if filename is None:",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "play_audio",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def play_audio(segment: AudioSegment, filename: str = None, autoplay: bool = False):\n    \"\"\"\n    Plays an MP3 clip in the Jupyter notebook.\n    Args:\n        segment (AudioSegment): The audio segment to play.\n        filename (str, optional): The filename to save the audio as. If not provided, a temporary file will be created.\n        autoplay (bool, optional): Whether to autoplay the audio. Defaults to False.\n    Returns:\n        IPython.display.Audio: An IPython Audio widget for playing the audio.\n    \"\"\"",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "join_audio_segments",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def join_audio_segments(audio_segments: list[AudioSegment], gap_ms=100) -> AudioSegment:\n    \"\"\"Joins audio segments together with a tiny gap between each one in ms\n    Returns a single joined up audio segment\"\"\"\n    gap_audio = AudioSegment.silent(duration=gap_ms)\n    audio_with_gap = [audio_seg + gap_audio for audio_seg in audio_segments]\n    return sum(audio_with_gap)\ndef speed_up_audio(\n    audio_segment: AudioSegment, speed_factor: float = 2.0\n) -> AudioSegment:\n    \"\"\"",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "speed_up_audio",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def speed_up_audio(\n    audio_segment: AudioSegment, speed_factor: float = 2.0\n) -> AudioSegment:\n    \"\"\"\n    Speed up an AudioSegment without changing its pitch, with added de-reverb.\n    :param audio_segment: Input AudioSegment\n    :param speed_factor: Factor by which to speed up the audio (e.g., 2.0 for double speed)\n    :return: Sped up AudioSegment\n    \"\"\"\n    # Convert AudioSegment to numpy array",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "generate_normal_and_fast_audio",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def generate_normal_and_fast_audio(\n    audio_segments: List[AudioSegment],\n) -> Tuple[AudioSegment, AudioSegment]:\n    \"\"\"\n    Generate normal speed and (10 copies) of a fast version of the dialogue. Designed to be\n    called after generate_audio_from_dialogue as that func returns a list of audio segments\n    :param audio_segments: List of AudioSegment objects representing each utterance\n    :return: A tuple containing (normal_speed_audio, fast_speed_audio)\n    \"\"\"\n    normal_speed = join_audio_segments(audio_segments, gap_ms=500)",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "generate_audio_from_dialogue",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def generate_audio_from_dialogue(\n    dialogue: List[Dict[str, str]],\n    config_language: Literal[\"source\", \"target\"] = \"target\",\n) -> List[AudioSegment]:\n    \"\"\"\n    Generate audio from dialogue using sequential processing.\n    Typicall only generated in the target language\n    Args:\n        dialogue: List of dialogue utterances\n        in_target_language: Whether to use target language voices",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "create_m4a_with_timed_lyrics",
        "kind": 2,
        "importPath": "src.audio_generation",
        "description": "src.audio_generation",
        "peekOfCode": "def create_m4a_with_timed_lyrics(\n    audio_segments: List[AudioSegment],\n    phrases: List[str],\n    output_file: str,\n    album_name: str,\n    track_title: str,\n    track_number: int,\n    total_tracks: int = 6,\n    cover_image: Optional[Image.Image] = None,\n) -> None:",
        "detail": "src.audio_generation",
        "documentation": {}
    },
    {
        "label": "VoiceProvider",
        "kind": 6,
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "peekOfCode": "class VoiceProvider(Enum):\n    GOOGLE = \"google\"\n    AZURE = \"azure\"\n    NONE = \"none\"  # Added for when no provider is available\nclass VoiceType(Enum):\n    STANDARD = \"standard\"\n    WAVENET = \"wavenet\"\n    NEURAL2 = \"neural2\"\n    NEURAL = \"neural\"\n    STUDIO = \"studio\"",
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "VoiceType",
        "kind": 6,
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "peekOfCode": "class VoiceType(Enum):\n    STANDARD = \"standard\"\n    WAVENET = \"wavenet\"\n    NEURAL2 = \"neural2\"\n    NEURAL = \"neural\"\n    STUDIO = \"studio\"\n    JOURNEY = \"journey\"\n    NONE = \"none\"  # Added for when no voice type is available\n@dataclass\nclass VoiceInfo:",
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "VoiceInfo",
        "kind": 6,
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "peekOfCode": "class VoiceInfo:\n    name: str\n    provider: VoiceProvider\n    voice_type: VoiceType\n    gender: str\n    language_code: str\n    country_code: str\n    voice_id: str\nclass VoiceManager:\n    \"\"\"Manages voice selection with lazy loading of voice data.",
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "VoiceManager",
        "kind": 6,
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "peekOfCode": "class VoiceManager:\n    \"\"\"Manages voice selection with lazy loading of voice data.\n    Terminology:    language_code = language-country pair like fr-FR or en-GB\n                    language_alpha = the language ISO code - usually 2 ALPHA standard but can be 3 (e.g. fr, en)\n                    country_code = the 2 ALPHA country code (GB, FR)\"\"\"\n    def __init__(self):\n        self.voices: Dict[str, List[VoiceInfo]] = {}\n        self.voice_type_ranking = [\n            VoiceType.STUDIO,\n            VoiceType.NEURAL2,",
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "ConfigLoader",
        "kind": 6,
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "peekOfCode": "class ConfigLoader:\n    def __init__(self, config_file=\"config.json\"):\n        \"\"\"Initialize with explicit object attributes\"\"\"\n        self.script_dir = os.path.dirname(os.path.abspath(__file__))\n        self.config_file = os.path.join(self.script_dir, config_file)\n        self.config = SimpleNamespace()\n        self._last_load_time = 0\n        self._file_modified_time = 0\n        self.voice_manager = VoiceManager()  # No immediate voice loading\n        self.time_api_last_called = 0  # New attribute for API rate limiting",
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "src.config_loader",
        "description": "src.config_loader",
        "peekOfCode": "config = ConfigLoader()  # Delegate to config object",
        "detail": "src.config_loader",
        "documentation": {}
    },
    {
        "label": "get_story_prompt",
        "kind": 2,
        "importPath": "src.dialogue_generation",
        "description": "src.dialogue_generation",
        "peekOfCode": "def get_story_prompt(verbs: list, vocab: list) -> str:\n    prompt = f\"\"\"Create a three-part story told through dialogue between Alex and Sam. Each part should be about 2 minutes long when spoken (approximately 300 words or 20-25 dialogue exchanges).\nStory Requirements:\n1. The story must have a clear narrative arc across three parts:\n   - Introduction (2 mins): Set up the situation and characters\n   - Development (2 mins): Present a challenge or complication\n   - Resolution (2 mins): Resolve the situation\n2. Use vocabulary naturally from these lists:\n   Verbs: {', '.join(verbs)}\n   Other words: {', '.join(vocab)}",
        "detail": "src.dialogue_generation",
        "documentation": {}
    },
    {
        "label": "generate_story",
        "kind": 2,
        "importPath": "src.dialogue_generation",
        "description": "src.dialogue_generation",
        "peekOfCode": "def generate_story(vocab_dict: Dict[str, List[str]]) -> Dict:\n    \"\"\"Extract dialogue from an LLM response.\n    Returns a dictionary with story parts as keys and dialogue lists as values.\n    \"\"\"\n    prompt = get_story_prompt(verbs=vocab_dict[\"verbs\"], vocab=vocab_dict[\"vocab\"])\n    llm_response = anthropic_generate(prompt, max_tokens=4000)\n    extracted_json = extract_json_from_llm_response(llm_response)\n    if not extracted_json:\n        print(\"No valid dialogue found in the response\")\n        return None",
        "detail": "src.dialogue_generation",
        "documentation": {}
    },
    {
        "label": "add_usage_to_words",
        "kind": 2,
        "importPath": "src.dialogue_generation",
        "description": "src.dialogue_generation",
        "peekOfCode": "def add_usage_to_words(word_list: List[str], category: str) -> str:\n    \"\"\"adds the number of times that words has been used to the word_list - getting\n    this data from the vocab_usage file - this can then be issued as string into the prompt\n    \"\"\"\n    # Load the current usage\n    with open(config.VOCAB_USAGE_PATH, \"r\") as f:\n        vocab_usage = json.load(f)\n    # make word_list all lower case\n    word_list = [word.lower() for word in word_list]\n    # Check if the category exists in vocab_usage",
        "detail": "src.dialogue_generation",
        "documentation": {}
    },
    {
        "label": "add_translations",
        "kind": 2,
        "importPath": "src.generate",
        "description": "src.generate",
        "peekOfCode": "def add_translations(story_data_dict):\n    \"\"\"Translates all the phrases and dialogue to the target language\"\"\"\n    for story_part in tqdm(story_data_dict, desc=\"adding translations\"):\n        print(f\"Beginning translation for {story_part}\")\n        if \"dialogue\" in story_data_dict[story_part]:\n            dialogue = story_data_dict[story_part][\"dialogue\"]\n            translated_dialogue = translate_dialogue(dialogue)\n            print(\"Translated dialogue\")\n            story_data_dict[story_part][\"translated_dialogue\"] = translated_dialogue\n        corrected_phrase_list = story_data_dict[story_part].get(\"corrected_phrase_list\")",
        "detail": "src.generate",
        "documentation": {}
    },
    {
        "label": "add_audio",
        "kind": 2,
        "importPath": "src.generate",
        "description": "src.generate",
        "peekOfCode": "def add_audio(story_data_dict, source_language_audio: bool = False):\n    \"\"\"Adds text-to-speech for english and target language for all dialogue and\n    practice phrases\"\"\"\n    for story_part in tqdm(story_data_dict, desc=\"adding audio\"):\n        if \"translated_dialogue\" in story_data_dict[story_part]:\n            print(f\"Beginning text-to-speech for {story_part}\")\n            translated_dialogue_audio_segments = generate_audio_from_dialogue(\n                story_data_dict[story_part][\"translated_dialogue\"],\n                config_language=\"target\",\n            )",
        "detail": "src.generate",
        "documentation": {}
    },
    {
        "label": "create_image_generation_prompt_for_story_part",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def create_image_generation_prompt_for_story_part(\n    story_part: Union[Dict, List[Dict]], anthropic_model: str = None\n) -> str:\n    \"\"\"\n    Create an image generation prompt from a story part or list of story parts.\n    Args:\n        story_part: Either a dictionary containing a 'dialogue' key, or a list of such dictionaries.\n            Each dialogue entry should be a list of speaker/text pairs.\n        anthropic_model: Optional model name for Claude\n    Returns:",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "create_image_generation_prompt",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def create_image_generation_prompt(phrase, anthropic_model: str = None):\n    \"\"\"\n    Create a specific image generation prompt based on a language learning phrase.\n    :param phrase: The language learning phrase to visualize\n    :return: A specific prompt for image generation\n    \"\"\"\n    llm_prompt = f\"\"\"\n    Given the following phrase for language learners: \"{phrase}\"\n    Create a specific, detailed prompt for generating an image that will help learners remember this phrase.\n    Focus on key nouns, verbs, or concepts that can be visually represented.",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "generate_image_deepai",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def generate_image_deepai(\n    prompt: str,\n    width: Union[str, int] = \"512\",\n    height: Union[str, int] = \"512\",\n    model: Literal[\"standard\", \"hd\"] = \"hd\",\n    negative_prompt: Optional[str] = None,\n) -> Image.Image:\n    \"\"\"\n    Generate an image using DeepAI's text2img API and return it as a PIL Image object.\n    Args:",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "generate_image_stability",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def generate_image_stability(\n    prompt: str,\n    negative_prompt: str = \"\",\n    style_preset: Optional[\n        Literal[\n            \"3d-model\",\n            \"analog-film\",\n            \"anime\",\n            \"cinematic\",\n            \"comic-book\",",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "generate_image_imagen",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def generate_image_imagen(\n    prompt: str,\n    model: Literal[\n        \"imagen-3.0-fast-generate-001\", \"imagen-3.0-generate-001\"\n    ] = \"imagen-3.0-generate-001\",\n) -> Optional[Image.Image]:\n    \"\"\"\n    Generate an image using the Vertex AI Imagen model with retry logic.\n    Args:\n        prompt: The text prompt to generate the image from",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "resize_image",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def resize_image(generated_image, height=500, width=500):\n    # Get the image bytes directly\n    image_data = generated_image._image_bytes\n    # Convert the image to PIL Image for potential resizing\n    image = Image.open(io.BytesIO(image_data))\n    # Resize the image if it's not 500x500\n    if image.size != (height, width):\n        image = image.resize((height, width))\n        # If we resized, convert the resized image back to bytes\n        img_byte_arr = io.BytesIO()",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "add_images_to_phrases",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def add_images_to_phrases(\n    phrases: List[str],\n    output_dir: str,\n    image_format: str = \"png\",\n    imagen_model: Literal[\n        \"imagen-3.0-fast-generate-001\", \"imagen-3.0-generate-001\"\n    ] = \"imagen-3.0-generate-001\",\n    anthropic_model=config.ANTHROPIC_MODEL_NAME,\n) -> Dict:\n    \"\"\"",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "add_image_paths",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def add_image_paths(story_dict: Dict[str, Any], image_dir: str) -> Dict[str, Any]:\n    \"\"\"\n    Add image paths to the story dictionary based on the English phrases.\n    Args:\n        story_dict: Dictionary containing story data with translated_phrase_list\n        image_dir: Directory containing the images\n    Returns:\n        Updated dictionary with image_path added for each story part\n    Note:\n        For each story part, expects translated_phrase_list to be a list of tuples",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "generate_story_image",
        "kind": 2,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "def generate_story_image(story_dialogue: str):\n    \"\"\"\n    Generate an image for a story using Google Cloud Vertex AI's Image Generation API.\n    :param story_plan: A string containing the story plan\n    :param project_id: Your Google Cloud project ID\n    :param location: The location of your Vertex AI endpoint\n    :return: Image data as bytes\n    \"\"\"\n    # Initialize Vertex AI\n    vertexai.init(project=config.PROJECT_ID, location=config.VERTEX_REGION)",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "STABILITY_API_KEY",
        "kind": 5,
        "importPath": "src.images",
        "description": "src.images",
        "peekOfCode": "STABILITY_API_KEY = os.getenv(\"STABILITY_API_KEY\")\ndef create_image_generation_prompt_for_story_part(\n    story_part: Union[Dict, List[Dict]], anthropic_model: str = None\n) -> str:\n    \"\"\"\n    Create an image generation prompt from a story part or list of story parts.\n    Args:\n        story_part: Either a dictionary containing a 'dialogue' key, or a list of such dictionaries.\n            Each dialogue entry should be a list of speaker/text pairs.\n        anthropic_model: Optional model name for Claude",
        "detail": "src.images",
        "documentation": {}
    },
    {
        "label": "initialise_usage_data",
        "kind": 2,
        "importPath": "src.initialise",
        "description": "src.initialise",
        "peekOfCode": "def initialise_usage_data(overwrite=False):\n    \"\"\"Creates two usage JSON files and sets use counts to 0 for all words.\n    Only use once when setting up this project\"\"\"\n    # Check if files exist and only proceed if overwrite is True\n    if (\n        os.path.exists(config.GRAMMAR_USAGE_PATH)\n        or os.path.exists(config.VOCAB_USAGE_PATH)\n    ) and not overwrite:\n        print(\"Usage files already exist. Set overwrite=True to reinitialize.\")\n        return",
        "detail": "src.initialise",
        "documentation": {}
    },
    {
        "label": "initialise_grammar_usage",
        "kind": 2,
        "importPath": "src.initialise",
        "description": "src.initialise",
        "peekOfCode": "def initialise_grammar_usage():\n    concepts_data = load_json(config.GRAMMAR_CONCEPTS)\n    usage_data = {}\n    for category, items in concepts_data.items():\n        usage_data[category] = {}\n        for item in items:\n            usage_data[category][item[\"name\"]] = {\n                \"use\": True,\n                \"times_seen\": 0,\n                \"example\": item[\"example\"],",
        "detail": "src.initialise",
        "documentation": {}
    },
    {
        "label": "initialise_vocab_usage",
        "kind": 2,
        "importPath": "src.initialise",
        "description": "src.initialise",
        "peekOfCode": "def initialise_vocab_usage():\n    # Load the known vocabulary\n    known_vocab = load_json(config.VOCAB_LIST)\n    # Initialize the usage dictionary\n    vocab_usage = {\n        \"verbs\": {verb: 0 for verb in known_vocab[\"verbs\"]},\n        \"vocab\": {word: 0 for word in known_vocab[\"vocab\"]},\n    }\n    # Save the usage dictionary to a new JSON file\n    save_json(vocab_usage, config.VOCAB_USAGE_PATH)",
        "detail": "src.initialise",
        "documentation": {}
    },
    {
        "label": "CONCEPTS_FILE",
        "kind": 5,
        "importPath": "src.initialise",
        "description": "src.initialise",
        "peekOfCode": "CONCEPTS_FILE = \"../data/grammar_concepts.json\"\nCONCEPTS_USAGE_FILE = \"../data/grammar_concepts_usage.json\"\nVOCAB_LIST = \"../data/known_vocab_list.json\"\nVOCAB_USAGE_FILE = \"../data/vocab_usage.json\"\nDATA_DIR = \"../data\"\ndef initialise_usage_data(overwrite=False):\n    \"\"\"Creates two usage JSON files and sets use counts to 0 for all words.\n    Only use once when setting up this project\"\"\"\n    # Check if files exist and only proceed if overwrite is True\n    if (",
        "detail": "src.initialise",
        "documentation": {}
    },
    {
        "label": "CONCEPTS_USAGE_FILE",
        "kind": 5,
        "importPath": "src.initialise",
        "description": "src.initialise",
        "peekOfCode": "CONCEPTS_USAGE_FILE = \"../data/grammar_concepts_usage.json\"\nVOCAB_LIST = \"../data/known_vocab_list.json\"\nVOCAB_USAGE_FILE = \"../data/vocab_usage.json\"\nDATA_DIR = \"../data\"\ndef initialise_usage_data(overwrite=False):\n    \"\"\"Creates two usage JSON files and sets use counts to 0 for all words.\n    Only use once when setting up this project\"\"\"\n    # Check if files exist and only proceed if overwrite is True\n    if (\n        os.path.exists(config.GRAMMAR_USAGE_PATH)",
        "detail": "src.initialise",
        "documentation": {}
    },
    {
        "label": "VOCAB_LIST",
        "kind": 5,
        "importPath": "src.initialise",
        "description": "src.initialise",
        "peekOfCode": "VOCAB_LIST = \"../data/known_vocab_list.json\"\nVOCAB_USAGE_FILE = \"../data/vocab_usage.json\"\nDATA_DIR = \"../data\"\ndef initialise_usage_data(overwrite=False):\n    \"\"\"Creates two usage JSON files and sets use counts to 0 for all words.\n    Only use once when setting up this project\"\"\"\n    # Check if files exist and only proceed if overwrite is True\n    if (\n        os.path.exists(config.GRAMMAR_USAGE_PATH)\n        or os.path.exists(config.VOCAB_USAGE_PATH)",
        "detail": "src.initialise",
        "documentation": {}
    },
    {
        "label": "VOCAB_USAGE_FILE",
        "kind": 5,
        "importPath": "src.initialise",
        "description": "src.initialise",
        "peekOfCode": "VOCAB_USAGE_FILE = \"../data/vocab_usage.json\"\nDATA_DIR = \"../data\"\ndef initialise_usage_data(overwrite=False):\n    \"\"\"Creates two usage JSON files and sets use counts to 0 for all words.\n    Only use once when setting up this project\"\"\"\n    # Check if files exist and only proceed if overwrite is True\n    if (\n        os.path.exists(config.GRAMMAR_USAGE_PATH)\n        or os.path.exists(config.VOCAB_USAGE_PATH)\n    ) and not overwrite:",
        "detail": "src.initialise",
        "documentation": {}
    },
    {
        "label": "DATA_DIR",
        "kind": 5,
        "importPath": "src.initialise",
        "description": "src.initialise",
        "peekOfCode": "DATA_DIR = \"../data\"\ndef initialise_usage_data(overwrite=False):\n    \"\"\"Creates two usage JSON files and sets use counts to 0 for all words.\n    Only use once when setting up this project\"\"\"\n    # Check if files exist and only proceed if overwrite is True\n    if (\n        os.path.exists(config.GRAMMAR_USAGE_PATH)\n        or os.path.exists(config.VOCAB_USAGE_PATH)\n    ) and not overwrite:\n        print(\"Usage files already exist. Set overwrite=True to reinitialize.\")",
        "detail": "src.initialise",
        "documentation": {}
    },
    {
        "label": "plot_vocabulary_growth",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def plot_vocabulary_growth(phrases: List[str], window: int = 10) -> None:\n    \"\"\"\n    Plot vocabulary growth with cumulative total, new words per phrase, rolling mean,\n    and overall mean.\n    Args:\n        phrases: List of phrases to analyze\n        window: Window size for rolling mean calculation\n    \"\"\"\n    vocab = set()\n    cumulative_counts = []",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "load_spacy_model",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def load_spacy_model():\n    \"\"\"Load spaCy model with error handling.\"\"\"\n    try:\n        return spacy.load(\"en_core_web_md\")\n    except OSError:\n        print(\"Downloading spaCy model...\")\n        spacy.cli.download(\"en_core_web_md\")\n        return spacy.load(\"en_core_web_md\")\ndef remove_matching_words(phrases: list[str], original_set: set[str]) -> set[str]:\n    \"\"\"",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "remove_matching_words",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def remove_matching_words(phrases: list[str], original_set: set[str]) -> set[str]:\n    \"\"\"\n    Remove items from original_set that match with any word in phrases,\n    ignoring parenthetical text in original_set items.\n    So if our original set had 'falling (over)' as an entry, it would be removed if our\n    phrases had 'falling' in it.\n    This is because falling (over) as a prompt to an LLM can give better context for phrase\n    creation, but we won't have the (over) returned in any of our phrases.\n    \"\"\"\n    updated_set = original_set.copy()",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "get_vocab_dictionary_from_phrases",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def get_vocab_dictionary_from_phrases(\n    english_phrases: List[str],\n) -> Dict[str, List[str]]:\n    \"\"\"Processes the english phrases to extract a vocabulary dictionary with keys\n    'verbs' and 'vocab'. This is so we can, for a given chunk of phrases we are learning (in\n    flash cards), extract the vocab, and then re-use that vocab to create a story to\n    listen to (practice long-form listening)\n    Returns: vocab_dict: {'verbs' : ['try', 'care', ...], 'vocab' : ['really', 'hello', ...]}\n    \"\"\"\n    vocab_pos_tuples = extract_vocab_and_pos(",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "extract_vocab_and_pos",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def extract_vocab_and_pos(english_phrases: List[str]) -> List[Tuple[str, str]]:\n    \"\"\"Returns the (lemma and POS) for feeding into update_vocab_usage, as a list.\"\"\"\n    # Process vocabulary\n    nlp = load_spacy_model()\n    vocab_set = set()\n    excluded_names = {\"sam\", \"alex\"}\n    for phrase in english_phrases:\n        doc = nlp(phrase)\n        for token in doc:\n            if (",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "get_vocab_dict_from_dialogue",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def get_vocab_dict_from_dialogue(\n    story_dict: Dict, limit_story_parts: list = None\n) -> Dict[str, List[str]]:\n    \"\"\"\n    For a given English dialogue story dictionary {'introduction' : {'dialogue' : [...] etc}, extracts the vocab used and places it into a dictionary\n    with keys 'verbs' and 'vocab', which is our common format.\n    Excludes punctuation, persons identified by spaCy, and the names 'sam' and 'alex'.\n    \"\"\"\n    if limit_story_parts:\n        story_parts_to_process = limit_story_parts",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "find_missing_vocabulary",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def find_missing_vocabulary(vocab_dict_source: dict, vocab_dict_target: dict) -> dict:\n    \"\"\"Compare vocabulary between source flashcards and target story to find gaps in coverage.\n    Identifies which words in the target story aren't covered by existing flashcards,\n    helping determine what new flashcards need to be created.\n    Args:\n        vocab_dict_source: Dictionary with 'verbs' and 'vocab' lists from existing flashcards\n        vocab_dict_target: Dictionary with 'verbs' and 'vocab' lists from target story\n    Returns:\n        Dictionary containing:\n        - missing_vocab: Dictionary with 'verbs' and 'vocab' lists containing uncovered words",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "process_phrase_vocabulary",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def process_phrase_vocabulary(phrase: str) -> tuple[set, set, set]:\n    \"\"\"Process a single phrase to extract verb and vocab matches\n    Returns:\n        tuple containing:\n        - set of (word, pos) tuples for all words\n        - set of verb matches\n        - set of vocab matches\n    \"\"\"\n    vocab_used = extract_vocab_and_pos([phrase])\n    verb_matches = set()",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "create_flashcard_index",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def create_flashcard_index(flashcard_phrases: list[str]) -> dict:\n    \"\"\"Create indexes mapping words to the flashcards containing them.\"\"\"\n    verb_index = {}  # word -> set of flashcard indices\n    vocab_index = {}\n    flashcard_word_counts = []\n    for idx, phrase in tqdm(\n        enumerate(flashcard_phrases),\n        desc=\"Indexes phrases...\",\n        total=len(flashcard_phrases),\n    ):",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "find_candidate_cards",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def find_candidate_cards(\n    remaining_verbs: set, remaining_vocab: set, flashcard_index: dict\n) -> set:\n    \"\"\"Find all cards that contain any remaining words\"\"\"\n    candidate_cards = set()\n    # Get all cards containing remaining verbs\n    for word in remaining_verbs:\n        if word in flashcard_index[\"verb_index\"]:\n            candidate_cards.update(flashcard_index[\"verb_index\"][word])\n    # Get all cards containing remaining vocab",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "find_best_card",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def find_best_card(\n    candidate_cards: set,\n    remaining_verbs: set,\n    remaining_vocab: set,\n    flashcard_index: dict,\n) -> tuple[int, dict]:\n    \"\"\"Find card with most matches from candidates\n    Returns:\n        tuple of (best_card_index, match_info)\n        where match_info contains verb and vocab matches",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "get_matching_flashcards_indexed",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def get_matching_flashcards_indexed(vocab_dict: dict, flashcard_index: dict) -> dict:\n    \"\"\"Find minimal set of flashcards that cover the vocabulary.\n    Prioritizes cards that contain the most uncovered words to minimize\n    the total number of cards needed.\n    Args:\n        vocab_dict: Dictionary with 'verbs' and 'vocab' lists to cover\n        flashcard_index: Pre-computed index mapping words to flashcard indices\n    Returns:\n        Dictionary containing:\n        - selected_cards: List of selected flashcards with match info",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "extract_substring_matches",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def extract_substring_matches(\n    new_phrases: List[str], target_phrases: Set[str]\n) -> Set[str]:\n    \"\"\"Should find matches due to the presence of phrasal verbs etc\n    in our target_phrases (original vocab set) as this might contain\n    multiple words or lexical chunks like 'what's the time?'\n    We are basically checking that the phrases we have generated (new_phrases) have successfully\n    'ticked off' words or lexical chunks we are trying to generate (target_phrases) that come\n    from our vocab dict.\n    WIth the set of phrases we return, we will remove those from the to-do list so we steadily",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "extract_spacy_lowercase_words",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def extract_spacy_lowercase_words(new_phrases: List[str]) -> Set[str]:\n    # Ensure the spaCy model is loaded\n    nlp = load_spacy_model()\n    # Initialize an empty set to store unique lowercase words\n    lowercase_words = set()\n    # Process each phrase with spaCy\n    for phrase in new_phrases:\n        doc = nlp(phrase)\n        # Add the lowercase version of each token's text to the set\n        lowercase_words.update(token.text.lower() for token in doc)",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "get_verb_and_vocab_lists",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def get_verb_and_vocab_lists(used_words: Set[Tuple[str, str]]) -> Dict[str, List[str]]:\n    \"\"\"\n    Separate the input set of (word, POS) tuples into verb and vocabulary lists.\n    Args:\n    used_words (Set[Tuple[str, str]]): A set of tuples containing (word, POS)\n    Returns:\n    Dict[str, List[str]]: A dictionary with 'verbs' and 'vocab' lists\n    \"\"\"\n    verb_list = []\n    vocab_list = []",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "extract_content_words",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def extract_content_words(phrase: str, nlp) -> Set[Tuple[str, str]]:\n    \"\"\"\n    Extract content words (verbs and meaningful vocabulary) from a phrase.\n    Returns set of (lemma, pos) tuples.\n    \"\"\"\n    doc = nlp(phrase.lower())\n    # Define parts of speech to exclude\n    exclude_pos = {\n        \"DET\",\n        \"PUNCT\",",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "check_vocab_match",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def check_vocab_match(\n    phrase_words: Set[Tuple[str, str]], vocab_dict: Dict[str, List[str]]\n) -> bool:\n    \"\"\"\n    Check if the content words from the phrase match the vocabulary dictionary.\n    Returns True if all content words are found in the vocabulary lists.\n    \"\"\"\n    for lemma, pos in phrase_words:\n        if pos in [\"VERB\", \"AUX\"]:\n            if lemma not in vocab_dict.get(\"verbs\", []):",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "phrase_matches_vocab",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def phrase_matches_vocab(\n    english_phrase: str, vocab_dictionary: Dict[str, List[str]]\n) -> bool:\n    \"\"\"\n    Check if a phrase only uses words from the provided vocabulary dictionary.\n    Args:\n        english_phrase: The English phrase to check\n        vocab_dictionary: Dictionary with 'verbs' and 'vocab' lists\n    Returns:\n        bool: True if all content words in the phrase are found in the vocabulary lists",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "filter_matching_phrases",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def filter_matching_phrases(\n    phrases: List[str], vocab_dictionary: Dict[str, List[str]]\n) -> List[str]:\n    \"\"\"\n    Filter a list of phrases to only include those that match the vocabulary.\n    Args:\n        phrases: List of English phrases to check\n        vocab_dictionary: Dictionary with 'verbs' and 'vocab' lists\n    Returns:\n        List[str]: Filtered list of phrases that only use words from the vocabulary",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "prepare_phrase_dataframe",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def prepare_phrase_dataframe(phrases: List[str]) -> pd.DataFrame:\n    \"\"\"Create DataFrame with parsed phrases and extract content words.\n    Args:\n        phrases: List of phrases to analyze\n    Returns:\n        DataFrame with columns:\n            - phrase: Original phrase\n            - doc: Spacy Doc object\n            - content_words: Set of lemmatized content words\n    \"\"\"",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "calculate_new_words",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def calculate_new_words(row: pd.Series, known_vocab: Set[str]) -> Dict:\n    \"\"\"Calculate new vocabulary metrics for a row.\"\"\"\n    new_words = row[\"content_words\"] - known_vocab\n    return {\n        \"new_words\": len(new_words),\n        \"new_vocab\": new_words,\n        \"new_ratio\": (\n            len(new_words) / row[\"total_words\"] if row[\"total_words\"] > 0 else 0\n        ),\n    }",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "optimize_sequence",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def optimize_sequence(df: pd.DataFrame, window_size: int = 5) -> pd.DataFrame:\n    \"\"\"Optimize the sequence of phrases for steady vocabulary acquisition.\n    Args:\n        df: DataFrame with parsed phrases (from prepare_phrase_dataframe)\n        window_size: Size of rolling window for local optimization\n    Returns:\n        DataFrame with optimized sequence and metrics\n    \"\"\"\n    # Calculate ideal rate\n    total_vocab = set().union(*df[\"content_words\"].values)",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "analyze_sequence",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def analyze_sequence(df: pd.DataFrame) -> Dict:\n    \"\"\"Analyze and print statistics about the optimized sequence.\"\"\"\n    stats = {\n        \"avg_new_words\": df[\"new_words\"].mean(),\n        \"std_new_words\": df[\"new_words\"].std(),\n        \"min_new_words\": df[\"new_words\"].min(),\n        \"max_new_words\": df[\"new_words\"].max(),\n        \"total_phrases\": len(df),\n        \"cumulative_vocab\": len(set().union(*df[\"content_words\"].values)),\n    }",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "optimize_vocab_sequence",
        "kind": 2,
        "importPath": "src.nlp",
        "description": "src.nlp",
        "peekOfCode": "def optimize_vocab_sequence(\n    phrases: List[str], window_size: int = 5\n) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"Main function to optimize phrase sequence for vocabulary acquisition.\n    Args:\n        phrases: List of phrases to optimize\n        window_size: Size of rolling window for local optimization\n    Returns:\n        Tuple of (optimized DataFrame, statistics dictionary)\n    \"\"\"",
        "detail": "src.nlp",
        "documentation": {}
    },
    {
        "label": "generate_phrases_from_vocab_dict",
        "kind": 2,
        "importPath": "src.phrase",
        "description": "src.phrase",
        "peekOfCode": "def generate_phrases_from_vocab_dict(\n    vocab_dict: Dict[str, List[str]],\n    max_iterations: int = 10,\n    length_phrase: str = \"6-9 words long\",\n    verbs_per_phrase: str = \"one or two verbs\",\n    num_phrases: int = 100,\n    localise: bool = False,  # whether to make the phrases aligned to the country\n) -> List[str]:\n    \"\"\"This takes a dict with keys 'verbs' and 'vocab'  and constructs phrases using them, iterating through until all words are exhausted.\n    Desgined for Longman Communication vocab lists with a verb : vocab ratio of about 1:4 and 1000 words tends to generate around 850 phrases in",
        "detail": "src.phrase",
        "documentation": {}
    },
    {
        "label": "generate_phrases_with_llm",
        "kind": 2,
        "importPath": "src.phrase",
        "description": "src.phrase",
        "peekOfCode": "def generate_phrases_with_llm(\n    verb_list: List[str],\n    vocab_list: List[str],\n    num_phrases: int = 100,\n    length_phrase: str = \"6-9 words long\",\n    verbs_per_phrase: str = \"one or two verbs\",\n    localise: bool = False,\n) -> List[str]:\n    localise_prompt_segment = \"\"  # default is blank\n    if localise:",
        "detail": "src.phrase",
        "documentation": {}
    },
    {
        "label": "generate_minimal_phrases_with_llm",
        "kind": 2,
        "importPath": "src.phrase",
        "description": "src.phrase",
        "peekOfCode": "def generate_minimal_phrases_with_llm(\n    word_list: List[str],\n    length_phrase: str = \"6-9 words long\",\n    verbs_per_phrase: str = \"one or two verbs\",\n) -> List[str]:\n    \"\"\"We don't localise the minimal phrases as we are trying to exhaust the vocab\"\"\"\n    prompt = f\"\"\"\n    Task: Create the minimum number of English phrases necessary to use all the words from the provided list at least once. Each phrase should be {length_phrase}.\n    Word List: {', '.join(word_list)}\n    Requirements:",
        "detail": "src.phrase",
        "documentation": {}
    },
    {
        "label": "update_word_usage",
        "kind": 2,
        "importPath": "src.phrase",
        "description": "src.phrase",
        "peekOfCode": "def update_word_usage(data: List[Dict], used_words: List[str]) -> List[Dict]:\n    for entry in data:\n        if entry[\"word\"] in used_words:\n            entry[\"used\"] = True\n    return data\ndef get_text_from_dialogue(dialogue: List[Dict[str, str]]) -> List[str]:\n    \"\"\"ignoring the speaker, just gets all the utterances from a dialogue and puts\n    them in a single list\"\"\"\n    phrases = []\n    for utterance in dialogue:",
        "detail": "src.phrase",
        "documentation": {}
    },
    {
        "label": "get_text_from_dialogue",
        "kind": 2,
        "importPath": "src.phrase",
        "description": "src.phrase",
        "peekOfCode": "def get_text_from_dialogue(dialogue: List[Dict[str, str]]) -> List[str]:\n    \"\"\"ignoring the speaker, just gets all the utterances from a dialogue and puts\n    them in a single list\"\"\"\n    phrases = []\n    for utterance in dialogue:\n        phrases.append(utterance[\"text\"])\n    return phrases\ndef get_sentences_from_text(phrases: List[str]) -> List[str]:\n    \"\"\"Splits up phrases which might have more than one sentence per phrase and splits into a list of separate sentences.\n    Returns a list of sentences.",
        "detail": "src.phrase",
        "documentation": {}
    },
    {
        "label": "get_sentences_from_text",
        "kind": 2,
        "importPath": "src.phrase",
        "description": "src.phrase",
        "peekOfCode": "def get_sentences_from_text(phrases: List[str]) -> List[str]:\n    \"\"\"Splits up phrases which might have more than one sentence per phrase and splits into a list of separate sentences.\n    Returns a list of sentences.\n    \"\"\"\n    nlp = spacy.load(\"en_core_web_md\")\n    sentences = []\n    for phrase in phrases:\n        doc = nlp(phrase)\n        for sent in doc.sents:\n            sentences.append(sent.text)",
        "detail": "src.phrase",
        "documentation": {}
    },
    {
        "label": "generate_practice_phrases_from_dialogue",
        "kind": 2,
        "importPath": "src.phrase",
        "description": "src.phrase",
        "peekOfCode": "def generate_practice_phrases_from_dialogue(\n    dialogue: List[Dict[str, str]]\n) -> List[str]:\n    \"\"\"Uses an LLM call to create practice phrases from a longer dialogue\"\"\"\n    phrases = get_text_from_dialogue(dialogue)\n    llm_prompt = f\"\"\"\n    I will provide you with a list of dialogue phrases. Your task is to create 20-30 new phrases based on this dialogue, this is to support language learning\n     where we learn new ways of rearranging the vocabulary to create a wider range of phrases.\n    To help yourself, first list the verbs, tenses and other vocab within the phrases this will help you adhere to the following rules:\n    1. Use only the vocabulary, verbs, grammatical structures, and tenses present in the original dialogue.",
        "detail": "src.phrase",
        "documentation": {}
    },
    {
        "label": "create_html_story",
        "kind": 2,
        "importPath": "src.story",
        "description": "src.story",
        "peekOfCode": "def create_html_story(\n    story_data_dict: Dict,\n    output_path: str,\n    component_path: str,\n    title: Optional[str] = None,\n    language: str = config.TARGET_LANGUAGE_NAME,\n) -> None:\n    \"\"\"\n    Create a standalone HTML file from the story data dictionary.\n    Args:",
        "detail": "src.story",
        "documentation": {}
    },
    {
        "label": "convert_audio_to_base64",
        "kind": 2,
        "importPath": "src.story",
        "description": "src.story",
        "peekOfCode": "def convert_audio_to_base64(audio_segment: AudioSegment) -> str:\n    \"\"\"Convert an AudioSegment to a base64 encoded string.\"\"\"\n    buffer = io.BytesIO()\n    audio_segment.export(buffer, format=\"mp3\")\n    buffer.seek(0)\n    audio_bytes = buffer.read()\n    return base64.b64encode(audio_bytes).decode(\"utf-8\")\ndef convert_m4a_file_to_base64(m4a_file_path: str) -> str:\n    \"\"\"\n    Convert an M4A file to a base64 encoded string.",
        "detail": "src.story",
        "documentation": {}
    },
    {
        "label": "convert_m4a_file_to_base64",
        "kind": 2,
        "importPath": "src.story",
        "description": "src.story",
        "peekOfCode": "def convert_m4a_file_to_base64(m4a_file_path: str) -> str:\n    \"\"\"\n    Convert an M4A file to a base64 encoded string.\n    Args:\n        m4a_file_path: Path to the M4A file\n    Returns:\n        str: Base64 encoded string representation of the M4A file\n    Raises:\n        FileNotFoundError: If the M4A file doesn't exist\n        IOError: If there's an error reading the file",
        "detail": "src.story",
        "documentation": {}
    },
    {
        "label": "create_album_files",
        "kind": 2,
        "importPath": "src.story",
        "description": "src.story",
        "peekOfCode": "def create_album_files(\n    story_data_dict: dict,\n    cover_image: Image.Image,\n    output_dir: str,\n    story_name_clean: str,\n):\n    \"\"\"Creates and saves M4A files for the story, with album artwork.\n    Each M4A contains normal dialogue, fast dialogue (repeated), and final dialogue.\"\"\"\n    PAUSE_TEXT = \"---------\"\n    GAP_BETWEEN_PHRASES = AudioSegment.silent(duration=500)",
        "detail": "src.story",
        "documentation": {}
    },
    {
        "label": "prepare_story_data_for_html",
        "kind": 2,
        "importPath": "src.story",
        "description": "src.story",
        "peekOfCode": "def prepare_story_data_for_html(story_data_dict: Dict) -> Dict:\n    \"\"\"Process the story data dictionary to include base64 encoded audio for both\n    normal dialogue utterances and fast dialogue versions.\"\"\"\n    prepared_data = {}\n    for section_name, section_data in story_data_dict.items():\n        prepared_data[section_name] = {\n            \"dialogue\": section_data.get(\"dialogue\", []),\n            \"translated_dialogue\": section_data.get(\"translated_dialogue\", []),\n            \"audio_data\": {\n                \"dialogue\": [],",
        "detail": "src.story",
        "documentation": {}
    },
    {
        "label": "batch_translate",
        "kind": 2,
        "importPath": "src.translation",
        "description": "src.translation",
        "peekOfCode": "def batch_translate(texts, batch_size=128):\n    \"\"\"Translate texts in batches.\"\"\"\n    translated_texts = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i : i + batch_size]\n        result = translate.Client().translate(\n            batch, target_language=config.TARGET_LANGUAGE_ALPHA2, source_language=\"en\"\n        )\n        translated_texts.extend([item[\"translatedText\"] for item in result])\n    return translated_texts",
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_from_english",
        "kind": 2,
        "importPath": "src.translation",
        "description": "src.translation",
        "peekOfCode": "def translate_from_english(\n    text: Union[List[str], str], target_language: str = None\n) -> List[str]:\n    \"\"\"translates text into the target_language, returns translated text. text can be a str or List[str]\"\"\"\n    if target_language is None:\n        target_language = config.TARGET_LANGUAGE_ALPHA2\n    result = translate.Client().translate(\n        text, target_language=target_language, source_language=\"en\"\n    )\n    if isinstance(result, list):",
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_dialogue",
        "kind": 2,
        "importPath": "src.translation",
        "description": "src.translation",
        "peekOfCode": "def translate_dialogue(dialogue: List[Dict[str, str]]) -> List[Dict[str, str]]:\n    \"\"\"translates the 'text' part of the dialogue, keeping the speaker parts\"\"\"\n    texts = [utterance[\"text\"] for utterance in dialogue]\n    translated_texts = batch_translate(texts)\n    translated_dialogue = []\n    for utterance, translated_text in zip(dialogue, translated_texts):\n        translated_utterance = utterance.copy()\n        translated_utterance[\"text\"] = translated_text\n        translated_dialogue.append(translated_utterance)\n    return translated_dialogue",
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "translate_phrases",
        "kind": 2,
        "importPath": "src.translation",
        "description": "src.translation",
        "peekOfCode": "def translate_phrases(corrected_phrases: List[str]) -> List[Tuple[str, str]]:\n    \"\"\"translates a list of english phrases and returns a tuple of english, target_language\n    phrases back as this is an easier format to pass into audio generation, and to manually inspect\n    \"\"\"\n    translated_phrases = batch_translate(corrected_phrases)\n    return list(zip(corrected_phrases, translated_phrases))\ndef tokenize_text(\n    text: str, language_code: str = config.TARGET_LANGUAGE_CODE\n) -> List[str]:\n    \"\"\"",
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "tokenize_text",
        "kind": 2,
        "importPath": "src.translation",
        "description": "src.translation",
        "peekOfCode": "def tokenize_text(\n    text: str, language_code: str = config.TARGET_LANGUAGE_CODE\n) -> List[str]:\n    \"\"\"\n    Tokenize text using language-appropriate methods.\n    For space-separated languages: Simply split on spaces\n    For other languages: Use Google Cloud Natural Language API\n    Args:\n        text: Text to tokenize\n        language_code: Two-letter language code (e.g. 'en', 'ja')",
        "detail": "src.translation",
        "documentation": {}
    },
    {
        "label": "sanitize_path_component",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def sanitize_path_component(s: str) -> str:\n    \"\"\"\n    Sanitize a string for use in GCS paths.\n    Replaces spaces with underscores, removes special characters, and converts to lowercase.\n    \"\"\"\n    # Replace one or more spaces with a single underscore\n    s = re.sub(r\"\\s+\", \"_\", s)\n    # Remove any characters that aren't alphanumeric, underscore, or hyphen\n    s = \"\".join(c for c in s if c.isalnum() or c in \"_-\")\n    # Convert to lowercase for consistency",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "construct_gcs_path",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def construct_gcs_path(\n    story_name: str,\n    language_name: Optional[str] = None,\n    bucket_name: Optional[str] = None,\n) -> str:\n    \"\"\"\n    Construct the Google Cloud Storage path for a story HTML file.\n    Args:\n        story_name: Name of the story (without .html extension)\n        language_name: Optional language name (defaults to config.TARGET_LANGUAGE_NAME)",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "upload_to_gcs",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def upload_to_gcs(\n    html_file_path: str,\n    language_name: Optional[str] = None,\n    bucket_name: Optional[str] = None,\n) -> str:\n    \"\"\"\n    Upload an HTML file to Google Cloud Storage with organized folder structure.\n    Args:\n        html_file_path: Path to the HTML file to upload\n        language_name: Language name (e.g. \"Swedish\", \"French\")",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "clean_filename",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def clean_filename(phrase: str) -> str:\n    \"\"\"Convert a phrase to a clean filename-safe string.\"\"\"\n    # Convert to lowercase\n    clean = phrase.lower()\n    # Replace any non-alphanumeric characters (except spaces) with empty string\n    clean = re.sub(r\"[^a-z0-9\\s]\", \"\", clean)\n    # Replace spaces with underscores\n    clean = clean.replace(\" \", \"_\")\n    # Remove any double underscores\n    clean = re.sub(r\"_+\", \"_\", clean)",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "string_to_large_int",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def string_to_large_int(s: str) -> int:\n    \"\"\"Notes in Anki have a unique ID, and so to create the note ID and ensure\n    it correlates with the content we can pass in the translated phrase as a string\n    and get back a large interger (a bit like a hash function).\n    So this can be used to create a numerical ID from a given phrase.\n    Args:\n        s (str): The string to convert (usually the translated phrase)\n    Returns:\n        int: A large interger (equivalent to a hash)\n    \"\"\"",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "create_test_story_dict",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def create_test_story_dict(\n    story_data_dict: Dict[str, Dict],\n    story_parts: int = 2,\n    phrases: int = 2,\n    from_index: int = 0,\n) -> Dict[str, Dict]:\n    \"\"\"\n    Create a smaller version of the story_data_dict for testing purposes.\n    Args:\n    story_data_dict (Dict[str, Dict]): The original story data dictionary.",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "convert_defaultdict",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def convert_defaultdict(d):\n    if isinstance(d, defaultdict):\n        d = {k: convert_defaultdict(v) for k, v in d.items()}\n    return d\ndef save_defaultdict(d, filepath):\n    normal_dict = convert_defaultdict(d)\n    save_json(normal_dict, filepath)\ndef filter_longman_words(\n    data: List[Dict], category: Literal[\"S1\", \"S2\", \"S3\", \"W1\", \"W2\", \"W3\"]\n) -> Dict[str, List[str]]:",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "save_defaultdict",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def save_defaultdict(d, filepath):\n    normal_dict = convert_defaultdict(d)\n    save_json(normal_dict, filepath)\ndef filter_longman_words(\n    data: List[Dict], category: Literal[\"S1\", \"S2\", \"S3\", \"W1\", \"W2\", \"W3\"]\n) -> Dict[str, List[str]]:\n    \"\"\"This will only work with the specific format of longman data in a nested JSON structure from: https://github.com/healthypackrat/longman-communication-3000.\n    S1 means part of the first 1000 vocab list for speech, W3 means part of the 3000 words (i.e. the third '1000' chunk) for writing\n    \"\"\"\n    s1_words = defaultdict(list)",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "filter_longman_words",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def filter_longman_words(\n    data: List[Dict], category: Literal[\"S1\", \"S2\", \"S3\", \"W1\", \"W2\", \"W3\"]\n) -> Dict[str, List[str]]:\n    \"\"\"This will only work with the specific format of longman data in a nested JSON structure from: https://github.com/healthypackrat/longman-communication-3000.\n    S1 means part of the first 1000 vocab list for speech, W3 means part of the 3000 words (i.e. the third '1000' chunk) for writing\n    \"\"\"\n    s1_words = defaultdict(list)\n    for entry in data:\n        if category in entry.get(\"frequencies\", []):\n            for word_class in entry.get(\"word_classes\", []):",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "get_longman_verb_vocab_dict",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def get_longman_verb_vocab_dict(\n    longman_file_path, category: Literal[\"S1\", \"S2\", \"S3\", \"W1\", \"W2\", \"W3\"]\n) -> Dict[str, List[str]]:\n    \"\"\"Returns a vocabulary dict with keys 'verbs' and 'vocab' for verbs and all other parts-of-speech. This is now in the\n    same format as the known_vocab_list.json as used in the rest of the code.\"\"\"\n    data = load_json(longman_file_path)\n    category_words = filter_longman_words(data, category=category)\n    words_dict = defaultdict(list)\n    for pos in category_words.keys():\n        if pos in [\"v\", \"auxillary\"]:",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "save_pickle",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def save_pickle(data: Any, file_path: str) -> None:\n    \"\"\"\n    Save data to a pickle file, with special handling for AudioSegment objects.\n    Args:\n        data: Any Python object that can be pickled, including those containing AudioSegment objects\n        file_path: Path where the pickle file will be saved\n    \"\"\"\n    try:\n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "load_pickle",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def load_pickle(file_path: str, default_value: Any = None) -> Any:\n    \"\"\"\n    Load data from a pickle file, with proper error handling.\n    Args:\n        file_path: Path to the pickle file\n        default_value: Value to return if file doesn't exist or loading fails (default: None)\n    Returns:\n        The unpickled data, or default_value if loading fails\n    \"\"\"\n    if not os.path.exists(file_path):",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "load_text_file",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def load_text_file(file_path) -> List[str]:\n    with open(file_path, \"r\") as f:\n        return [line.strip() for line in f.readlines()]\ndef save_text_file(lines: List[str], file_path: str) -> None:\n    \"\"\"Save a list of strings to a text file, one per line.\n    Args:\n        lines: List of strings to save\n        file_path: Path where the file will be saved\n    \"\"\"\n    with open(file_path, \"w\") as f:",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "save_text_file",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def save_text_file(lines: List[str], file_path: str) -> None:\n    \"\"\"Save a list of strings to a text file, one per line.\n    Args:\n        lines: List of strings to save\n        file_path: Path where the file will be saved\n    \"\"\"\n    with open(file_path, \"w\") as f:\n        for line in lines:\n            f.write(f\"{line}\\n\")\ndef load_json(file_path) -> dict:",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "load_json",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def load_json(file_path) -> dict:\n    \"\"\"Returns {} if JSON does not exist\"\"\"\n    if not os.path.exists(file_path):\n        print(\"file does not exist, returning empty dict\")\n        return {}\n    with open(file_path, \"r\") as file:\n        return json.load(file)\ndef save_json(data, file_path):\n    with open(file_path, \"w\") as file:\n        json.dump(data, file, indent=2)",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "save_json",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def save_json(data, file_path):\n    with open(file_path, \"w\") as file:\n        json.dump(data, file, indent=2)\ndef get_caller_name():\n    \"\"\"Method 1: Using inspect.stack()\"\"\"\n    # Get the frame 2 levels up (1 would be this function, 2 is the caller)\n    caller_frame = inspect.stack()[2]\n    return caller_frame.function\ndef ok_to_query_api() -> bool:\n    \"\"\"Check if enough time has passed since the last API call.",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "get_caller_name",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def get_caller_name():\n    \"\"\"Method 1: Using inspect.stack()\"\"\"\n    # Get the frame 2 levels up (1 would be this function, 2 is the caller)\n    caller_frame = inspect.stack()[2]\n    return caller_frame.function\ndef ok_to_query_api() -> bool:\n    \"\"\"Check if enough time has passed since the last API call.\n    If not enough time has passed, wait for the remaining time.\n    Returns:\n        bool: True when it's ok to proceed with the API call",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "ok_to_query_api",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def ok_to_query_api() -> bool:\n    \"\"\"Check if enough time has passed since the last API call.\n    If not enough time has passed, wait for the remaining time.\n    Returns:\n        bool: True when it's ok to proceed with the API call\n    \"\"\"\n    time_since_last_call = config.get_time_since_last_api_call()\n    if time_since_last_call >= config.API_DELAY_SECONDS:\n        config.update_api_timestamp()\n        return True",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "anthropic_generate",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def anthropic_generate(prompt: str, max_tokens: int = 1024, model: str = None) -> str:\n    \"\"\"given a prompt generates an LLM response. The default model is specified in the config file.\n    Most likely the largest Anthropic model. The region paramater in the config will have to match where that model\n    is available\"\"\"\n    print(\n        f\"Function that called this one: {get_caller_name()}. Sleeping for 20 seconds\"\n    )\n    ok_to_query_api()\n    client = AnthropicVertex(\n        region=config.ANTHROPIC_REGION, project_id=config.PROJECT_ID",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "extract_json_from_llm_response",
        "kind": 2,
        "importPath": "src.utils",
        "description": "src.utils",
        "peekOfCode": "def extract_json_from_llm_response(response):\n    \"\"\"\n    Extract JSON from an LLM response.\n    :param response: String containing the LLM's response\n    :return: Extracted JSON as a Python object, or None if no valid JSON is found\n    \"\"\"\n    # Try to find JSON-like structure in the response\n    json_pattern = (\n        r\"\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}))*\\}\"\n    )",
        "detail": "src.utils",
        "documentation": {}
    },
    {
        "label": "test_japanese",
        "kind": 2,
        "importPath": "tests.test_anki_tools",
        "description": "tests.test_anki_tools",
        "peekOfCode": "def test_japanese():\n    \"\"\"Test Japanese words and phrases\"\"\"\n    # Test just 'world'\n    result = generate_wiktionary_links(\"世界\", \"Japanese\", \"ja\")\n    print(f\"\\nJapanese 'world': {result}\")\n    assert 'href=\"https://en.wiktionary.org/wiki/%E4%B8%96%E7%95%8C#Japanese\"' in result\n    time.sleep(1)\n    # Test 'hello world'\n    result = generate_wiktionary_links(\"こんにちは世界\", \"Japanese\", \"ja\")\n    print(f\"Japanese 'hello world': {result}\")",
        "detail": "tests.test_anki_tools",
        "documentation": {}
    },
    {
        "label": "test_chinese",
        "kind": 2,
        "importPath": "tests.test_anki_tools",
        "description": "tests.test_anki_tools",
        "peekOfCode": "def test_chinese():\n    \"\"\"Test Chinese (Mandarin) words and phrases\"\"\"\n    # Test just 'world'\n    result = generate_wiktionary_links(\"世界\", \"Chinese\", \"zh\")\n    print(f\"\\nChinese 'world': {result}\")\n    assert 'href=\"https://en.wiktionary.org/wiki/%E4%B8%96%E7%95%8C#Chinese\"' in result\n    time.sleep(1)\n    # Test 'hello world'\n    result = generate_wiktionary_links(\"你好世界\", \"Chinese\", \"zh\")\n    print(f\"Chinese 'hello world': {result}\")",
        "detail": "tests.test_anki_tools",
        "documentation": {}
    },
    {
        "label": "test_swedish",
        "kind": 2,
        "importPath": "tests.test_anki_tools",
        "description": "tests.test_anki_tools",
        "peekOfCode": "def test_swedish():\n    \"\"\"Test Swedish words and phrases\"\"\"\n    # Test just 'world'\n    result = generate_wiktionary_links(\"värld\", \"Swedish\", \"sv\")\n    print(f\"\\nSwedish 'world': {result}\")\n    assert 'href=\"https://en.wiktionary.org/wiki/v%C3%A4rld#Swedish\"' in result\n    time.sleep(1)\n    # Test 'hello world'\n    result = generate_wiktionary_links(\"hej värld\", \"Swedish\", \"sv\")\n    print(f\"Swedish 'hello world': {result}\")",
        "detail": "tests.test_anki_tools",
        "documentation": {}
    },
    {
        "label": "test_nonexistent_word",
        "kind": 2,
        "importPath": "tests.test_anki_tools",
        "description": "tests.test_anki_tools",
        "peekOfCode": "def test_nonexistent_word():\n    \"\"\"Test handling of words not in Wiktionary\"\"\"\n    nonsense = \"xyzqabc123\"\n    result = generate_wiktionary_links(nonsense, \"English\", \"en\")\n    print(f\"\\nResult for nonexistent word: {result}\")\n    assert \"href=\" not in result\n    assert nonsense in result  # Word should be present but not linked\n    time.sleep(1)\ndef test_real_japanese_phrase():\n    \"\"\"Test a natural Japanese phrase\"\"\"",
        "detail": "tests.test_anki_tools",
        "documentation": {}
    },
    {
        "label": "test_real_japanese_phrase",
        "kind": 2,
        "importPath": "tests.test_anki_tools",
        "description": "tests.test_anki_tools",
        "peekOfCode": "def test_real_japanese_phrase():\n    \"\"\"Test a natural Japanese phrase\"\"\"\n    result = generate_wiktionary_links(\"私は猫が好きです\", \"Japanese\", \"ja\")\n    print(f\"\\nJapanese natural phrase: {result}\")\n    # Just print for inspection - links will depend on what's in Wiktionary\n    time.sleep(1)",
        "detail": "tests.test_anki_tools",
        "documentation": {}
    },
    {
        "label": "sample_story_dict",
        "kind": 2,
        "importPath": "tests.test_dialogue_generation",
        "description": "tests.test_dialogue_generation",
        "peekOfCode": "def sample_story_dict() -> Dict:\n    return {\n        \"introduction\": {\n            \"dialogue\": [\n                {\"speaker\": \"Alice\", \"text\": \"I am reading a book.\"},\n                {\"speaker\": \"Bob\", \"text\": \"That's great! I love reading too.\"},\n                {\n                    \"speaker\": \"Alice\",\n                    \"text\": \"It's about space exploration. Very interesting!\",\n                },",
        "detail": "tests.test_dialogue_generation",
        "documentation": {}
    },
    {
        "label": "test_get_vocab_dict_from_dialogue",
        "kind": 2,
        "importPath": "tests.test_dialogue_generation",
        "description": "tests.test_dialogue_generation",
        "peekOfCode": "def test_get_vocab_dict_from_dialogue(story_dict, expected_output):\n    result = get_vocab_dict_from_dialogue(story_dict)\n    assert set(result[\"verbs\"]) == set(expected_output[\"verbs\"])\n    assert set(result[\"vocab\"]) == set(expected_output[\"vocab\"])\ndef test_get_vocab_dict_with_limit_parts():\n    story_dict = {\n        \"part1\": {\"dialogue\": [{\"speaker\": \"Alice\", \"text\": \"I run fast.\"}]},\n        \"part2\": {\"dialogue\": [{\"speaker\": \"Bob\", \"text\": \"I swim well.\"}]},\n    }\n    result = get_vocab_dict_from_dialogue(story_dict, limit_story_parts=[\"part1\"])",
        "detail": "tests.test_dialogue_generation",
        "documentation": {}
    },
    {
        "label": "test_get_vocab_dict_with_limit_parts",
        "kind": 2,
        "importPath": "tests.test_dialogue_generation",
        "description": "tests.test_dialogue_generation",
        "peekOfCode": "def test_get_vocab_dict_with_limit_parts():\n    story_dict = {\n        \"part1\": {\"dialogue\": [{\"speaker\": \"Alice\", \"text\": \"I run fast.\"}]},\n        \"part2\": {\"dialogue\": [{\"speaker\": \"Bob\", \"text\": \"I swim well.\"}]},\n    }\n    result = get_vocab_dict_from_dialogue(story_dict, limit_story_parts=[\"part1\"])\n    assert set(result[\"verbs\"]) == {\"run\"}\n    assert set(result[\"vocab\"]) == {\"i\", \"fast\"}\ndef test_missing_story_part():\n    story_dict = {\"part1\": {\"dialogue\": []}}",
        "detail": "tests.test_dialogue_generation",
        "documentation": {}
    },
    {
        "label": "test_missing_story_part",
        "kind": 2,
        "importPath": "tests.test_dialogue_generation",
        "description": "tests.test_dialogue_generation",
        "peekOfCode": "def test_missing_story_part():\n    story_dict = {\"part1\": {\"dialogue\": []}}\n    with pytest.raises(KeyError):\n        get_vocab_dict_from_dialogue(story_dict, limit_story_parts=[\"missing_part\"])",
        "detail": "tests.test_dialogue_generation",
        "documentation": {}
    },
    {
        "label": "test_remove_matching_words",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_remove_matching_words(phrases, original_set, expected):\n    result = remove_matching_words(phrases, original_set)\n    assert result == expected\n@pytest.mark.parametrize(\n    \"new_phrases, target_phrases, expected\",\n    [\n        # Basic question mark handling\n        (\n            [\"What is your name?\", \"How are you?\"],\n            {\"What is your name?\", \"your name\", \"how\"},",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_extract_substring_matches_question_marks",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_extract_substring_matches_question_marks(\n    new_phrases, target_phrases, expected\n):\n    result = extract_substring_matches(new_phrases, target_phrases)\n    assert result == expected\nimport pytest\nfrom typing import Dict, List\n@pytest.mark.parametrize(\n    \"phrases, expected_dict\",\n    [",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_get_vocab_dictionary_from_phrases",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_get_vocab_dictionary_from_phrases(\n    phrases: List[str], expected_dict: Dict[str, List[str]]\n):\n    result = get_vocab_dictionary_from_phrases(phrases)\n    # Check that we have the expected keys\n    assert sorted(result.keys()) == sorted(expected_dict.keys())\n    # Check that each list contains exactly the expected items (order doesn't matter)\n    assert sorted(result[\"verbs\"]) == sorted(expected_dict[\"verbs\"])\n    assert sorted(result[\"vocab\"]) == sorted(expected_dict[\"vocab\"])\nimport pytest",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_process_phrase_vocabulary",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_process_phrase_vocabulary(phrase, expected_verb_count, expected_vocab_count):\n    \"\"\"Test vocabulary extraction from individual phrases\"\"\"\n    vocab_used, verb_matches, vocab_matches = process_phrase_vocabulary(phrase)\n    assert len(verb_matches) == expected_verb_count\n    assert len(vocab_matches) == expected_vocab_count\n    assert len(vocab_used) == expected_verb_count + expected_vocab_count\n@pytest.mark.parametrize(\n    \"phrases,search_word,expected_indices\",\n    [\n        # Test verb 'run' appearing in different positions",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_flashcard_index",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_flashcard_index(phrases, search_word, expected_indices):\n    \"\"\"Test the full indexing functionality with real spaCy processing\"\"\"\n    result = create_flashcard_index(phrases)\n    # The word could be in either verb_index or vocab_index\n    found_indices = result[\"verb_index\"].get(search_word, []) + result[\n        \"vocab_index\"\n    ].get(search_word, [])\n    assert sorted(found_indices) == sorted(expected_indices)\n    assert len(result[\"word_counts\"]) == len(phrases)\n    assert result[\"phrases\"] == phrases",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_complex_example",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_complex_example():\n    \"\"\"Test a more complex example with multiple types of matches\"\"\"\n    phrases = [\n        \"The big cat runs quickly\",\n        \"Two cats are sleeping peacefully\",\n        \"A small dog and a big cat play together\",\n        \"Birds fly in the blue sky\",\n    ]\n    result = create_flashcard_index(phrases)\n    # Test various aspects of the indexing",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_find_candidate_cards",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_find_candidate_cards():\n    # Create simple test index\n    flashcard_index = {\n        \"verb_index\": {\"run\": [0, 2], \"cry\": [1]},\n        \"vocab_index\": {\"mum\": [0, 1], \"fast\": [2]},\n    }\n    remaining_verbs = {\"run\", \"cry\"}\n    remaining_vocab = {\"mum\"}\n    candidates = find_candidate_cards(remaining_verbs, remaining_vocab, flashcard_index)\n    assert candidates == {0, 1, 2}  # Should find all cards containing run, cry or mum",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_find_best_card",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_find_best_card():\n    flashcard_index = {\n        \"word_counts\": [\n            {\"words\": [(\"run\", \"VERB\"), (\"mum\", \"NOUN\")]},  # card 0\n            {\"words\": [(\"cry\", \"VERB\"), (\"mum\", \"NOUN\")]},  # card 1\n            {\"words\": [(\"run\", \"VERB\"), (\"fast\", \"ADJ\")]},  # card 2\n        ]\n    }\n    remaining_verbs = {\"run\", \"cry\"}\n    remaining_vocab = {\"mum\", \"fast\"}",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_get_matching_flashcards",
        "kind": 2,
        "importPath": "tests.test_nlp",
        "description": "tests.test_nlp",
        "peekOfCode": "def test_get_matching_flashcards(vocab_dict, expected_matches):\n    # Create test index with sample phrases\n    flashcard_index = {\n        \"phrases\": [\"He ran crying to his mum\", \"The boy cried to mum\", \"Run fast\"],\n        \"verb_index\": {\"run\": [0, 2], \"cry\": [0, 1]},\n        \"vocab_index\": {\"mum\": [0, 1], \"fast\": [2]},\n        \"word_counts\": [\n            {\"words\": [(\"run\", \"VERB\"), (\"cry\", \"VERB\"), (\"mum\", \"NOUN\")]},\n            {\"words\": [(\"cry\", \"VERB\"), (\"mum\", \"NOUN\")]},\n            {\"words\": [(\"run\", \"VERB\"), (\"fast\", \"ADJ\")]},",
        "detail": "tests.test_nlp",
        "documentation": {}
    },
    {
        "label": "test_tokenize_text",
        "kind": 2,
        "importPath": "tests.test_translation",
        "description": "tests.test_translation",
        "peekOfCode": "def test_tokenize_text(\n    text: str, language_code: str, expected: List[str], description: str\n):\n    \"\"\"\n    Test tokenization across different languages and scenarios.\n    Note: When using the actual Google API, results might differ from expected.\n    This test assumes ideal tokenization - in practice you might want to:\n    1. Mock the API response\n    2. Test for patterns rather than exact matches\n    3. Skip API-dependent tests in certain environments",
        "detail": "tests.test_translation",
        "documentation": {}
    },
    {
        "label": "test_translate_from_english",
        "kind": 2,
        "importPath": "tests.test_translation",
        "description": "tests.test_translation",
        "peekOfCode": "def test_translate_from_english(mock_translate_client):\n    # Setup\n    mock_client = MagicMock()\n    mock_translate_client.return_value = mock_client\n    mock_client.translate.return_value = {\"translatedText\": \"Hola mundo\"}\n    # Test with default target language\n    result = translate_from_english(\"Hello world\")\n    assert result == [\"Hola mundo\"]\n    mock_client.translate.assert_called_with(\n        \"Hello world\",",
        "detail": "tests.test_translation",
        "documentation": {}
    },
    {
        "label": "mock_func_tranlsate_from_english",
        "kind": 2,
        "importPath": "tests.test_translation",
        "description": "tests.test_translation",
        "peekOfCode": "def mock_func_tranlsate_from_english(text):\n    if isinstance(text, list):\n        return [f\"Translated: {item}\" for item in text]\n    else:\n        return [f\"Translated: {text}\"]\n@patch(\"src.translation.batch_translate\")\ndef test_translate_dialogue(mock_batch_translate):\n    # Setup\n    mock_batch_translate.return_value = [\n        \"Translated: Hello\",",
        "detail": "tests.test_translation",
        "documentation": {}
    },
    {
        "label": "test_translate_dialogue",
        "kind": 2,
        "importPath": "tests.test_translation",
        "description": "tests.test_translation",
        "peekOfCode": "def test_translate_dialogue(mock_batch_translate):\n    # Setup\n    mock_batch_translate.return_value = [\n        \"Translated: Hello\",\n        \"Translated: How are you?\",\n    ]\n    dialogue = [\n        {\"speaker\": \"Alice\", \"text\": \"Hello\"},\n        {\"speaker\": \"Bob\", \"text\": \"How are you?\"},\n    ]",
        "detail": "tests.test_translation",
        "documentation": {}
    },
    {
        "label": "test_translate_dialogue_deep_copy",
        "kind": 2,
        "importPath": "tests.test_translation",
        "description": "tests.test_translation",
        "peekOfCode": "def test_translate_dialogue_deep_copy(mock_batch_translate):\n    # Setup\n    mock_batch_translate.return_value = [\n        \"Translated: Hello\",\n        \"Translated: How are you?\",\n    ]\n    original_dialogue = [\n        {\"speaker\": \"Alice\", \"text\": \"Hello\"},\n        {\"speaker\": \"Bob\", \"text\": \"How are you?\"},\n    ]",
        "detail": "tests.test_translation",
        "documentation": {}
    },
    {
        "label": "test_translate_phrases",
        "kind": 2,
        "importPath": "tests.test_translation",
        "description": "tests.test_translation",
        "peekOfCode": "def test_translate_phrases(mock_batch_translate):\n    # Setup\n    mock_batch_translate.return_value = [\n        \"Translated: Hello\",\n        \"Translated: How are you?\",\n    ]\n    phrases = [\"Hello\", \"How are you?\"]\n    # Test\n    result = translate_phrases(phrases)\n    # Assert",
        "detail": "tests.test_translation",
        "documentation": {}
    },
    {
        "label": "test_sanitize_path_component",
        "kind": 2,
        "importPath": "tests.test_utils",
        "description": "tests.test_utils",
        "peekOfCode": "def test_sanitize_path_component(input_str, expected):\n    assert sanitize_path_component(input_str) == expected\n@pytest.mark.parametrize(\n    \"story_name, language_name, bucket_name, expected\",\n    [\n        # Basic path construction\n        (\n            \"my_story.html\",\n            \"Swedish\",\n            \"test-bucket\",",
        "detail": "tests.test_utils",
        "documentation": {}
    },
    {
        "label": "test_construct_gcs_path",
        "kind": 2,
        "importPath": "tests.test_utils",
        "description": "tests.test_utils",
        "peekOfCode": "def test_construct_gcs_path(story_name, language_name, bucket_name, expected):\n    result = construct_gcs_path(story_name, language_name, bucket_name)\n    assert result == expected\ndef test_construct_gcs_path_default_config(monkeypatch):\n    # Mock the config values\n    class MockConfig:\n        GCS_PUBLIC_BUCKET = \"default-bucket\"\n        TARGET_LANGUAGE_NAME = \"Spanish\"\n    monkeypatch.setattr(\"src.utils.config\", MockConfig())\n    # Test with defaults",
        "detail": "tests.test_utils",
        "documentation": {}
    },
    {
        "label": "test_construct_gcs_path_default_config",
        "kind": 2,
        "importPath": "tests.test_utils",
        "description": "tests.test_utils",
        "peekOfCode": "def test_construct_gcs_path_default_config(monkeypatch):\n    # Mock the config values\n    class MockConfig:\n        GCS_PUBLIC_BUCKET = \"default-bucket\"\n        TARGET_LANGUAGE_NAME = \"Spanish\"\n    monkeypatch.setattr(\"src.utils.config\", MockConfig())\n    # Test with defaults\n    result = construct_gcs_path(\"story.html\")\n    assert result == \"gs://default-bucket/spanish/story/story.html\"\n    # Test overriding just the language",
        "detail": "tests.test_utils",
        "documentation": {}
    }
]