{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Generation\n",
    "We remember things better as stories. The plan here is to pick a subset of our phrases, extract the vocabularly, and generate a story based off of them. We can then pull in more flashcards / phrases to ensure a more complete phrase coverage.\n",
    "\n",
    "The story name will be story_some_title; when added as a 'tag' into Anki, this will add a hyperlink to a google cloud bucket of a specific format of bucket/language/story_name/story_name.html\n",
    "\n",
    "This means it is easy to add new stories to an existing flashcard deck, and the links will update as soon as you add the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "PAY_FOR_API = True #change to True to run cells that cost money via API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from src.config_loader import config\n",
    "from src.nlp import (\n",
    "    create_flashcard_index,\n",
    "    get_vocab_dict_from_dialogue,\n",
    "    get_vocab_dictionary_from_phrases,\n",
    "    find_missing_vocabulary,\n",
    ")\n",
    "from src.utils import load_json, load_text_file, save_json, save_pickle, upload_to_gcs\n",
    "from src.anki_tools import get_deck_contents, AnkiCollectionReader\n",
    "# Add the parent directory of 'src' to the Python path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add directories\n",
    "story images can be re-used between languages, but audio files are language specific, so we structure the story directory story_name/language with audio files in 'language/' and images and the english JSON file in story_name dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = Path().absolute()  # This gives src/notebooks\n",
    "phrase_dir = notebook_dir.parent / \"data\" / \"phrases\" #where we store text files of phrases\n",
    "story_dir = notebook_dir.parent / \"outputs\" / \"stories\" # where we store our stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we already have flashcards generated for some phrases:\n",
    "a flashcard index allows us to select flashcards that cover a specific vocabulary range, it's quite computationally expensive, but is generated\n",
    "using create_flashcard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASE_LIST_NAME = \"longman_1000_phrases\"\n",
    "phrase_file = phrase_dir / f\"{PHRASE_LIST_NAME}.txt\"\n",
    "phrases = load_text_file(phrase_file)\n",
    "pprint(f\"First few phrases {phrases[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the flashcard index\n",
    "This makes it very fast to find matching flashcards from a given vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long process, so only create if it doesn't exist\n",
    "notebook_dir = Path().absolute()  # This gives src/notebooks\n",
    "index_file = phrase_dir / f\"{PHRASE_LIST_NAME}_index.json\"\n",
    "\n",
    "if index_file.exists():\n",
    "    phrase_index = load_json(index_file)\n",
    "else:\n",
    "    phrase_index = create_flashcard_index(phrases)\n",
    "    save_json(data=phrase_index, file_path=index_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample some phrases to generate the story from\n",
    "This will pin the story to the vocab found in some pre-existing phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can obtain phrases we know to create a story from:\n",
    "# NOTE: you must close Anki Desktop when trying to form a connection here\n",
    "with AnkiCollectionReader() as reader:\n",
    "    pprint(reader.get_deck_names())\n",
    "\n",
    "#this will print out deck_id : deck_name -> we want to copy the relevant deck_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECK_NAME = \"RapidRetention - Swedish - LM1000\"\n",
    "df = get_deck_contents(deck_name=DECK_NAME) #calculates knowledge score\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find phrases we know, and limit the flashcard index to those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.phrase import get_phrase_indices\n",
    "known_phrases = df.query(\"knowledge_score > 0.2\").sort_values(by=\"knowledge_score\", ascending=False)['EnglishText'].tolist()\n",
    "\n",
    "#we need to know the location of each phrase as an integer in the phrase_index\n",
    "known_phrase_indicies = get_phrase_indices(known_phrases = known_phrases, all_phrases = phrase_index['phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp import remove_unknown_index_values\n",
    "from copy import deepcopy\n",
    "#if we don't know a phrase, we don't want to retrieve that from the index and link it to a story\n",
    "known_index = deepcopy(phrase_index)\n",
    "known_index['verb_index'] = remove_unknown_index_values(known_phrase_indicies, known_index['verb_index'])\n",
    "known_index['vocab_index'] = remove_unknown_index_values(known_phrase_indicies, known_index['vocab_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_phrases = random.sample(known_phrases, min(75, len(known_phrases)))\n",
    "vocab_dict_flashcards = get_vocab_dictionary_from_phrases(sampled_phrases) #75 phrases should give a decent amount of vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dialogue_generation import generate_story\n",
    "story_name, story_dialogue = generate_story(vocab_dict_flashcards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_story_name = f\"story_{story_name.lower().replace(' ', '_')}\"\n",
    "story_path = story_dir / clean_story_name / f\"{clean_story_name}.json\"\n",
    "\n",
    "save_json(story_dialogue, story_path)\n",
    "print(f\"saved story to {story_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the LLM goes a bit beyond the vocab found in the flashcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_dict_story = get_vocab_dict_from_dialogue(story_dialogue, limit_story_parts=None)\n",
    "vocab_overlap = find_missing_vocabulary(vocab_dict_flashcards, vocab_dict_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve flashcards we know that better fit the story vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp import get_matching_flashcards_indexed\n",
    "\n",
    "# Let's pull all the existing phrases we need to cover the vocab on our story\n",
    "#remember we modified the index to only use flashcards we known\n",
    "known_results = get_matching_flashcards_indexed(vocab_dict_story, known_index)\n",
    "known_flashcards = [card.get('phrase') for card in known_results['selected_cards']]\n",
    "print(\"Average knowledge: \", df.loc[df['EnglishText'].isin(known_flashcards)].knowledge_score.mean())\n",
    "known_vocab_dict = get_vocab_dictionary_from_phrases(known_flashcards)\n",
    "missing_vocab = find_missing_vocabulary(vocab_dict_source=known_vocab_dict, vocab_dict_target=vocab_dict_story)\n",
    "missing_vocab_dict = missing_vocab[\"missing_vocab\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now supplement these with any remaining flascards we don't yet know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should have a higher match in the cell above, we can now draw missing flashcards from the full index\n",
    "\n",
    "additional_results = get_matching_flashcards_indexed(missing_vocab_dict, phrase_index)\n",
    "additional_flashcards = [card.get('phrase') for card in additional_results['selected_cards']]\n",
    "print(len(additional_flashcards))\n",
    "\n",
    "all_flashcards = additional_flashcards + known_flashcards\n",
    "all_flashcards_vocab_dict = get_vocab_dictionary_from_phrases(all_flashcards)\n",
    "final_missing_vocab = find_missing_vocabulary(all_flashcards_vocab_dict, vocab_dict_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We need {len(all_flashcards)} flashcards to cover the story\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the story files\n",
    "Once you are happy with the flashcard coverage, you can:\n",
    "* translate and add audio\n",
    "* create the story images\n",
    "* create the story album files (M4a files with synced lyrics)\n",
    "* create the story HTML file using those previous files, and upload to Google Cloud Storage\n",
    "* tag the flascards with the story name...this will then mean you can link to the story from within Anki (the template uses tags to auto-create hyperlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you generate a specific story you can just load it here:\n",
    "# clean_story_name = \"story_roblox_bot_trouble\"\n",
    "# story_dialogue = load_json(story_dir / clean_story_name / f\"{clean_story_name}.json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.generate import add_translations, add_audio\n",
    "story_dialogue_audio = add_translations(story_dialogue)\n",
    "story_dialogue_audio = add_audio(story_dialogue_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this has target language content in now so we save in language dir\n",
    "save_pickle(data=story_dialogue_audio, file_path=story_dir / clean_story_name / config.TARGET_LANGUAGE_NAME / f\"{clean_story_name}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image files for each part of the story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.images import generate_and_save_story_images\n",
    "    \n",
    "image_data = generate_and_save_story_images(story_dict=story_dialogue, output_dir = story_dir / clean_story_name, story_name=clean_story_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M4A audio files which you will be able to download and play via a media player.\n",
    "They have synced lyrics which can be viewed in the Oto Music Player app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from src.story import create_album_files\n",
    "FIRST_STORY_PART = list(image_data.keys())[0]\n",
    "#may need to change depending on size of story made and what parts there are\n",
    "album_image = Image.open(story_dir / clean_story_name / f\"{clean_story_name}_{FIRST_STORY_PART}.png\")\n",
    "#create m4a file:\n",
    "create_album_files(story_data_dict=story_dialogue_audio, cover_image=album_image, output_dir=story_dir / clean_story_name / config.TARGET_LANGUAGE_NAME, story_name=clean_story_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the main html file - this wraps up the M4A files and image files within it, so it's self-contained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.story import create_html_story\n",
    "\n",
    "create_html_story(\n",
    "            story_dialogue_audio,\n",
    "            story_dir / clean_story_name, #the langauge sub-folders will be picked up automatically\n",
    "            component_path=\"../src/StoryViewer.js\",\n",
    "            story_name=clean_story_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to a public google cloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_story_path = story_dir / clean_story_name / config.TARGET_LANGUAGE_NAME / f\"{clean_story_name}.html\"\n",
    "upload_to_gcs(html_file_path=html_story_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking stories to flash cards\n",
    "We will use the Anki tag feature. Given a list of english phrases that are required to understand a story, we can tag each of those phrases within a specific Anki Deck.\n",
    "\n",
    "The card template will turn any tag starting story_ into a hyperlink to the public google cloud bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes this needs running twice...\n",
    "from src.anki_tools import add_tag_to_matching_notes\n",
    "updates, errors = add_tag_to_matching_notes(\n",
    "    deck_name=DECK_NAME,\n",
    "    phrases=all_flashcards,\n",
    "    tag=clean_story_name\n",
    ")\n",
    "\n",
    "print(f\"Updated {updates} notes\")\n",
    "if errors:\n",
    "    print(\"Errors encountered:\")\n",
    "    for error in errors:\n",
    "        print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deck = get_deck_contents(DECK_NAME)\n",
    "df_deck.query(\"tags == @clean_story_name\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we should know most of the vocab...\n",
    "df_deck.query(\"tags == @clean_story_name\").knowledge_score.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
