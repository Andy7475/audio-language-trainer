{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Generation\n",
    "We remember things better as stories. The plan here is to pick a subset of our phrases, extract the vocabularly, and generate a story based off of them. We can then pull in more flashcards / phrases to ensure a more complete phrase coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "# Add the parent directory of 'src' to the Python path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"First few phrases ['Do you want to become a famous writer?', 'Let me show \"\n",
      " \"you around the city', 'We need to handle this situation carefully', 'Stop \"\n",
      " 'wasting time on this\\', \\'Do you like playing the guitar at night?\\', \"I\\'m '\n",
      " 'taking a vacation next month\", \"Don\\'t forget to wear a helmet while '\n",
      " 'cycling\", \"Let\\'s cut unnecessary expenses this year\", \"We\\'re producing a '\n",
      " 'new product soon\", \\'Did you remember to turn off the stove?\\']')\n"
     ]
    }
   ],
   "source": [
    "from src.utils import load_text_file, save_json, load_json\n",
    "from src.nlp import get_vocab_dictionary_from_phrases, get_vocab_dict_from_dialogue, compare_vocab_overlap, create_flashcard_index\n",
    "from src.config_loader import config\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "filepath = \"../data/longman_1000_phrases.txt\"\n",
    "phrases = load_text_file(filepath)\n",
    "pprint(f\"First few phrases {phrases[:10]}\")\n",
    "\n",
    "#we already have flashcards generated for some phrases:\n",
    "#a flashcard index allows us to select flashcards that cover a specific\n",
    "#vocabulary range, it's quite computationally expensive, but is generated\n",
    "#using create_flashcard_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the flashcard index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexes phrases...: 100%|██████████| 841/841 [17:26<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# long process, so only create if it doesn't exist\n",
    "notebook_dir = Path().absolute()  # This gives src/notebooks\n",
    "data_dir = notebook_dir.parent / \"data\" / \"longman_1000_phrase_index.json\"\n",
    "\n",
    "if data_dir.exists():\n",
    "    phrase_index = load_json(data_dir)\n",
    "else:\n",
    "    phrase_index = create_flashcard_index(phrases)\n",
    "    save_json(phrase_index, data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample some phrases to generate the story from\n",
    "This will pin the story to the vocab found in some pre-existing phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict_flashcards = get_vocab_dictionary_from_phrases(phrases[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dialogue_generation import generate_story\n",
    "\n",
    "story_path = notebook_dir.parent / \"data\" / \"stories\" / \"test_story\" / \"story_community_park.json\"\n",
    "\n",
    "if story_path.exists():\n",
    "    story_50_phrases = load_json(story_path)\n",
    "else:\n",
    "    story_50_phrases = generate_story(vocab_dict_flashcards)\n",
    "    save_json(story_50_phrases, story_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the LLM goes a bit beyond the vocab found in the flashcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src. nlp import get_vocab_dict_from_dialogue\n",
    "vocab_dict_story = get_vocab_dict_from_dialogue(story_50_phrases, limit_story_parts=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VOCABULARY COVERAGE ANALYSIS ===\n",
      "Target verbs covered by flashcards: 46.3%\n",
      "Target vocabulary covered by flashcards: 31.1%\n",
      "\n",
      "Verbs needing new flashcards:\n",
      "['give', 'could', 'cycle', 'love', 'involve'] ...\n",
      "\n",
      "Vocabulary needing new flashcards:\n",
      "['absolutely', 'work', 'forward', 'since', 'lot'] ...\n"
     ]
    }
   ],
   "source": [
    "from src.nlp import find_missing_vocabulary\n",
    "\n",
    "vocab_overlap = find_missing_vocabulary(vocab_dict_flashcards, vocab_dict_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp import get_matching_flashcards_indexed\n",
    "# Let's pull all the existing phrases we need to cover the vocab on our story\n",
    "results = get_matching_flashcards_indexed(vocab_dict_story, phrase_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VOCABULARY COVERAGE ANALYSIS ===\n",
      "Target verbs covered by flashcards: 88.1%\n",
      "Target vocabulary covered by flashcards: 83.3%\n",
      "\n",
      "Verbs needing new flashcards:\n",
      "['cycle', 'plant', 'involve', 'delay', 'brainstorm'] ...\n",
      "\n",
      "Vocabulary needing new flashcards:\n",
      "['potluck', 'hey', 'spot', 'charge', 'able'] ...\n"
     ]
    }
   ],
   "source": [
    "proposed_flashcard_phrases = [card.get('phrase') for card in results['selected_cards']]\n",
    "vocab_from_new_flashcards = get_vocab_dictionary_from_phrases(proposed_flashcard_phrases)\n",
    "new_overlap = find_missing_vocabulary(vocab_from_new_flashcards, vocab_dict_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': ['cycle',\n",
       "  'plant',\n",
       "  'involve',\n",
       "  'delay',\n",
       "  'brainstorm',\n",
       "  'ride',\n",
       "  'accomplish',\n",
       "  'create'],\n",
       " 'vocab': ['potluck',\n",
       "  'hey',\n",
       "  'spot',\n",
       "  'charge',\n",
       "  'able',\n",
       "  'outdoors',\n",
       "  'sore',\n",
       "  'perfect',\n",
       "  '6',\n",
       "  'empty',\n",
       "  'productive',\n",
       "  'campaign',\n",
       "  'maybe',\n",
       "  'fundraiser',\n",
       "  'construction',\n",
       "  'handiwork',\n",
       "  'fundraising',\n",
       "  'support',\n",
       "  'finger',\n",
       "  'glad',\n",
       "  'wow',\n",
       "  'downtown',\n",
       "  'snack',\n",
       "  'alright',\n",
       "  'disappointing',\n",
       "  'proud',\n",
       "  'agreed',\n",
       "  'mini',\n",
       "  'apparently',\n",
       "  'hmm']}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can fill in the gap with some missing flashcards:\n",
    "\n",
    "missing_vocab_dict = new_overlap['missing_vocab']\n",
    "missing_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function that called this one: generate_minimal_phrases_with_llm. Sleeping for 20 seconds\n",
      "Iteration 1/10\n",
      "Generated 11 phrases - with minimal phrase prompt\n",
      "We have 0 verbs and 0 vocab words left\n",
      "All words have been used. Phrase generation complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hey, let's brainstorm ideas for our fundraising campaign downtown.\",\n",
       " 'Wow, your handiwork on this mini plant is perfect!',\n",
       " 'Maybe we can create a productive cycle to accomplish more.',\n",
       " 'Did you spot that disappointing construction delay downtown?',\n",
       " \"I'm glad you're able to support the potluck fundraiser.\",\n",
       " 'Shall we ride our bikes outdoors this afternoon?',\n",
       " 'Hmm, apparently my finger is sore from typing.',\n",
       " \"Don't forget to charge your phone before leaving.\",\n",
       " 'We agreed to involve 6 people in the project.',\n",
       " \"Are you proud of what we've been able to accomplish?\",\n",
       " \"Alright, let's have a snack and empty our minds.\"]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.phrase import generate_phrases_from_vocab_dict\n",
    "\n",
    "missing_phrases = generate_phrases_from_vocab_dict(missing_vocab_dict)\n",
    "missing_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need 99 flashcards to cover the story\n"
     ]
    }
   ],
   "source": [
    "num_cards = len(results[\"selected_cards\"])\n",
    "print(f\"We need {num_cards + len(missing_phrases)} flashcards to cover the story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_text_file\n",
    "\n",
    "save_text_file(proposed_flashcard_phrases + missing_phrases, \"../data/stories/test_story/test_phrases.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to generate images for the missing phrases, then we can create an anki deck for that particualr story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
