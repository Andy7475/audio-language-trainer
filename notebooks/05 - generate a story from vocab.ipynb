{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Generation\n",
    "We remember things better as stories. The plan here is to pick a subset of our phrases, extract the vocabularly, and generate a story based off of them. We can then pull in more flashcards / phrases to ensure a more complete phrase coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from src.config_loader import config\n",
    "from src.nlp import (\n",
    "    create_flashcard_index,\n",
    "    get_vocab_dict_from_dialogue,\n",
    "    get_vocab_dictionary_from_phrases,\n",
    ")\n",
    "from src.utils import load_json, load_text_file, save_json\n",
    "\n",
    "load_dotenv()\n",
    "# Add the parent directory of 'src' to the Python path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filepath = \"../data/longman_1000_phrases.txt\"\n",
    "phrases = load_text_file(filepath)\n",
    "pprint(f\"First few phrases {phrases[:10]}\")\n",
    "\n",
    "#we already have flashcards generated for some phrases:\n",
    "#a flashcard index allows us to select flashcards that cover a specific\n",
    "#vocabulary range, it's quite computationally expensive, but is generated\n",
    "#using create_flashcard_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the flashcard index\n",
    "This makes it very fast to find matching flashcards from a given vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long process, so only create if it doesn't exist\n",
    "notebook_dir = Path().absolute()  # This gives src/notebooks\n",
    "data_dir = notebook_dir.parent / \"data\" / \"longman_1000_phrase_index.json\"\n",
    "\n",
    "if data_dir.exists():\n",
    "    phrase_index = load_json(data_dir)\n",
    "else:\n",
    "    phrase_index = create_flashcard_index(phrases)\n",
    "    save_json(phrase_index, data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample some phrases to generate the story from\n",
    "This will pin the story to the vocab found in some pre-existing phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict_flashcards = get_vocab_dictionary_from_phrases(phrases[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dialogue_generation import generate_story\n",
    "\n",
    "story_path = notebook_dir.parent / \"data\" / \"stories\" / \"test_story\" / \"story_community_park.json\"\n",
    "\n",
    "if story_path.exists():\n",
    "    story_50_phrases = load_json(story_path)\n",
    "else:\n",
    "    story_50_phrases = generate_story(vocab_dict_flashcards)\n",
    "    save_json(story_50_phrases, story_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the LLM goes a bit beyond the vocab found in the flashcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src. nlp import get_vocab_dict_from_dialogue\n",
    "\n",
    "vocab_dict_story = get_vocab_dict_from_dialogue(story_50_phrases, limit_story_parts=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp import find_missing_vocabulary\n",
    "\n",
    "vocab_overlap = find_missing_vocabulary(vocab_dict_flashcards, vocab_dict_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp import get_matching_flashcards_indexed\n",
    "\n",
    "# Let's pull all the existing phrases we need to cover the vocab on our story\n",
    "results = get_matching_flashcards_indexed(vocab_dict_story, phrase_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_flashcard_phrases = [card.get('phrase') for card in results['selected_cards']]\n",
    "vocab_from_new_flashcards = get_vocab_dictionary_from_phrases(proposed_flashcard_phrases)\n",
    "new_overlap = find_missing_vocabulary(vocab_from_new_flashcards, vocab_dict_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can fill in the gap with some missing flashcards:\n",
    "\n",
    "missing_vocab_dict = new_overlap['missing_vocab']\n",
    "missing_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.phrase import generate_phrases_from_vocab_dict\n",
    "\n",
    "missing_phrases = generate_phrases_from_vocab_dict(missing_vocab_dict)\n",
    "missing_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cards = len(results[\"selected_cards\"])\n",
    "print(f\"We need {num_cards + len(missing_phrases)} flashcards to cover the story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_text_file\n",
    "\n",
    "save_text_file(proposed_flashcard_phrases + missing_phrases, \"../data/stories/test_story/test_phrases.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to generate images for the missing phrases, then we can create an anki deck for that particualr story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.images import add_images_to_phrases\n",
    "\n",
    "PAY_FOR_API = True\n",
    "\n",
    "output_dir = notebook_dir.parent / \"data\" / \"longman_phrase_images\" / \"longman1000\"\n",
    "\n",
    "if not output_dir.exists():\n",
    "    print(\"wrong directory\")\n",
    "    PAY_FOR_API = False\n",
    "\n",
    "if PAY_FOR_API:\n",
    "    image_files_and_prompts = add_images_to_phrases(phrases=missing_phrases, output_dir = output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking stories to flash cards\n",
    "We will use the Anki tag feature. Given a list of english phrases that are required to understand a story, we can tag each of those phrases within a specific Anki Deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the phrases\n",
    "phrases = load_text_file( \"../data/stories/test_story/test_phrases.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Default',\n",
      " 1731524665442: 'Swedish EAL',\n",
      " 1731700590019: 'Custom study session',\n",
      " 1732020971325: 'RapidRetention - Swedish - LM1000',\n",
      " 1732309563077: 'RapidRetention - Dutch - LM1000',\n",
      " 1732312948269: 'RapidRetention - German - LM1000',\n",
      " 1732313960891: 'RapidRetention - Arabic - LM1000',\n",
      " 1732314196963: 'RapidRetention - Spanish - LM1000',\n",
      " 1732314413500: 'RapidRetention - Japanese - LM1000',\n",
      " 1732316149591: 'RapidRetention - Russian - LM1000',\n",
      " 1732316158895: 'RapidRetention - Basque - LM1000',\n",
      " 1732316821915: 'RapidRetention - French - LM1000',\n",
      " 1732316936163: 'RapidRetention - Italian - LM1000',\n",
      " 1732460522330: 'RapidRetention - Persian - LM1000',\n",
      " 1732465028917: 'RapidRetention - Mandarin Chinese - LM1000',\n",
      " 1732637740663: 'RapidRetention - Welsh - LM1000',\n",
      " 1732869083179: 'RapidRetention - Serbian - LM1000',\n",
      " 1732980361514: 'RapidRetention - Russian - GCSE',\n",
      " 1732993700879: 'Persian Alphabet',\n",
      " 1733170456922: 'RapidRetention - Swedish - GCSE',\n",
      " 1733171641992: 'RapidRetention - Mandarin Chinese - GCSE',\n",
      " 1734260227418: 'RapidRetention - Swedish - NumbersDays',\n",
      " 1734261644938: 'RapidRetention - Russian - NumbersDays',\n",
      " 1734264578929: 'RapidRetention - Italian - NumbersDays',\n",
      " 1734266283013: 'RapidRetention - Welsh - NumbersDays',\n",
      " 1734267010699: 'RapidRetention - Ukrainian - NumbersDays'}\n"
     ]
    }
   ],
   "source": [
    "from src.anki_tools import AnkiCollectionReader\n",
    "\n",
    "with AnkiCollectionReader() as reader:\n",
    "    pprint(reader.get_deck_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-language-trainer\\src\\anki_tools.py:223:save() is deprecated: saving is automatic\n",
      "Updated 88 notes\n"
     ]
    }
   ],
   "source": [
    "from src.anki_tools import add_tag_to_matching_notes\n",
    "deck_name = \"RapidRetention - Swedish - LM1000\"\n",
    "updates, errors = add_tag_to_matching_notes(\n",
    "    deck_name=deck_name,\n",
    "    phrases=phrases,\n",
    "    tag=\"story_community_park\"\n",
    ")\n",
    "\n",
    "print(f\"Updated {updates} notes\")\n",
    "if errors:\n",
    "    print(\"Errors encountered:\")\n",
    "    for error in errors:\n",
    "        print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
