{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "load_dotenv()\n",
    "# Add the parent directory of 'src' to the Python path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "    \n",
    "from src.config_loader import config, VoiceManager\n",
    "from src.phrase import generate_phrases_with_llm, generate_phrases_from_vocab_dict\n",
    "from src.utils import load_json\n",
    "from src.anki_tools import convert_anki_to_story_dict, AnkiCollectionReader, export_to_anki_with_images\n",
    "from src.utils import load_text_file, save_json, load_json\n",
    "from src.dialogue_generation import get_story_prompt, generate_story\n",
    "from src.config_loader import config, VoiceManager, VoiceInfo, VoiceType, VoiceProvider\n",
    "from pprint import pprint\n",
    "import random\n",
    "import os\n",
    "\n",
    "from langcodes import Language\n",
    "\n",
    "ANKI_PATH = \"C:/Users/i5/AppData/Roaming/Anki2/User 1/collection.anki2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_text_file, save_json, load_json\n",
    "from src.nlp import get_vocab_dictionary_from_phrases, get_vocab_dict_from_dialogue, get_matching_flashcards,compare_vocab_overlap\n",
    "from src.config_loader import config\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "filepath = \"../data/longman_1000_phrases.txt\"\n",
    "phrases = load_text_file(filepath)\n",
    "pprint(f\"First few phrases {phrases[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocab_dict = get_vocab_dictionary_from_phrases(phrases[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(test_vocab_dict, \"test_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function that called this one: generate_story. Sleeping for 20 seconds\n"
     ]
    }
   ],
   "source": [
    "test_story = generate_story(test_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(test_story, \"test_story_50_phrases.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_used = get_vocab_dict_from_dialogue(test_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding translations:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning translation for introduction\n",
      "Config file has been modified. Reloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding translations:  33%|███▎      | 1/3 [00:01<00:03,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated dialogue\n",
      "Beginning translation for development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding translations:  67%|██████▋   | 2/3 [00:03<00:01,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated dialogue\n",
      "Beginning translation for resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding translations: 100%|██████████| 3/3 [00:04<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated dialogue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.generate import add_translations\n",
    "\n",
    "\n",
    "test_story = add_translations(test_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding audio:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning text-to-speech for introduction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dialogue audio: 100%|██████████| 16/16 [02:25<00:00,  9.06s/it]\n",
      "adding audio:  33%|███▎      | 1/3 [02:41<05:23, 161.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-speech for dialogue done\n",
      "Beginning text-to-speech for development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dialogue audio: 100%|██████████| 19/19 [02:50<00:00,  8.96s/it]\n",
      "adding audio:  67%|██████▋   | 2/3 [05:33<02:47, 167.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-speech for dialogue done\n",
      "Beginning text-to-speech for resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dialogue audio: 100%|██████████| 18/18 [02:40<00:00,  8.93s/it]\n",
      "adding audio: 100%|██████████| 3/3 [08:15<00:00, 165.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-speech for dialogue done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.generate import add_audio\n",
    "\n",
    "test_story_audio = add_audio(test_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML story created at: ../outputs/test/test.html\n"
     ]
    }
   ],
   "source": [
    "from src.utils import create_html_story\n",
    "\n",
    "output_dir = \"../outputs/test/test.html\"\n",
    "create_html_story(\n",
    "            test_story_audio,\n",
    "            output_dir,\n",
    "            component_path=\"../src/StoryViewer.js\",\n",
    "            title=\"test_short_story\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERB ANALYSIS ===\n",
      "Original verbs: 74\n",
      "Verbs used in story: 67\n",
      "Verbs from original used: 32 (43.2%)\n",
      "New verbs introduced: 35\n",
      "Examples of new verbs: ['look', 'delay', 'come', 'hope', 'ride']\n",
      "\n",
      "=== VOCABULARY ANALYSIS ===\n",
      "Original vocabulary: 153\n",
      "Vocabulary used in story: 180\n",
      "Vocabulary from original used: 66 (43.1%)\n",
      "New vocabulary introduced: 114\n",
      "Examples of new vocabulary: ['proud', 'able', 'worth', 'and', 'empty']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verb_overlap': {'be',\n",
       "  'bring',\n",
       "  'build',\n",
       "  'can',\n",
       "  'do',\n",
       "  'enjoy',\n",
       "  'excuse',\n",
       "  'find',\n",
       "  'forget',\n",
       "  'get',\n",
       "  'go',\n",
       "  'have',\n",
       "  'hear',\n",
       "  'let',\n",
       "  'like',\n",
       "  'love',\n",
       "  'might',\n",
       "  'organize',\n",
       "  'pay',\n",
       "  'plan',\n",
       "  'play',\n",
       "  'remember',\n",
       "  'see',\n",
       "  'should',\n",
       "  'show',\n",
       "  'spend',\n",
       "  'support',\n",
       "  'thank',\n",
       "  'think',\n",
       "  'want',\n",
       "  'wear',\n",
       "  'will'},\n",
       " 'new_verbs': {\"'ve\",\n",
       "  'accomplish',\n",
       "  'ask',\n",
       "  'believe',\n",
       "  'brainstorm',\n",
       "  'check',\n",
       "  'come',\n",
       "  'could',\n",
       "  'create',\n",
       "  'cycle',\n",
       "  'delay',\n",
       "  'discuss',\n",
       "  'eat',\n",
       "  'expect',\n",
       "  'give',\n",
       "  'happen',\n",
       "  'help',\n",
       "  'hope',\n",
       "  'involve',\n",
       "  'know',\n",
       "  'lead',\n",
       "  'look',\n",
       "  'maintain',\n",
       "  'meet',\n",
       "  'plant',\n",
       "  'ride',\n",
       "  'say',\n",
       "  'sound',\n",
       "  'speak',\n",
       "  'start',\n",
       "  'suppose',\n",
       "  'take',\n",
       "  'turn',\n",
       "  'worry',\n",
       "  'would'},\n",
       " 'unused_verbs': {'accept',\n",
       "  'answer',\n",
       "  'apply',\n",
       "  'bet',\n",
       "  'call',\n",
       "  'carry',\n",
       "  'choose',\n",
       "  'collect',\n",
       "  'cook',\n",
       "  'fit',\n",
       "  'join',\n",
       "  'keep',\n",
       "  'learn',\n",
       "  'leave',\n",
       "  'lift',\n",
       "  'live',\n",
       "  'lose',\n",
       "  'move',\n",
       "  'need',\n",
       "  'offer',\n",
       "  'open',\n",
       "  'produce',\n",
       "  'read',\n",
       "  'recognize',\n",
       "  'rid',\n",
       "  'run',\n",
       "  'shake',\n",
       "  'shut',\n",
       "  'sing',\n",
       "  'singe',\n",
       "  'stop',\n",
       "  'teach',\n",
       "  'tell',\n",
       "  'tend',\n",
       "  'throw',\n",
       "  'try',\n",
       "  'wait',\n",
       "  'walk',\n",
       "  'waste',\n",
       "  'watch',\n",
       "  'wish',\n",
       "  'write'},\n",
       " 'vocab_overlap': {'a',\n",
       "  'afternoon',\n",
       "  'all',\n",
       "  'around',\n",
       "  'at',\n",
       "  'beautiful',\n",
       "  'city',\n",
       "  'community',\n",
       "  'cycling',\n",
       "  'different',\n",
       "  'during',\n",
       "  'even',\n",
       "  'evening',\n",
       "  'every',\n",
       "  'for',\n",
       "  'from',\n",
       "  'guitar',\n",
       "  'he',\n",
       "  'helmet',\n",
       "  'his',\n",
       "  'how',\n",
       "  'i',\n",
       "  'if',\n",
       "  'in',\n",
       "  'it',\n",
       "  'just',\n",
       "  'local',\n",
       "  'location',\n",
       "  'minister',\n",
       "  'month',\n",
       "  'more',\n",
       "  'my',\n",
       "  'neighborhood',\n",
       "  'new',\n",
       "  'newspaper',\n",
       "  'next',\n",
       "  'not',\n",
       "  'of',\n",
       "  'office',\n",
       "  'our',\n",
       "  'park',\n",
       "  'party',\n",
       "  'path',\n",
       "  'people',\n",
       "  'really',\n",
       "  'right',\n",
       "  'so',\n",
       "  'soon',\n",
       "  'surprise',\n",
       "  'terrible',\n",
       "  'that',\n",
       "  'the',\n",
       "  'they',\n",
       "  'this',\n",
       "  'time',\n",
       "  'to',\n",
       "  'today',\n",
       "  'traffic',\n",
       "  'us',\n",
       "  'we',\n",
       "  'where',\n",
       "  'which',\n",
       "  'with',\n",
       "  'year',\n",
       "  'you',\n",
       "  'your'},\n",
       " 'new_vocab': {'6',\n",
       "  'able',\n",
       "  'about',\n",
       "  'absolutely',\n",
       "  'after',\n",
       "  'again',\n",
       "  'agreed',\n",
       "  'alright',\n",
       "  'amazing',\n",
       "  'an',\n",
       "  'and',\n",
       "  'any',\n",
       "  'anything',\n",
       "  'apparently',\n",
       "  'back',\n",
       "  'big',\n",
       "  'break',\n",
       "  'budget',\n",
       "  'business',\n",
       "  'but',\n",
       "  'bye',\n",
       "  'call',\n",
       "  'campaign',\n",
       "  'charge',\n",
       "  'construction',\n",
       "  'definitely',\n",
       "  'disappointing',\n",
       "  'downtown',\n",
       "  'empty',\n",
       "  'enough',\n",
       "  'event',\n",
       "  'everyone',\n",
       "  'exactly',\n",
       "  'family',\n",
       "  'finger',\n",
       "  'first',\n",
       "  'flower',\n",
       "  'forward',\n",
       "  'free',\n",
       "  'fundraiser',\n",
       "  'fundraising',\n",
       "  'glad',\n",
       "  'good',\n",
       "  'great',\n",
       "  'handiwork',\n",
       "  'hard',\n",
       "  'here',\n",
       "  'hey',\n",
       "  'hmm',\n",
       "  'idea',\n",
       "  'issue',\n",
       "  'last',\n",
       "  'later',\n",
       "  'like',\n",
       "  'little',\n",
       "  'lot',\n",
       "  'maybe',\n",
       "  'meeting',\n",
       "  'mini',\n",
       "  'money',\n",
       "  'music',\n",
       "  'news',\n",
       "  'nice',\n",
       "  'no',\n",
       "  'now',\n",
       "  'off',\n",
       "  'oh',\n",
       "  'opportunity',\n",
       "  'out',\n",
       "  'outdoors',\n",
       "  'over',\n",
       "  'perfect',\n",
       "  'person',\n",
       "  'picnic',\n",
       "  'place',\n",
       "  'plan',\n",
       "  'potluck',\n",
       "  'productive',\n",
       "  'project',\n",
       "  'proud',\n",
       "  'safety',\n",
       "  'sign',\n",
       "  'since',\n",
       "  'snack',\n",
       "  'some',\n",
       "  'something',\n",
       "  'sometimes',\n",
       "  'sore',\n",
       "  'space',\n",
       "  'spot',\n",
       "  'still',\n",
       "  'strange',\n",
       "  'support',\n",
       "  'sure',\n",
       "  'than',\n",
       "  'then',\n",
       "  'there',\n",
       "  'those',\n",
       "  'together',\n",
       "  'too',\n",
       "  'true',\n",
       "  'up',\n",
       "  'walk',\n",
       "  'way',\n",
       "  'week',\n",
       "  'weekend',\n",
       "  'well',\n",
       "  'what',\n",
       "  'when',\n",
       "  'without',\n",
       "  'work',\n",
       "  'worth',\n",
       "  'wow',\n",
       "  'yeah'},\n",
       " 'unused_vocab': {\"'s\",\n",
       "  'action',\n",
       "  'advice',\n",
       "  'ago',\n",
       "  'airport',\n",
       "  'alone',\n",
       "  'another',\n",
       "  'army',\n",
       "  'as',\n",
       "  'attention',\n",
       "  'away',\n",
       "  'beach',\n",
       "  'behind',\n",
       "  'body',\n",
       "  'book',\n",
       "  'boring',\n",
       "  'by',\n",
       "  'career',\n",
       "  'club',\n",
       "  'company',\n",
       "  'consequence',\n",
       "  'conversation',\n",
       "  'customer',\n",
       "  'dad',\n",
       "  'daughter',\n",
       "  'dead',\n",
       "  'detail',\n",
       "  'down',\n",
       "  'driver',\n",
       "  'efficiently',\n",
       "  'embarrassing',\n",
       "  'everybody',\n",
       "  'factory',\n",
       "  'front',\n",
       "  'furniture',\n",
       "  'garden',\n",
       "  'hand',\n",
       "  'heavy',\n",
       "  'her',\n",
       "  'hole',\n",
       "  'holiday',\n",
       "  'honestly',\n",
       "  'important',\n",
       "  'interest',\n",
       "  'job',\n",
       "  'journey',\n",
       "  'key',\n",
       "  'kid',\n",
       "  'late',\n",
       "  'letter',\n",
       "  'lunch',\n",
       "  'makeup',\n",
       "  'mom',\n",
       "  'movie',\n",
       "  'myself',\n",
       "  'name',\n",
       "  'negative',\n",
       "  'night',\n",
       "  'on',\n",
       "  'please',\n",
       "  'position',\n",
       "  'possible',\n",
       "  'product',\n",
       "  'proper',\n",
       "  'question',\n",
       "  'quiet',\n",
       "  'rain',\n",
       "  'reasonable',\n",
       "  'response',\n",
       "  'salary',\n",
       "  'seashell',\n",
       "  'she',\n",
       "  'south',\n",
       "  'story',\n",
       "  'table',\n",
       "  'their',\n",
       "  'thin',\n",
       "  'thought',\n",
       "  'through',\n",
       "  'tomorrow',\n",
       "  'useful',\n",
       "  'wall',\n",
       "  'weather',\n",
       "  'weight',\n",
       "  'while',\n",
       "  'world',\n",
       "  'yourself'}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_vocab_overlap(test_vocab_dict, vocab_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:13,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create index once - to retrieve matching flashcards that already exist\n",
    "from src.nlp import create_flashcard_index, get_matching_flashcards_indexed\n",
    "\n",
    "flashcard_index = create_flashcard_index(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(flashcard_index, \"test_flashcard_index.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "verb matches: 100%|██████████| 67/67 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 180/180 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 809/809 [00:00<00:00, 134869.90it/s]\n",
      "verb matches: 100%|██████████| 64/64 [00:00<00:00, 63897.99it/s]\n",
      "vocab matches: 100%|██████████| 173/173 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 788/788 [00:00<00:00, 196979.05it/s]\n",
      "verb matches: 100%|██████████| 60/60 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 168/168 [00:00<00:00, 168012.18it/s]\n",
      "ranking cards: 100%|██████████| 771/771 [00:00<00:00, 154270.03it/s]\n",
      "verb matches: 100%|██████████| 59/59 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 162/162 [00:00<00:00, 162398.96it/s]\n",
      "ranking cards: 100%|██████████| 762/762 [00:00<00:00, 189194.32it/s]\n",
      "verb matches: 100%|██████████| 57/57 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 157/157 [00:00<00:00, 156861.77it/s]\n",
      "ranking cards: 100%|██████████| 740/740 [00:00<00:00, 185145.85it/s]\n",
      "verb matches: 100%|██████████| 55/55 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 152/152 [00:00<00:00, 151757.73it/s]\n",
      "ranking cards: 100%|██████████| 719/719 [00:00<00:00, 119884.90it/s]\n",
      "verb matches: 100%|██████████| 53/53 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 148/148 [00:00<00:00, 148152.03it/s]\n",
      "ranking cards: 100%|██████████| 683/683 [00:00<00:00, 136662.04it/s]\n",
      "verb matches: 100%|██████████| 52/52 [00:00<00:00, 51966.60it/s]\n",
      "vocab matches: 100%|██████████| 143/143 [00:00<00:00, 143386.44it/s]\n",
      "ranking cards: 100%|██████████| 661/661 [00:00<00:00, 132253.73it/s]\n",
      "verb matches: 100%|██████████| 50/50 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 140/140 [00:00<00:00, 139743.59it/s]\n",
      "ranking cards: 100%|██████████| 608/608 [00:00<00:00, 202729.69it/s]\n",
      "verb matches: 100%|██████████| 48/48 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 137/137 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 596/596 [00:00<00:00, 149064.11it/s]\n",
      "verb matches: 100%|██████████| 46/46 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 135/135 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 582/582 [00:00<00:00, 145510.55it/s]\n",
      "verb matches: 100%|██████████| 45/45 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 132/132 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 576/576 [00:00<00:00, 192044.44it/s]\n",
      "verb matches: 100%|██████████| 44/44 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 129/129 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 542/542 [00:00<00:00, 180622.34it/s]\n",
      "verb matches: 100%|██████████| 44/44 [00:00<00:00, 44003.19it/s]\n",
      "vocab matches: 100%|██████████| 125/125 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 514/514 [00:00<00:00, 128585.96it/s]\n",
      "verb matches: 100%|██████████| 43/43 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 122/122 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 504/504 [00:00<00:00, 126061.73it/s]\n",
      "verb matches: 100%|██████████| 41/41 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 121/121 [00:00<00:00, 121066.50it/s]\n",
      "ranking cards: 100%|██████████| 497/497 [00:00<00:00, 165705.01it/s]\n",
      "verb matches: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 119/119 [00:00<00:00, 119093.81it/s]\n",
      "ranking cards: 100%|██████████| 475/475 [00:00<00:00, 52777.41it/s]\n",
      "verb matches: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 116/116 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 468/468 [00:00<00:00, 116952.71it/s]\n",
      "verb matches: 100%|██████████| 38/38 [00:00<00:00, 37984.64it/s]\n",
      "vocab matches: 100%|██████████| 115/115 [00:00<00:00, 115035.76it/s]\n",
      "ranking cards: 100%|██████████| 459/459 [00:00<00:00, 114758.32it/s]\n",
      "verb matches: 100%|██████████| 36/36 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 114/114 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 451/451 [00:00<00:00, 112778.34it/s]\n",
      "verb matches: 100%|██████████| 35/35 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 426/426 [00:00<00:00, 141976.44it/s]\n",
      "verb matches: 100%|██████████| 35/35 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 109/109 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 422/422 [00:00<00:00, 140699.23it/s]\n",
      "verb matches: 100%|██████████| 34/34 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 107/107 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 416/416 [00:00<00:00, 138698.77it/s]\n",
      "verb matches: 100%|██████████| 33/33 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 105/105 [00:00<00:00, 105107.86it/s]\n",
      "ranking cards: 100%|██████████| 394/394 [00:00<00:00, 98489.53it/s]\n",
      "verb matches: 100%|██████████| 33/33 [00:00<00:00, 33065.46it/s]\n",
      "vocab matches: 100%|██████████| 102/102 [00:00<00:00, 102007.39it/s]\n",
      "ranking cards: 100%|██████████| 375/375 [00:00<00:00, 125028.93it/s]\n",
      "verb matches: 100%|██████████| 32/32 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 100/100 [00:00<00:00, 99959.58it/s]\n",
      "ranking cards: 100%|██████████| 357/357 [00:00<00:00, 178576.81it/s]\n",
      "verb matches: 100%|██████████| 31/31 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 98/98 [00:00<00:00, 98124.08it/s]\n",
      "ranking cards: 100%|██████████| 307/307 [00:00<00:00, 153639.34it/s]\n",
      "verb matches: 100%|██████████| 30/30 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 96/96 [00:00<00:00, 96052.76it/s]\n",
      "ranking cards: 100%|██████████| 284/284 [00:00<00:00, 94628.40it/s]\n",
      "verb matches: 100%|██████████| 29/29 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 95/95 [00:00<00:00, 95006.89it/s]\n",
      "ranking cards: 100%|██████████| 280/280 [00:00<00:00, 280020.30it/s]\n",
      "verb matches: 100%|██████████| 29/29 [00:00<00:00, 28919.36it/s]\n",
      "vocab matches: 100%|██████████| 93/93 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 275/275 [00:00<00:00, 137542.76it/s]\n",
      "verb matches: 100%|██████████| 28/28 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 92/92 [00:00<00:00, 91984.74it/s]\n",
      "ranking cards: 100%|██████████| 259/259 [00:00<00:00, 86394.52it/s]\n",
      "verb matches: 100%|██████████| 27/27 [00:00<00:00, 26969.80it/s]\n",
      "vocab matches: 100%|██████████| 91/91 [00:00<00:00, 15167.77it/s]\n",
      "ranking cards: 100%|██████████| 256/256 [00:00<00:00, 128055.08it/s]\n",
      "verb matches: 100%|██████████| 26/26 [00:00<00:00, 8670.05it/s]\n",
      "vocab matches: 100%|██████████| 90/90 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 252/252 [00:00<00:00, 125934.07it/s]\n",
      "verb matches: 100%|██████████| 25/25 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 89/89 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 234/234 [00:00<00:00, 233793.98it/s]\n",
      "verb matches: 100%|██████████| 24/24 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 88/88 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 223/223 [00:00<00:00, 111521.38it/s]\n",
      "verb matches: 100%|██████████| 23/23 [00:00<00:00, 22996.18it/s]\n",
      "vocab matches: 100%|██████████| 87/87 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 220/220 [00:00<00:00, 110021.09it/s]\n",
      "verb matches: 100%|██████████| 22/22 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 86/86 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 213/213 [00:00<00:00, 106469.64it/s]\n",
      "verb matches: 100%|██████████| 22/22 [00:00<00:00, 22033.12it/s]\n",
      "vocab matches: 100%|██████████| 84/84 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 208/208 [00:00<00:00, 104057.16it/s]\n",
      "verb matches: 100%|██████████| 22/22 [00:00<00:00, 21970.16it/s]\n",
      "vocab matches: 100%|██████████| 82/82 [00:00<00:00, 82025.50it/s]\n",
      "ranking cards: 100%|██████████| 198/198 [00:00<00:00, 98983.57it/s]\n",
      "verb matches: 100%|██████████| 22/22 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 80/80 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 195/195 [00:00<00:00, 64978.89it/s]\n",
      "verb matches: 100%|██████████| 20/20 [00:00<00:00, 20006.22it/s]\n",
      "vocab matches: 100%|██████████| 80/80 [00:00<00:00, 80197.02it/s]\n",
      "ranking cards: 100%|██████████| 183/183 [00:00<00:00, 91441.22it/s]\n",
      "verb matches: 100%|██████████| 20/20 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 78/78 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 182/182 [00:00<00:00, 91006.60it/s]\n",
      "verb matches: 100%|██████████| 20/20 [00:00<00:00, 20006.22it/s]\n",
      "vocab matches: 100%|██████████| 76/76 [00:00<00:00, 75896.93it/s]\n",
      "ranking cards: 100%|██████████| 175/175 [00:00<00:00, 87443.79it/s]\n",
      "verb matches: 100%|██████████| 19/19 [00:00<00:00, 19010.44it/s]\n",
      "vocab matches: 100%|██████████| 75/75 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 168/168 [00:00<00:00, 168052.25it/s]\n",
      "verb matches: 100%|██████████| 18/18 [00:00<00:00, 18031.40it/s]\n",
      "vocab matches: 100%|██████████| 74/74 [00:00<00:00, 74023.01it/s]\n",
      "ranking cards: 100%|██████████| 152/152 [00:00<00:00, 75996.45it/s]\n",
      "verb matches: 100%|██████████| 17/17 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 73/73 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 144/144 [00:00<00:00, 28811.71it/s]\n",
      "verb matches: 100%|██████████| 15/15 [00:00<00:00, 15001.09it/s]\n",
      "vocab matches: 100%|██████████| 73/73 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 135/135 [00:00<00:00, 134752.75it/s]\n",
      "verb matches: 100%|██████████| 15/15 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 71/71 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 129/129 [00:00<00:00, 129194.18it/s]\n",
      "verb matches: 100%|██████████| 14/14 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 70/70 [00:00<00:00, 69971.71it/s]\n",
      "ranking cards: 100%|██████████| 121/121 [00:00<00:00, 121182.14it/s]\n",
      "verb matches: 100%|██████████| 14/14 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 68/68 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 117/117 [00:00<00:00, 116924.84it/s]\n",
      "verb matches: 100%|██████████| 14/14 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 67/67 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 115/115 [00:00<00:00, 57442.53it/s]\n",
      "verb matches: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 67/67 [00:00<00:00, 66988.88it/s]\n",
      "ranking cards: 100%|██████████| 112/112 [00:00<00:00, 111981.42it/s]\n",
      "verb matches: 100%|██████████| 13/13 [00:00<00:00, 13013.35it/s]\n",
      "vocab matches: 100%|██████████| 66/66 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 108/108 [00:00<00:00, 53971.74it/s]\n",
      "verb matches: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 65/65 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 106/106 [00:00<00:00, 105554.66it/s]\n",
      "verb matches: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 64/64 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 105/105 [00:00<00:00, 105107.86it/s]\n",
      "verb matches: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 63/63 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 98/98 [00:00<00:00, 97937.05it/s]\n",
      "verb matches: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 62/62 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 96/96 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 61/61 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 95/95 [00:00<00:00, 94893.76it/s]\n",
      "verb matches: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 60/60 [00:00<00:00, 60104.67it/s]\n",
      "ranking cards: 100%|██████████| 92/92 [00:00<00:00, 91962.81it/s]\n",
      "verb matches: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 60/60 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 86/86 [00:00<00:00, 85985.73it/s]\n",
      "verb matches: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 59/59 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 74/74 [00:00<00:00, 74182.24it/s]\n",
      "verb matches: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 58/58 [00:00<00:00, 57990.38it/s]\n",
      "ranking cards: 100%|██████████| 66/66 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 12/12 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 57/57 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 65/65 [00:00<00:00, 65020.21it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<00:00, 11000.80it/s]\n",
      "vocab matches: 100%|██████████| 57/57 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 63/63 [00:00<00:00, 62929.54it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 56/56 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 58/58 [00:00<00:00, 58087.30it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<00:00, 11000.80it/s]\n",
      "vocab matches: 100%|██████████| 55/55 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 57/57 [00:00<00:00, 57140.37it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 54/54 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 55/55 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 53/53 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 54/54 [00:00<00:00, 53991.04it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 52/52 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 53/53 [00:00<00:00, 52991.21it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 51/51 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 51/51 [00:00<00:00, 50846.09it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 50/50 [00:00<00:00, 49932.19it/s]\n",
      "ranking cards: 100%|██████████| 50/50 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 49/49 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 46/46 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<00:00, 11006.05it/s]\n",
      "vocab matches: 100%|██████████| 48/48 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 45/45 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 47/47 [00:00<00:00, 46958.62it/s]\n",
      "ranking cards: 100%|██████████| 41/41 [00:00<00:00, 40973.66it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<00:00, 11019.19it/s]\n",
      "vocab matches: 100%|██████████| 46/46 [00:00<00:00, 45992.37it/s]\n",
      "ranking cards: 100%|██████████| 37/37 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 11/11 [00:00<00:00, 10990.32it/s]\n",
      "vocab matches: 100%|██████████| 45/45 [00:00<00:00, 44971.09it/s]\n",
      "ranking cards: 100%|██████████| 36/36 [00:00<00:00, 36028.38it/s]\n",
      "verb matches: 100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 45/45 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 33/33 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 44/44 [00:00<00:00, 44055.71it/s]\n",
      "ranking cards: 100%|██████████| 32/32 [00:00<00:00, 32025.23it/s]\n",
      "verb matches: 100%|██████████| 10/10 [00:00<00:00, 4999.17it/s]\n",
      "vocab matches: 100%|██████████| 43/43 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 30/30 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 10/10 [00:00<00:00, 9995.96it/s]\n",
      "vocab matches: 100%|██████████| 42/42 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 28/28 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 10/10 [00:00<00:00, 5000.36it/s]\n",
      "vocab matches: 100%|██████████| 41/41 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 26/26 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 40/40 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 25/25 [00:00<00:00, 24989.90it/s]\n",
      "verb matches: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 40/40 [00:00<00:00, 39993.36it/s]\n",
      "ranking cards: 100%|██████████| 24/24 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 39/39 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 21/21 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 38/38 [00:00<00:00, 38066.29it/s]\n",
      "ranking cards: 100%|██████████| 17/17 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 37/37 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 16/16 [00:00<00:00, 15982.11it/s]\n",
      "verb matches: 100%|██████████| 9/9 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 36/36 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 36/36 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 7/7 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 35/35 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 34/34 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "verb matches: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 33/33 [00:00<?, ?it/s]\n",
      "ranking cards: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "verb matches: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "vocab matches: 100%|██████████| 32/32 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use indexed version for faster matching\n",
    "results = get_matching_flashcards_indexed(vocab_used, flashcard_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_flashcard_phrases = [card.get('phrase') for card in results['selected_cards']]\n",
    "vocab_from_flashcards = get_vocab_dictionary_from_phrases(proposed_flashcard_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERB ANALYSIS ===\n",
      "Original verbs: 67\n",
      "Verbs used in story: 103\n",
      "Verbs from original used: 59 (88.1%)\n",
      "New verbs introduced: 44\n",
      "Examples of new verbs: ['listen', 'provide', 'write', 'push', 'feed']\n",
      "\n",
      "=== VOCABULARY ANALYSIS ===\n",
      "Original vocabulary: 180\n",
      "Vocabulary used in story: 274\n",
      "Vocabulary from original used: 148 (82.2%)\n",
      "New vocabulary introduced: 126\n",
      "Examples of new vocabulary: ['problem', 'solution', 'noise', 'sorry', 'food']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verb_overlap': {\"'ve\",\n",
       "  'ask',\n",
       "  'be',\n",
       "  'believe',\n",
       "  'bring',\n",
       "  'build',\n",
       "  'can',\n",
       "  'check',\n",
       "  'come',\n",
       "  'could',\n",
       "  'discuss',\n",
       "  'do',\n",
       "  'eat',\n",
       "  'enjoy',\n",
       "  'excuse',\n",
       "  'expect',\n",
       "  'find',\n",
       "  'forget',\n",
       "  'get',\n",
       "  'give',\n",
       "  'go',\n",
       "  'happen',\n",
       "  'have',\n",
       "  'hear',\n",
       "  'help',\n",
       "  'hope',\n",
       "  'know',\n",
       "  'lead',\n",
       "  'let',\n",
       "  'like',\n",
       "  'look',\n",
       "  'love',\n",
       "  'maintain',\n",
       "  'meet',\n",
       "  'might',\n",
       "  'organize',\n",
       "  'pay',\n",
       "  'plan',\n",
       "  'play',\n",
       "  'remember',\n",
       "  'say',\n",
       "  'see',\n",
       "  'should',\n",
       "  'show',\n",
       "  'sound',\n",
       "  'speak',\n",
       "  'spend',\n",
       "  'start',\n",
       "  'support',\n",
       "  'suppose',\n",
       "  'take',\n",
       "  'thank',\n",
       "  'think',\n",
       "  'turn',\n",
       "  'want',\n",
       "  'wear',\n",
       "  'will',\n",
       "  'worry',\n",
       "  'would'},\n",
       " 'new_verbs': {'break',\n",
       "  'carry',\n",
       "  'choose',\n",
       "  'complain',\n",
       "  'cook',\n",
       "  'drop',\n",
       "  'end',\n",
       "  'ensure',\n",
       "  'explain',\n",
       "  'feed',\n",
       "  'feel',\n",
       "  'fight',\n",
       "  'follow',\n",
       "  'graduate',\n",
       "  'imagine',\n",
       "  'injure',\n",
       "  'listen',\n",
       "  'live',\n",
       "  'make',\n",
       "  'mind',\n",
       "  'move',\n",
       "  'must',\n",
       "  'need',\n",
       "  'note',\n",
       "  'provide',\n",
       "  'push',\n",
       "  'put',\n",
       "  'reach',\n",
       "  'read',\n",
       "  'save',\n",
       "  'seem',\n",
       "  'shall',\n",
       "  'shut',\n",
       "  'sit',\n",
       "  'sort',\n",
       "  'stand',\n",
       "  'talk',\n",
       "  'tend',\n",
       "  'throw',\n",
       "  'try',\n",
       "  'win',\n",
       "  'wonder',\n",
       "  'work',\n",
       "  'write'},\n",
       " 'unused_verbs': {'accomplish',\n",
       "  'brainstorm',\n",
       "  'create',\n",
       "  'cycle',\n",
       "  'delay',\n",
       "  'involve',\n",
       "  'plant',\n",
       "  'ride'},\n",
       " 'vocab_overlap': {'a',\n",
       "  'about',\n",
       "  'absolutely',\n",
       "  'after',\n",
       "  'afternoon',\n",
       "  'again',\n",
       "  'all',\n",
       "  'amazing',\n",
       "  'an',\n",
       "  'and',\n",
       "  'any',\n",
       "  'anything',\n",
       "  'around',\n",
       "  'at',\n",
       "  'back',\n",
       "  'beautiful',\n",
       "  'big',\n",
       "  'break',\n",
       "  'budget',\n",
       "  'business',\n",
       "  'but',\n",
       "  'bye',\n",
       "  'city',\n",
       "  'community',\n",
       "  'cycling',\n",
       "  'definitely',\n",
       "  'different',\n",
       "  'during',\n",
       "  'enough',\n",
       "  'even',\n",
       "  'evening',\n",
       "  'event',\n",
       "  'every',\n",
       "  'everyone',\n",
       "  'family',\n",
       "  'first',\n",
       "  'flower',\n",
       "  'for',\n",
       "  'forward',\n",
       "  'free',\n",
       "  'from',\n",
       "  'good',\n",
       "  'great',\n",
       "  'guitar',\n",
       "  'hard',\n",
       "  'he',\n",
       "  'helmet',\n",
       "  'here',\n",
       "  'his',\n",
       "  'how',\n",
       "  'i',\n",
       "  'idea',\n",
       "  'if',\n",
       "  'in',\n",
       "  'issue',\n",
       "  'it',\n",
       "  'just',\n",
       "  'last',\n",
       "  'later',\n",
       "  'like',\n",
       "  'little',\n",
       "  'local',\n",
       "  'location',\n",
       "  'lot',\n",
       "  'meeting',\n",
       "  'minister',\n",
       "  'money',\n",
       "  'month',\n",
       "  'more',\n",
       "  'music',\n",
       "  'my',\n",
       "  'neighborhood',\n",
       "  'new',\n",
       "  'news',\n",
       "  'newspaper',\n",
       "  'next',\n",
       "  'nice',\n",
       "  'no',\n",
       "  'not',\n",
       "  'now',\n",
       "  'of',\n",
       "  'off',\n",
       "  'office',\n",
       "  'oh',\n",
       "  'opportunity',\n",
       "  'our',\n",
       "  'out',\n",
       "  'over',\n",
       "  'park',\n",
       "  'party',\n",
       "  'path',\n",
       "  'people',\n",
       "  'person',\n",
       "  'picnic',\n",
       "  'place',\n",
       "  'plan',\n",
       "  'project',\n",
       "  'really',\n",
       "  'right',\n",
       "  'safety',\n",
       "  'sign',\n",
       "  'since',\n",
       "  'so',\n",
       "  'some',\n",
       "  'something',\n",
       "  'sometimes',\n",
       "  'soon',\n",
       "  'space',\n",
       "  'still',\n",
       "  'strange',\n",
       "  'sure',\n",
       "  'surprise',\n",
       "  'terrible',\n",
       "  'than',\n",
       "  'that',\n",
       "  'the',\n",
       "  'then',\n",
       "  'there',\n",
       "  'they',\n",
       "  'this',\n",
       "  'those',\n",
       "  'time',\n",
       "  'to',\n",
       "  'today',\n",
       "  'together',\n",
       "  'too',\n",
       "  'traffic',\n",
       "  'true',\n",
       "  'up',\n",
       "  'us',\n",
       "  'walk',\n",
       "  'way',\n",
       "  'we',\n",
       "  'week',\n",
       "  'weekend',\n",
       "  'well',\n",
       "  'what',\n",
       "  'when',\n",
       "  'where',\n",
       "  'which',\n",
       "  'with',\n",
       "  'without',\n",
       "  'work',\n",
       "  'worth',\n",
       "  'yeah',\n",
       "  'year',\n",
       "  'you',\n",
       "  'your'},\n",
       " 'new_vocab': {\"'s\",\n",
       "  'account',\n",
       "  'act',\n",
       "  'actually',\n",
       "  'agreement',\n",
       "  'already',\n",
       "  'another',\n",
       "  'as',\n",
       "  'attention',\n",
       "  'balance',\n",
       "  'bank',\n",
       "  'before',\n",
       "  'beginning',\n",
       "  'book',\n",
       "  'boy',\n",
       "  'by',\n",
       "  'cake',\n",
       "  'car',\n",
       "  'career',\n",
       "  'certain',\n",
       "  'clearly',\n",
       "  'coffee',\n",
       "  'concern',\n",
       "  'day',\n",
       "  'destination',\n",
       "  'detail',\n",
       "  'difference',\n",
       "  'dinner',\n",
       "  'direction',\n",
       "  'dog',\n",
       "  'door',\n",
       "  'down',\n",
       "  'driver',\n",
       "  'duck',\n",
       "  'each',\n",
       "  'early',\n",
       "  'else',\n",
       "  'email',\n",
       "  'embarrassing',\n",
       "  'entire',\n",
       "  'excited',\n",
       "  'factory',\n",
       "  'film',\n",
       "  'fine',\n",
       "  'food',\n",
       "  'fresh',\n",
       "  'garden',\n",
       "  'gesture',\n",
       "  'glass',\n",
       "  'hand',\n",
       "  'help',\n",
       "  'her',\n",
       "  'holiday',\n",
       "  'house',\n",
       "  'instead',\n",
       "  'interest',\n",
       "  'job',\n",
       "  'kind',\n",
       "  'late',\n",
       "  'least',\n",
       "  'letter',\n",
       "  'life',\n",
       "  'light',\n",
       "  'lottery',\n",
       "  'low',\n",
       "  'lunch',\n",
       "  'mom',\n",
       "  'much',\n",
       "  'name',\n",
       "  'near',\n",
       "  'never',\n",
       "  'night',\n",
       "  'noise',\n",
       "  'nothing',\n",
       "  'offer',\n",
       "  'open',\n",
       "  'operation',\n",
       "  'own',\n",
       "  'possible',\n",
       "  'pound',\n",
       "  'problem',\n",
       "  'quick',\n",
       "  'quiet',\n",
       "  'quietly',\n",
       "  'rate',\n",
       "  'rather',\n",
       "  'room',\n",
       "  'rumor',\n",
       "  'same',\n",
       "  'serious',\n",
       "  'she',\n",
       "  'shirt',\n",
       "  'shop',\n",
       "  'sick',\n",
       "  'simply',\n",
       "  'small',\n",
       "  'social',\n",
       "  'solution',\n",
       "  'someday',\n",
       "  'sorry',\n",
       "  'speech',\n",
       "  'spring',\n",
       "  'straight',\n",
       "  'stuff',\n",
       "  'sunset',\n",
       "  'system',\n",
       "  'team',\n",
       "  'themselves',\n",
       "  'thing',\n",
       "  'tomorrow',\n",
       "  'tonight',\n",
       "  'totally',\n",
       "  'toy',\n",
       "  'trouble',\n",
       "  'two',\n",
       "  'unlikely',\n",
       "  'upstairs',\n",
       "  'usual',\n",
       "  'victory',\n",
       "  'weather',\n",
       "  'while',\n",
       "  'wide',\n",
       "  'wife',\n",
       "  'world',\n",
       "  'yes',\n",
       "  'yesterday'},\n",
       " 'unused_vocab': {'6',\n",
       "  'able',\n",
       "  'agreed',\n",
       "  'alright',\n",
       "  'apparently',\n",
       "  'call',\n",
       "  'campaign',\n",
       "  'charge',\n",
       "  'construction',\n",
       "  'disappointing',\n",
       "  'downtown',\n",
       "  'empty',\n",
       "  'exactly',\n",
       "  'finger',\n",
       "  'fundraiser',\n",
       "  'fundraising',\n",
       "  'glad',\n",
       "  'handiwork',\n",
       "  'hey',\n",
       "  'hmm',\n",
       "  'maybe',\n",
       "  'mini',\n",
       "  'outdoors',\n",
       "  'perfect',\n",
       "  'potluck',\n",
       "  'productive',\n",
       "  'proud',\n",
       "  'snack',\n",
       "  'sore',\n",
       "  'spot',\n",
       "  'support',\n",
       "  'wow'}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_vocab_overlap(vocab_used, vocab_from_flashcards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"He's going to show us around the new office\",\n",
       " 'Do you think it could still happen, even if it seems unlikely?',\n",
       " 'Bye for now, see you later this evening!',\n",
       " \"I'm here to help in any way I can\",\n",
       " \"Shouldn't we spend more time with family?\",\n",
       " \"Instead of complaining, let's find a solution together.\",\n",
       " 'Look at that beautiful sunset over there',\n",
       " \"I'll get back to your email as soon as I can\",\n",
       " 'Remember when we met at the coffee shop last week?',\n",
       " \"Don't forget to wear a helmet while cycling\",\n",
       " \"Did the minister really support the community's interests?\",\n",
       " \"Did you hear they're shutting down the factory next month?\",\n",
       " 'Sometimes we just need to mind our own business',\n",
       " 'Make sure to pay attention to every little detail',\n",
       " 'Do you like playing the guitar at night?',\n",
       " 'I should have chosen a different career path',\n",
       " 'The traffic will be terrible for drivers during the holiday',\n",
       " 'She might forget to bring her lunch today',\n",
       " \"I'm planning to organize the office party soon\",\n",
       " \"Let's not worry so much about small things\",\n",
       " \"We must ensure everyone's safety first.\",\n",
       " \"I've never seen anything like this!\",\n",
       " 'How do you maintain work-life balance?',\n",
       " \"The operation was serious, but she's all right.\",\n",
       " 'Should we start from the beginning again?',\n",
       " 'I just want to sit quietly and read my book',\n",
       " 'Did you know he met his wife at a social event?',\n",
       " 'Did you enjoy the beautiful garden this afternoon?',\n",
       " \"She's writing a letter to the local newspaper\",\n",
       " 'I suppose the meeting will end soon',\n",
       " 'Shall we take a quick break?',\n",
       " \"They're turning off the lights early\",\n",
       " 'What did you say to him yesterday?',\n",
       " 'Stand up straight when you give your speech',\n",
       " 'I hope the weather will be nice tomorrow',\n",
       " 'Excuse me, where is the nearest bank?',\n",
       " \"I'm sorry, but that's simply not good enough.\",\n",
       " 'Even a low rate is better than nothing.',\n",
       " \"Yeah, I'm totally fine with that, no problem.\",\n",
       " \"Actually, I'd rather not discuss it, if possible.\",\n",
       " 'Have you been wearing the same shirt since then?',\n",
       " \"Shouldn't we try to save some money?\",\n",
       " 'Are you coming to the party next weekend?',\n",
       " \"We're building a new house next year\",\n",
       " 'I love listening to music while cooking dinner',\n",
       " 'Can you believe they ate the entire cake by themselves?',\n",
       " 'I want to live in a big city someday',\n",
       " 'He sounds excited about the new project',\n",
       " \"Wasn't that film absolutely amazing?\",\n",
       " 'Quick, help me carry this injured person to safety',\n",
       " 'Thank you so much for your kind gesture',\n",
       " \"Let's throw a surprise party for mom\",\n",
       " 'I want to live in a quiet neighborhood',\n",
       " 'I wonder if that rumor is true',\n",
       " 'She works hard to provide for her family',\n",
       " \"I tend to forget people's names, it's so embarrassing\",\n",
       " 'Should we move to another location?',\n",
       " 'Shall we go to the park to feed the ducks?',\n",
       " 'Do you check your account balance every day?',\n",
       " \"Let's try to sort out our differences\",\n",
       " \"Do you think they'll reach an agreement soon?\",\n",
       " 'Just follow the signs to reach your destination',\n",
       " 'Ask for help if you need it',\n",
       " \"Act now, before it's too late!\",\n",
       " \"I'm taking the dog for a walk\",\n",
       " 'Can you imagine a world without any troubles?',\n",
       " \"I'll definitely note your concerns about this.\",\n",
       " \"Yes, I'm sure it's worth at least a pound.\",\n",
       " \"Let's meet at the usual place tonight\",\n",
       " 'Is this space wide enough for us?',\n",
       " \"Let's talk about our holiday plans\",\n",
       " 'Have you already thought about this a lot?',\n",
       " \"Don't you think it's a good idea?\",\n",
       " 'Shall we think of something else?',\n",
       " \"I'm certain this is the right way forward.\",\n",
       " 'I want to lead the team to victory',\n",
       " \"Wouldn't it be great to win the lottery?\",\n",
       " 'Did you hear about the latest news?',\n",
       " 'Which job offer should I choose?',\n",
       " 'Look at the fresh flowers she put in each room',\n",
       " \"Don't forget to bring your own food to the picnic\",\n",
       " \"I'm expecting to graduate next spring\",\n",
       " 'Did you hear that strange noise from upstairs?',\n",
       " 'Push the door open with your free hand',\n",
       " \"Let's talk about the opportunity over coffee\",\n",
       " 'We should discuss this issue more',\n",
       " 'Speak clearly when you give directions',\n",
       " 'Look at those two boys fighting over a toy car',\n",
       " 'I feel sick after eating that stuff',\n",
       " 'Oh no, I dropped the glass and it broke!',\n",
       " 'They just explained the budget system to me']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': {'accomplish',\n",
       "  'brainstorm',\n",
       "  'create',\n",
       "  'cycle',\n",
       "  'delay',\n",
       "  'involve',\n",
       "  'plant',\n",
       "  'ride'},\n",
       " 'vocab': {'6',\n",
       "  'able',\n",
       "  'agreed',\n",
       "  'alright',\n",
       "  'apparently',\n",
       "  'call',\n",
       "  'campaign',\n",
       "  'charge',\n",
       "  'construction',\n",
       "  'disappointing',\n",
       "  'downtown',\n",
       "  'empty',\n",
       "  'exactly',\n",
       "  'finger',\n",
       "  'fundraiser',\n",
       "  'fundraising',\n",
       "  'glad',\n",
       "  'handiwork',\n",
       "  'hey',\n",
       "  'hmm',\n",
       "  'maybe',\n",
       "  'mini',\n",
       "  'outdoors',\n",
       "  'perfect',\n",
       "  'potluck',\n",
       "  'productive',\n",
       "  'proud',\n",
       "  'snack',\n",
       "  'sore',\n",
       "  'spot',\n",
       "  'support',\n",
       "  'wow'}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['remaining_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_phrases = get_matching_flashcards(vocab_used, phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate story from broad Longman vocabulary pool\n",
    "story = generate_story_from_vocab(longman_vocab_dict)\n",
    "\n",
    "2. Extract all vocabulary used in story\n",
    "story_vocab = extract_vocab_from_story(story)\n",
    "\n",
    "3. Compare with Longman dictionary to identify source\n",
    "used_vocab = compare_vocab_overlap(longman_vocab_dict, story_vocab)\n",
    "\n",
    "4. get existing flashcards we already have\n",
    "5. generate new ones\n",
    ". Generate flashcards for:\n",
    "    - All story vocabulary that appears in Longman list\n",
    "    - Common connecting words needed for natural speech\n",
    "    - Tag cards with which story they appear in\n",
    "flashcards = generate_flashcards_for_story(story_vocab, story_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSegment.silent(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm._load_google_voices(\"fr-FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "locale=\"fr-FR\"\n",
    "language_object = Language.get(locale)\n",
    "speech_key = os.getenv(\"AZURE_API_KEY\")\n",
    "if not speech_key:\n",
    "    print(\"Warning: AZURE_API_KEY not found in environment variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "service_region = os.getenv(\"AZURE_REGION\", \"eastus\")\n",
    "speech_config = speechsdk.SpeechConfig(\n",
    "    subscription=speech_key, region=service_region\n",
    ")\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(\n",
    "    speech_config=speech_config\n",
    ")\n",
    "\n",
    "result = speech_synthesizer.get_voices_async(locale=locale).get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = result.voices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice.local_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = result.voices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice.voice_type._name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for voice in result.voices:\n",
    "    voice_type = (\n",
    "        VoiceType.NEURAL\n",
    "        if voice.voice_type._name_ == \"OnlineNeural\"\n",
    "        else VoiceType.STANDARD\n",
    "    )\n",
    "\n",
    "    voice_info = VoiceInfo(\n",
    "        name=voice.local_name,\n",
    "        provider=VoiceProvider.AZURE,\n",
    "        voice_type=voice_type,\n",
    "        gender=voice.gender._name_.upper(),\n",
    "        language_code=voice.locale,\n",
    "        country_code=language_object.territory,\n",
    "        voice_id=voice.short_name,\n",
    "    )\n",
    "\n",
    "    all.append(voice_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm._load_azure_voices(\"fr-FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "response = client.list_voices(language_code=\"fr-FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.voices[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang.is_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang.territory_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang.language_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.translation import translate_from_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_from_english(\"hello\", \"cmn-TW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config._load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.get_voice_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.voice_manager._load_azure_voices(\"fr-FR\")\n",
    "config.voice_manager.voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.voice_manager._lazy_load_voices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.voice_manager.voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = AnkiCollectionReader(ANKI_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = reader.col.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config._load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_knowledge_score(collection_path: str, deck_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate a knowledge score (0-1) for each card in the deck based on review history.\n",
    "    \n",
    "    Factors considered:\n",
    "    - Current interval (longer intervals suggest better knowledge)\n",
    "    - Ease factor (higher ease suggests better retention)\n",
    "    - Review success rate (ratio of Good/Easy vs Again buttons)\n",
    "    - Time since last review (recent successful reviews weighted more)\n",
    "    - Review time trends (decreasing review times suggest familiarity)\n",
    "    \n",
    "    Args:\n",
    "        collection_path: Path to the .anki2 collection file\n",
    "        deck_name: Name of the deck to analyze\n",
    "    \n",
    "    Returns:\n",
    "        dict: Card IDs mapped to their knowledge scores and contributing factors\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    scores = {}\n",
    "    with AnkiCollectionReader(collection_path) as reader:\n",
    "        # Get deck ID\n",
    "        deck = reader.col.decks.by_name(deck_name)\n",
    "        if not deck:\n",
    "            raise ValueError(f\"Deck '{deck_name}' not found\")\n",
    "            \n",
    "        # Get all cards in deck\n",
    "        card_ids = reader.col.find_cards(f\"did:{deck['id']}\")\n",
    "        \n",
    "        # For each card, analyze its review history\n",
    "        for card_id in card_ids:\n",
    "            card = reader.col.get_card(card_id)\n",
    "            note = reader.col.get_note(card.nid)\n",
    "            \n",
    "            # Get review logs for this card\n",
    "            reviews = reader.col.db.all(\n",
    "                \"SELECT ease, ivl, factor, time, type FROM revlog WHERE cid = ? ORDER BY id\",\n",
    "                card_id\n",
    "            )\n",
    "            \n",
    "            if not reviews:\n",
    "                scores[card_id] = {\n",
    "                    'score': 0,\n",
    "                    'reason': 'No reviews yet',\n",
    "                    'note_fields': dict(note.items())\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            # Calculate component scores\n",
    "            \n",
    "            # 1. Interval score (0-0.4): Longer intervals suggest better knowledge\n",
    "            max_interval = 365  # Cap at 1 year for scoring\n",
    "            current_interval = abs(card.ivl)  # Use absolute value to handle negative intervals\n",
    "            interval_score = min(current_interval / max_interval, 1) * 0.4\n",
    "            \n",
    "            # 2. Ease score (0-0.2): Higher ease factors suggest better retention\n",
    "            min_ease = 1300  # Minimum ease factor\n",
    "            max_ease = 3100  # Maximum ease factor\n",
    "            ease_score = (card.factor - min_ease) / (max_ease - min_ease) * 0.2\n",
    "            ease_score = max(0, min(ease_score, 0.2))  # Clamp between 0-0.2\n",
    "            \n",
    "            # 3. Review success score (0-0.3)\n",
    "            success_count = sum(1 for r in reviews if r[0] >= 3)  # Count Good/Easy\n",
    "            total_reviews = len(reviews)\n",
    "            success_score = (success_count / total_reviews) * 0.3 if total_reviews > 0 else 0\n",
    "            \n",
    "            # 4. Review time trend score (0-0.1)\n",
    "            # Lower and/or decreasing review times suggest familiarity\n",
    "            if len(reviews) >= 3:\n",
    "                recent_times = [r[3] for r in reviews[-3:]]  # Last 3 review times\n",
    "                avg_time = sum(recent_times) / len(recent_times)\n",
    "                time_score = min(1, max(0, (30000 - avg_time) / 30000)) * 0.1  # Scale around 30s\n",
    "            else:\n",
    "                time_score = 0\n",
    "                \n",
    "            # Calculate final score\n",
    "            final_score = interval_score + ease_score + success_score + time_score\n",
    "            \n",
    "            # Store results\n",
    "            scores[card_id] = {\n",
    "                'score': round(final_score, 3),\n",
    "                'components': {\n",
    "                    'interval_score': round(interval_score, 3),\n",
    "                    'ease_score': round(ease_score, 3),\n",
    "                    'success_score': round(success_score, 3),\n",
    "                    'time_score': round(time_score, 3)\n",
    "                },\n",
    "                'stats': {\n",
    "                    'current_interval': current_interval,\n",
    "                    'ease_factor': card.factor,\n",
    "                    'review_success_rate': round(success_count / total_reviews, 2) if total_reviews > 0 else 0,\n",
    "                    'total_reviews': total_reviews\n",
    "                },\n",
    "                'note_fields': dict(note.items())\n",
    "            }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for card_id, data in scores.items():\n",
    "        row = {\n",
    "            'card_id': card_id\n",
    "        }\n",
    "        \n",
    "        # Add note fields\n",
    "        row.update(data['note_fields'])\n",
    "        \n",
    "        # Add component scores\n",
    "        if 'components' in data:\n",
    "            row.update(data['components'])\n",
    "        \n",
    "        # Add statistics\n",
    "        if 'stats' in data:\n",
    "            row.update(data['stats'])\n",
    "        \n",
    "        # Add final score\n",
    "        row['knowledge_score'] = data['score']\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Reorder columns to have card_id first, then content fields, then scores\n",
    "    score_cols = ['interval_score', 'ease_score', 'success_score', 'time_score', 'knowledge_score']\n",
    "    stat_cols = ['current_interval', 'ease_factor', 'review_success_rate', 'total_reviews']\n",
    "    content_cols = [col for col in df.columns if col not in score_cols + stat_cols + ['card_id']]\n",
    "    \n",
    "    df = df[['card_id'] + content_cols + stat_cols + score_cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_knowledge_scores(scores: dict, num_examples: int = 5):\n",
    "    \"\"\"\n",
    "    Print formatted knowledge scores with examples of high and low scoring cards.\n",
    "    \n",
    "    Args:\n",
    "        scores: Dictionary of scores from calculate_knowledge_score()\n",
    "        num_examples: Number of high/low scoring examples to show\n",
    "    \"\"\"\n",
    "    # Calculate overall statistics\n",
    "    all_scores = [s['score'] for s in scores.values()]\n",
    "    avg_score = sum(all_scores) / len(all_scores) if all_scores else 0\n",
    "    \n",
    "    print(f\"Analysis of {len(scores)} cards:\")\n",
    "    print(f\"Average knowledge score: {avg_score:.3f}\")\n",
    "    print(f\"Score distribution:\")\n",
    "    \n",
    "    # Show score distribution\n",
    "    ranges = [(0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]\n",
    "    for low, high in ranges:\n",
    "        count = sum(1 for s in all_scores if low <= s < high)\n",
    "        print(f\"{low:.1f}-{high:.1f}: {count} cards ({count/len(all_scores)*100:.1f}%)\")\n",
    "    \n",
    "    # Show examples of highest and lowest scoring cards\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1]['score'])\n",
    "    \n",
    "    print(f\"\\nLowest {num_examples} scoring cards:\")\n",
    "    for card_id, data in sorted_scores[:num_examples]:\n",
    "        print(f\"\\nCard ID: {card_id}\")\n",
    "        print(f\"Score: {data['score']:.3f}\")\n",
    "        if 'components' in data:\n",
    "            print(\"Score components:\")\n",
    "            for component, value in data['components'].items():\n",
    "                print(f\"  {component}: {value:.3f}\")\n",
    "        print(\"Content:\")\n",
    "        for field, content in data['note_fields'].items():\n",
    "            # Truncate long content for display\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"  {field}: {content_preview}\")\n",
    "    \n",
    "    print(f\"\\nHighest {num_examples} scoring cards:\")\n",
    "    for card_id, data in sorted_scores[-num_examples:]:\n",
    "        print(f\"\\nCard ID: {card_id}\")\n",
    "        print(f\"Score: {data['score']:.3f}\")\n",
    "        if 'components' in data:\n",
    "            print(\"Score components:\")\n",
    "            for component, value in data['components'].items():\n",
    "                print(f\"  {component}: {value:.3f}\")\n",
    "        print(\"Content:\")\n",
    "        for field, content in data['note_fields'].items():\n",
    "            # Truncate long content for display\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"  {field}: {content_preview}\")\n",
    "\n",
    "# Example usage:\n",
    "# scores = calculate_knowledge_score(\"path/to/collection.anki2\", \"My Deck\")\n",
    "# print_knowledge_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_knowledge_score(ANKI_PATH, \"RapidRetention - Swedish - LM1000\")\n",
    "\n",
    "# View summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort by knowledge score\n",
    "df.groupby(\"EnglishText\").agg({\"knowledge_score\" : \"mean\"}).sort_values(by=\"knowledge_score\", ascending=False).head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_phrase = \"4-5 words long, but treat common lexical chunks (I'm going to.., Do you.., Let us.. etc) as a single word\"\n",
    "verbs_per_phrase = \"one verb (but OK for an additional adverb if required)\"\n",
    "gcse_phrases = generate_phrases_from_vocab_dict(vocab_dict=gcse_vocab, max_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcse_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm._lazy_load_voices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_text = \"سلام دنیا\"\n",
    "aud = slow_text_to_speech(persian_text, config_language=\"target\", gender=\"MALE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_remove_within_brackets(\"falling (over)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from src.phrase import generate_phrases_from_vocab_dict\n",
    "\n",
    "@dataclass\n",
    "class ProcessingResult:\n",
    "    pairs: List[Tuple[str, str]]\n",
    "    problem_lines: Dict[int, str]  # line number -> original line content\n",
    "    skipped_lines: Dict[int, str]  # line number -> reason for skipping\n",
    "\n",
    "def process_anki_file(file_path: str) -> ProcessingResult:\n",
    "    \"\"\"\n",
    "    Process tab-separated Anki export data from a file and pair up variations.\n",
    "    If there are more English variations than French, repeat the French term.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the tab-separated text file\n",
    "        \n",
    "    Returns:\n",
    "        ProcessingResult containing:\n",
    "        - processed pairs\n",
    "        - dictionary of problem lines\n",
    "        - dictionary of skipped lines\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    problem_lines = {}\n",
    "    skipped_lines = {}\n",
    "    \n",
    "    # Read and process the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            # Skip empty lines and comments\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                skipped_lines[line_num] = \"Empty line\"\n",
    "                continue\n",
    "            if line.startswith('#'):\n",
    "                skipped_lines[line_num] = \"Comment line\"\n",
    "                continue\n",
    "                \n",
    "            # Split line into columns\n",
    "            columns = line.split('\\t')\n",
    "            if len(columns) < 2:\n",
    "                skipped_lines[line_num] = \"Insufficient columns\"\n",
    "                continue\n",
    "                \n",
    "            french_terms = columns[0].strip()\n",
    "            english_terms = columns[1].strip()\n",
    "            \n",
    "            # Split variations\n",
    "            french_variations = [term.strip() for term in french_terms.split('/')]\n",
    "            english_variations = [term.strip() for term in english_terms.replace('/', ',').split(',')]\n",
    "            \n",
    "            # If there are more English variations than French ones\n",
    "            if len(french_variations) < len(english_variations):\n",
    "                if len(french_variations) == 1:  # If there's only one French term, repeat it\n",
    "                    french_term = french_variations[0]\n",
    "                    # Filter out 'i.e.' from English variations\n",
    "                    english_variations = [eng for eng in english_variations if eng.lower() != 'i.e.']\n",
    "                    # Create pairs with repeated French term\n",
    "                    for eng in english_variations:\n",
    "                        if eng:  # Only add if English term is non-empty\n",
    "                            result.append((french_term, eng))\n",
    "                else:\n",
    "                    # If multiple French terms but still fewer than English, record problem\n",
    "                    problem_lines[line_num] = line\n",
    "                    print(f\"\\nWARNING - Complex mismatch on line {line_num}:\")\n",
    "                    print(f\"French variations ({len(french_variations)}): {french_variations}\")\n",
    "                    print(f\"English variations ({len(english_variations)}): {english_variations}\")\n",
    "            else:\n",
    "                # Normal case where French variations >= English variations\n",
    "                for fr, eng in zip(french_variations, english_variations):\n",
    "                    if fr and eng:  # Only add if both terms are non-empty\n",
    "                        result.append((fr, eng))\n",
    "    \n",
    "    return ProcessingResult(pairs=result, problem_lines=problem_lines, skipped_lines=skipped_lines)\n",
    "\n",
    "\n",
    "processed_data = process_anki_file(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_word_pairs(pairs: List[Tuple[str, str]]) -> Dict[str, Set[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Categorize word pairs into verbs and vocab based on English 'to ' prefix.\n",
    "    \n",
    "    Args:\n",
    "        pairs: List of tuples containing (french_word, english_word)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'verbs' and 'vocab' keys containing sets of (french, english) pairs\n",
    "    \"\"\"\n",
    "    vocab_dict = {\n",
    "        'verbs': set(),\n",
    "        'vocab': set()\n",
    "    }\n",
    "    \n",
    "    for french, english in pairs:\n",
    "        # Clean up any trailing/leading whitespace\n",
    "        french = french.strip()\n",
    "        english = english.strip()\n",
    "        \n",
    "        # Check if it's a verb (starts with 'to ')\n",
    "        if english.lower().startswith('to ') | english.lower().startswith('to be '):\n",
    "            # Remove 'to ' and add to verbs\n",
    "            english_cleaned = english[3:].strip()  # Remove 'to ' prefix\n",
    "            vocab_dict['verbs'].add(english_cleaned)\n",
    "        else:\n",
    "            # Add to vocab\n",
    "            vocab_dict['vocab'].add(english)\n",
    "    \n",
    "    vocab_dict['vocab'] = list(vocab_dict['vocab'])\n",
    "    vocab_dict['verbs'] = list(vocab_dict['verbs'])\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcse_dict = categorize_word_pairs(processed_data.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_json\n",
    "\n",
    "\n",
    "save_json(gcse_dict, \"../outputs/gcse_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_phrases = generate_phrases_from_vocab_dict(gcse_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Configure speech service\n",
    "speech_key = os.getenv(\"AZURE_API_KEY\")\n",
    "service_region = \"eastus\"  # Default region\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Get available voices\n",
    "result = speech_synthesizer.get_voices_async().get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voice in result.voices:\n",
    "    print(voice._short_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voice in result.voices:\n",
    "    print(voice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from IPython.display import Audio\n",
    "import io\n",
    "\n",
    "# Configure speech service\n",
    "speech_key = os.getenv(\"AZURE_API_KEY\")\n",
    "service_region = \"eastus\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "# Note: the voice setting will not overwrite the voice element in input SSML.\n",
    "speech_config.speech_synthesis_voice_name = \"en-US-AmberNeural\"\n",
    "\n",
    "text = \"Hello World!\"\n",
    "\n",
    "# use the default speaker as audio output.\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "result = speech_synthesizer.speak_text_async(text).get()\n",
    "# Check result\n",
    "if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesized for text [{}]\".format(text))\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.audio_generation import play_audio\n",
    "\n",
    "display(Audio(result.audio_data, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# #from src.config_loader import config\n",
    "# from src.utils import create_html_story, create_test_story_dict, load_json\n",
    "# from src.audio_generation import text_to_speech\n",
    "# from src.translation import tokenize_text\n",
    "# from src.anki_tools import generate_wiktionary_links\n",
    "from src.nlp import filter_matching_phrases\n",
    "from src.utils import load_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"../data/gcse_vocab_list_cambridge.json\", \"r\") as gcse:\n",
    "    gcse_dict = json.load(gcse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/longman_phrases_convo_1000.txt\", \"r\") as core:\n",
    "    core_phrases = [line.strip(\"\\n\") for line in core.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcse_phrases = filter_matching_phrases(core_phrases, gcse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcse_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict = load_json(\"..\\data\\longman_phrase_images\\phrase_image_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "for key in image_dict:\n",
    "    if isinstance(image_dict.get(key), str):\n",
    "        continue\n",
    "    else:\n",
    "        phrases.append(image_dict[key]['phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "test_phrases = random.sample(phrases, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(directory_path):\n",
    "    \"\"\"\n",
    "    Merge multiple JSON files containing transformed phrases into a single JSON file,\n",
    "    removing duplicates based on both original and conversational fields.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Merged dictionary containing unique transformed phrases\n",
    "    \"\"\"\n",
    "    # Use a set to track unique phrases (as tuples of original and conversational)\n",
    "    unique_phrases = set()\n",
    "    all_phrases = []\n",
    "    \n",
    "    # Convert directory path to Path object\n",
    "    dir_path = Path(directory_path)\n",
    "    \n",
    "    # Counter for tracking statistics\n",
    "    total_phrases = 0\n",
    "    \n",
    "    # Iterate through all JSON files in the directory\n",
    "    for json_file in dir_path.glob('*.json'):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Extract transformed_phrases from each file\n",
    "                if 'transformed_phrases' in data:\n",
    "                    phrases = data['transformed_phrases']\n",
    "                    total_phrases += len(phrases)\n",
    "                    \n",
    "                    # Process each phrase\n",
    "                    for phrase in phrases:\n",
    "                        # Create tuple of the phrase fields for uniqueness checking\n",
    "                        phrase_tuple = (phrase['original'], phrase['conversational'])\n",
    "                        \n",
    "                        # Only add if we haven't seen this combination before\n",
    "                        if phrase_tuple not in unique_phrases:\n",
    "                            unique_phrases.add(phrase_tuple)\n",
    "                            all_phrases.append(phrase)\n",
    "                else:\n",
    "                    print(f\"Warning: 'transformed_phrases' not found in {json_file}\")\n",
    "                    \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not parse JSON from {json_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {str(e)}\")\n",
    "    \n",
    "    # Create merged dictionary\n",
    "    merged_data = {\n",
    "        \"transformed_phrases\": all_phrases\n",
    "    }\n",
    "    \n",
    "    # Save merged data to a new JSON file\n",
    "    output_path = dir_path / 'merged_phrases.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Print statistics\n",
    "    duplicates = total_phrases - len(all_phrases)\n",
    "    print(f\"Processing complete:\")\n",
    "    print(f\"- Total phrases processed: {total_phrases}\")\n",
    "    print(f\"- Unique phrases: {len(all_phrases)}\")\n",
    "    print(f\"- Duplicates removed: {duplicates}\")\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dat = merge_json_files(\"../outputs/phrase_changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversational_phrases(transformed_phrases, output_file:str =\"../data/longman_phrases_convo_1000.txt\") -> None:\n",
    "    \"\"\"\n",
    "    Extract and save conversational phrases to a text file.\n",
    "    \n",
    "    Args:\n",
    "        transformed_phrases: List of dictionaries containing 'original' and 'conversational' phrases\n",
    "        output_file: Path to the output text file\n",
    "        \n",
    "    Prints summary of the operation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for phrase_pair in transformed_phrases:\n",
    "                f.write(phrase_pair['conversational'] + '\\n')\n",
    "                \n",
    "        # Count phrases saved\n",
    "        phrase_count = len(transformed_phrases)\n",
    "        print(f\"\\nOperation Summary:\")\n",
    "        print(f\"Successfully saved {phrase_count} phrases to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving phrases to {output_file}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_conversational_phrases(merged_dat[\"transformed_phrases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Dict, List\n",
    "from src.utils import clean_filename\n",
    "\n",
    "def copy_rename_phrase_images(transformed_phrases: List[Dict[str, str]], image_dir: str=\"..\\data\\longman_phrase_images\") -> None:\n",
    "    \"\"\"\n",
    "    Copy and rename image files based on transformed phrases.\n",
    "    \n",
    "    Args:\n",
    "        transformed_phrases: List of dictionaries containing 'original' and 'conversational' phrases\n",
    "        image_dir: Directory containing the image files\n",
    "        \n",
    "    Prints summary of operations and lists any missing original images.\n",
    "    \"\"\"\n",
    "    successful_copies = 0\n",
    "    missing_originals = []\n",
    "    \n",
    "    # Ensure image directory exists\n",
    "    if not os.path.exists(image_dir):\n",
    "        raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
    "        \n",
    "    for phrase_pair in transformed_phrases:\n",
    "        original_phrase = phrase_pair['original']\n",
    "        conversational_phrase = phrase_pair['conversational']\n",
    "        \n",
    "        # Generate filenames\n",
    "        original_filename = clean_filename(original_phrase) + '.png'\n",
    "        new_filename = clean_filename(conversational_phrase) + '.png'\n",
    "        \n",
    "        original_path = os.path.join(image_dir, original_filename)\n",
    "        new_path = os.path.join(image_dir, new_filename)\n",
    "        \n",
    "        # Check if original file exists\n",
    "        if not os.path.exists(original_path):\n",
    "            missing_originals.append(original_phrase)\n",
    "            continue\n",
    "            \n",
    "        # Skip if destination file already exists\n",
    "        if os.path.exists(new_path):\n",
    "            print(f\"Warning: Destination file already exists, skipping: {new_filename}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Copy the file\n",
    "            shutil.copy2(original_path, new_path)\n",
    "            successful_copies += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {original_filename} to {new_filename}: {str(e)}\")\n",
    "            \n",
    "    # Print summary\n",
    "    print(\"\\nOperation Summary:\")\n",
    "    print(f\"Successfully copied and renamed: {successful_copies} images\")\n",
    "    print(f\"Missing original images: {len(missing_originals)}\")\n",
    "    \n",
    "    if missing_originals:\n",
    "        print(\"\\nMissing original images for these phrases:\")\n",
    "        for phrase in missing_originals:\n",
    "            print(f\"- {phrase}\")\n",
    "            \n",
    "    return successful_copies, missing_originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_copies, missing_originals = copy_rename_phrase_images(merged_dat['transformed_phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_phrase_chunk():\n",
    "    for index in range(0,len(phrases), 50):\n",
    "        yield phrases[index:index+50]\n",
    "\n",
    "phrase_iter= next_phrase_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(phrase_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Transform each sentence into a more natural, conversational phrase. Each original phrase has an associated teaching image, so the core situation and meaning MUST remain identical. Your response should be a valid JSON object with this structure:\n",
    "\n",
    "{{\n",
    "  \"transformed_phrases\": [\n",
    "    {{\n",
    "      \"original\": \"<original phrase>\",\n",
    "      \"conversational\": \"<transformed phrase>\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Rules for transformation:\n",
    "\n",
    "1. Keep ALL key vocabulary from the original (nouns, verbs, adjectives) - as these phrases are for learning that vocab\n",
    "\n",
    "2. Maintain the EXACT same situation/scene as the original, since it matches an existing image\n",
    "\n",
    "3. Transform into the most natural way someone would express this idea in conversation, mainly replace phrases that are narrative and passive. Replace with:\n",
    "\n",
    "   - Imperative (\"Don't be late...\")\n",
    "   - Simple statements (\"The traffic was terrible...\")\n",
    "   - First-person expressions (\"I enjoy...\")\n",
    "   - Questions (\"Shall we...?\", \"Do you ...?\", \"Did they...?\" etc)\n",
    "4. Keep the original phrase if a change isn't required or modifying the phrase would result in something unnatural sounding\n",
    "\n",
    "Examples:\n",
    "\n",
    "{{\n",
    "\"original\": \"We watched the sunrise over the mountains\",\n",
    "\"conversational\": \"We watched the sunrise over the mountains\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"She will call you as soon as possible\",\n",
    "\"conversational\": \"She will call you as soon as possible\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"They joined the protest against high taxes\",\n",
    "\"conversational\": \"Did you join the protest against high taxes?\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"He will receive an award for his work\",\n",
    "\"conversational\": \"I'm receiving an award for my work\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"She likes to play with her pet cat\",\n",
    "\"conversational\": \"Do you like playing with your pet cat?\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"He covered his face with his hands\",\n",
    "\"conversational\": \"Cover your face with your hands\"\n",
    "}},\n",
    "\n",
    "Here are the phrases to transform:\n",
    "\n",
    "{next(phrase_iter)}\n",
    "\n",
    "Output only valid JSON following the structure above.\"\"\"\n",
    "\n",
    "pyperclip.copy(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "pyperclip.copy(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[image_dict.get(key).get('phrase', '') for key in image_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = tokenize_text(\"hello world\", language_code=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = generate_wiktionary_links(\"こんにちは世界\", \"Japanese\", \"ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segment = slow_text_to_speech(\n",
    "        text=\"let's speak slowly and clearly\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.audio_generation import slow_text_to_speech\n",
    "from IPython.display import Audio\n",
    "import io\n",
    "\n",
    "# Test cases with languages, voice names, and potential issues\n",
    "test_phrases = [\n",
    "    # English with apostrophes\n",
    "    (\"I can't believe it!\", \"en-GB\", \"en-GB-Wavenet-D\"),\n",
    "    (\"Don't you'll I'm they're\", \"en-GB\", \"en-GB-Wavenet-D\"),\n",
    "    \n",
    "    # HTML entities that might appear\n",
    "    (\"Let&#39;s go &amp; have fun!\", \"en-US\", \"en-US-Wavenet-D\"),\n",
    "    \n",
    "    # Italian with apostrophes\n",
    "    (\"L'italiano è bellissimo\", \"it-IT\", \"it-IT-Wavenet-A\"),\n",
    "    \n",
    "    # Japanese (no apostrophes but needs tokenization)\n",
    "    (\"私は日本語を勉強しています\", \"ja-JP\", \"ja-JP-Wavenet-B\"),\n",
    "    \n",
    "    # Chinese test\n",
    "    (\"我正在学习中文\", \"zh-CN\", \"cmn-CN-Wavenet-A\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# Test each phrase and play the audio\n",
    "for text, lang_code, voice in test_phrases[0:2]:\n",
    "    print(f\"\\nTesting: {text}\")\n",
    "    print(f\"Language: {lang_code}\")\n",
    "    print(f\"Voice: {voice}\")\n",
    "    \n",
    "    audio_segment = slow_text_to_speech(\n",
    "        text, \n",
    "        language_code=lang_code,\n",
    "        voice_name=voice\n",
    "    )\n",
    "    \n",
    "    # Convert to format playable in notebook\n",
    "    buffer = io.BytesIO()\n",
    "    audio_segment.export(buffer, format=\"wav\")\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Display audio player\n",
    "    display(Audio(buffer.read(), rate=audio_segment.frame_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper = EnhancedSSMLClipper(\n",
    "    word_rate=\"0.85\",    # Very slow\n",
    "    word_pitch=\"-1st\",   # Slightly lower\n",
    "    break_time=\"300ms\",  # Longer breaks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Russian text: \"Hello! How are you? I am learning Russian.\"\n",
    "russian_text = \"Здравствуйте! Как дела? Я изучаю русский язык.\"\n",
    "\n",
    "# Russian neural voice name\n",
    "russian_voice = \"ru-RU-Standard-B\"  # Female voice\n",
    "clipper.synthesize_speech(russian_text, \"enhanced.mp3\", russian_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = text_to_speech(russian_text, \"ru\", russian_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_voice = \"it-IT-Neural2-C\"\n",
    "italian_text1 = \"Buongiorno! Come stai? Oggi vado in spiaggia.\"\n",
    "clipper.synthesize_speech(italian_text1, \"enhanced.mp3\", italian_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.anki_tools import generate_wiktionary_links_non_english\n",
    "\n",
    "\n",
    "def test_wiktionary_links():\n",
    "    \"\"\"Test function to demonstrate usage.\"\"\"\n",
    "    test_cases = [\n",
    "        (\"goodbye England\", \"uk\"),  # Ukrainian\n",
    "        (\"book reading\", \"sv\"),  # Swedish\n",
    "        (\"coffee shop\", \"ja\"),  # Japanese\n",
    "    ]\n",
    "    \n",
    "    for phrase, lang_code in test_cases:\n",
    "        print(f\"\\nTesting {lang_code} Wiktionary links for: {phrase}\")\n",
    "        result = generate_wiktionary_links_non_english(phrase, lang_code)\n",
    "        print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.target_language_voice_models[\"language_code\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
