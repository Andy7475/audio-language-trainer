{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg path added to system PATH: C:\\Program Files\\ffmpeg-7.0-essentials_build\\bin\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "load_dotenv()\n",
    "# Add the parent directory of 'src' to the Python path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "    \n",
    "from src.config_loader import config, VoiceManager\n",
    "from src.phrase import generate_phrases_with_llm, generate_phrases_from_vocab_dict\n",
    "from src.utils import load_json\n",
    "from src.anki_tools import convert_anki_to_story_dict, AnkiCollectionReader, export_to_anki_with_images\n",
    "from src.utils import load_text_file, save_json, load_json\n",
    "from src.dialogue_generation import get_story_prompt, generate_story\n",
    "from src.config_loader import config, VoiceManager, VoiceInfo, VoiceType, VoiceProvider\n",
    "from pprint import pprint\n",
    "import random\n",
    "import os\n",
    "\n",
    "from langcodes import Language\n",
    "\n",
    "ANKI_PATH = \"C:/Users/i5/AppData/Roaming/Anki2/User 1/collection.anki2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "\"Three white cats on Monday\",\n",
    "\"Seven red books on Tuesday\",\n",
    "\"Twelve silver spoons at bedtime\",\n",
    "\"Twenty gold stars in December\",\n",
    "\"Five brown dogs in January\",\n",
    "\"Eight grey birds in February\",\n",
    "\"Two pink cakes on Wednesday\",\n",
    "\"Nine yellow bananas on Thursday\",\n",
    "\"Sixteen purple grapes for breakfast\",\n",
    "\"Four green apples in March\",\n",
    "\"Thirteen blue pencils in April\",\n",
    "\"One black horse in May\",\n",
    "\"Ten orange fish on Friday\",\n",
    "\"Eighteen yellow flower on Saturday\",\n",
    "\"Six green cups at lunchtime\",\n",
    "\"Eleven red cookies in June\",\n",
    "\"Three red cars in July\",\n",
    "\"Fifteen fluffy clouds in August\",\n",
    "\"Fourteen brown rabbits on Sunday\",\n",
    "\"Nineteen meals at dinnertime\",\n",
    "\"Eight golden croissants for breakfast\",\n",
    "\"Seven red shoes in September\",\n",
    "\"Four brown bears in October\",\n",
    "\"Twenty yellow leaves in November\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_text_file, save_json, load_json\n",
    "from src.nlp import get_vocab_dictionary_from_phrases, get_vocab_dict_from_dialogue, create_flashcard_index, plot_vocabulary_growth\n",
    "from src.config_loader import config\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "filepath = \"../data/stories/test_story/test_phrases.txt\"\n",
    "phrases = load_text_file(filepath)\n",
    "pprint(f\"First few phrases {phrases[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "\n",
    "plot_vocabulary_growth(phrases, window=10)  # Using smaller window for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp import optimize_vocab_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp import analyze_sequence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimized sequence and statistics\n",
    "optimized_df, stats = optimize_vocab_sequence(phrases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrangement = optimized_df['phrase'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vocabulary_growth(new_arrangement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(test_vocab_dict, \"test_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_story = generate_story(test_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(test_story, \"test_story_50_phrases.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_used = get_vocab_dict_from_dialogue(test_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.generate import add_translations\n",
    "\n",
    "\n",
    "test_story = add_translations(test_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.generate import add_audio\n",
    "\n",
    "test_story_audio = add_audio(test_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import create_html_story\n",
    "\n",
    "output_dir = \"../outputs/test/test.html\"\n",
    "create_html_story(\n",
    "            test_story_audio,\n",
    "            output_dir,\n",
    "            component_path=\"../src/StoryViewer.js\",\n",
    "            title=\"test_short_story\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vocab_overlap(test_vocab_dict, vocab_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index once - to retrieve matching flashcards that already exist\n",
    "from src.nlp import create_flashcard_index, get_matching_flashcards_indexed\n",
    "\n",
    "flashcard_index = create_flashcard_index(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(flashcard_index, \"test_flashcard_index.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use indexed version for faster matching\n",
    "results = get_matching_flashcards_indexed(vocab_used, flashcard_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_flashcard_phrases = [card.get('phrase') for card in results['selected_cards']]\n",
    "vocab_from_flashcards = get_vocab_dictionary_from_phrases(proposed_flashcard_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vocab_overlap(vocab_used, vocab_from_flashcards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['remaining_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_phrases = get_matching_flashcards(vocab_used, phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate story from broad Longman vocabulary pool\n",
    "story = generate_story_from_vocab(longman_vocab_dict)\n",
    "\n",
    "2. Extract all vocabulary used in story\n",
    "story_vocab = extract_vocab_from_story(story)\n",
    "\n",
    "3. Compare with Longman dictionary to identify source\n",
    "used_vocab = compare_vocab_overlap(longman_vocab_dict, story_vocab)\n",
    "\n",
    "4. get existing flashcards we already have\n",
    "5. generate new ones\n",
    ". Generate flashcards for:\n",
    "    - All story vocabulary that appears in Longman list\n",
    "    - Common connecting words needed for natural speech\n",
    "    - Tag cards with which story they appear in\n",
    "flashcards = generate_flashcards_for_story(story_vocab, story_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioSegment.silent(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm._load_google_voices(\"fr-FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "locale=\"fr-FR\"\n",
    "language_object = Language.get(locale)\n",
    "speech_key = os.getenv(\"AZURE_API_KEY\")\n",
    "if not speech_key:\n",
    "    print(\"Warning: AZURE_API_KEY not found in environment variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "service_region = os.getenv(\"AZURE_REGION\", \"eastus\")\n",
    "speech_config = speechsdk.SpeechConfig(\n",
    "    subscription=speech_key, region=service_region\n",
    ")\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(\n",
    "    speech_config=speech_config\n",
    ")\n",
    "\n",
    "result = speech_synthesizer.get_voices_async(locale=locale).get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = result.voices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice.local_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = result.voices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice.voice_type._name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for voice in result.voices:\n",
    "    voice_type = (\n",
    "        VoiceType.NEURAL\n",
    "        if voice.voice_type._name_ == \"OnlineNeural\"\n",
    "        else VoiceType.STANDARD\n",
    "    )\n",
    "\n",
    "    voice_info = VoiceInfo(\n",
    "        name=voice.local_name,\n",
    "        provider=VoiceProvider.AZURE,\n",
    "        voice_type=voice_type,\n",
    "        gender=voice.gender._name_.upper(),\n",
    "        language_code=voice.locale,\n",
    "        country_code=language_object.territory,\n",
    "        voice_id=voice.short_name,\n",
    "    )\n",
    "\n",
    "    all.append(voice_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm._load_azure_voices(\"fr-FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "response = client.list_voices(language_code=\"fr-FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.voices[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang.is_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang.territory_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang.language_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.translation import translate_from_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_from_english(\"hello\", \"cmn-TW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config._load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.get_voice_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.voice_manager._load_azure_voices(\"fr-FR\")\n",
    "config.voice_manager.voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.voice_manager._lazy_load_voices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.voice_manager.voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = AnkiCollectionReader(ANKI_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = reader.col.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config._load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_knowledge_score(collection_path: str, deck_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate a knowledge score (0-1) for each card in the deck based on review history.\n",
    "    \n",
    "    Factors considered:\n",
    "    - Current interval (longer intervals suggest better knowledge)\n",
    "    - Ease factor (higher ease suggests better retention)\n",
    "    - Review success rate (ratio of Good/Easy vs Again buttons)\n",
    "    - Time since last review (recent successful reviews weighted more)\n",
    "    - Review time trends (decreasing review times suggest familiarity)\n",
    "    \n",
    "    Args:\n",
    "        collection_path: Path to the .anki2 collection file\n",
    "        deck_name: Name of the deck to analyze\n",
    "    \n",
    "    Returns:\n",
    "        dict: Card IDs mapped to their knowledge scores and contributing factors\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    scores = {}\n",
    "    with AnkiCollectionReader(collection_path) as reader:\n",
    "        # Get deck ID\n",
    "        deck = reader.col.decks.by_name(deck_name)\n",
    "        if not deck:\n",
    "            raise ValueError(f\"Deck '{deck_name}' not found\")\n",
    "            \n",
    "        # Get all cards in deck\n",
    "        card_ids = reader.col.find_cards(f\"did:{deck['id']}\")\n",
    "        \n",
    "        # For each card, analyze its review history\n",
    "        for card_id in card_ids:\n",
    "            card = reader.col.get_card(card_id)\n",
    "            note = reader.col.get_note(card.nid)\n",
    "            \n",
    "            # Get review logs for this card\n",
    "            reviews = reader.col.db.all(\n",
    "                \"SELECT ease, ivl, factor, time, type FROM revlog WHERE cid = ? ORDER BY id\",\n",
    "                card_id\n",
    "            )\n",
    "            \n",
    "            if not reviews:\n",
    "                scores[card_id] = {\n",
    "                    'score': 0,\n",
    "                    'reason': 'No reviews yet',\n",
    "                    'note_fields': dict(note.items())\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            # Calculate component scores\n",
    "            \n",
    "            # 1. Interval score (0-0.4): Longer intervals suggest better knowledge\n",
    "            max_interval = 365  # Cap at 1 year for scoring\n",
    "            current_interval = abs(card.ivl)  # Use absolute value to handle negative intervals\n",
    "            interval_score = min(current_interval / max_interval, 1) * 0.4\n",
    "            \n",
    "            # 2. Ease score (0-0.2): Higher ease factors suggest better retention\n",
    "            min_ease = 1300  # Minimum ease factor\n",
    "            max_ease = 3100  # Maximum ease factor\n",
    "            ease_score = (card.factor - min_ease) / (max_ease - min_ease) * 0.2\n",
    "            ease_score = max(0, min(ease_score, 0.2))  # Clamp between 0-0.2\n",
    "            \n",
    "            # 3. Review success score (0-0.3)\n",
    "            success_count = sum(1 for r in reviews if r[0] >= 3)  # Count Good/Easy\n",
    "            total_reviews = len(reviews)\n",
    "            success_score = (success_count / total_reviews) * 0.3 if total_reviews > 0 else 0\n",
    "            \n",
    "            # 4. Review time trend score (0-0.1)\n",
    "            # Lower and/or decreasing review times suggest familiarity\n",
    "            if len(reviews) >= 3:\n",
    "                recent_times = [r[3] for r in reviews[-3:]]  # Last 3 review times\n",
    "                avg_time = sum(recent_times) / len(recent_times)\n",
    "                time_score = min(1, max(0, (30000 - avg_time) / 30000)) * 0.1  # Scale around 30s\n",
    "            else:\n",
    "                time_score = 0\n",
    "                \n",
    "            # Calculate final score\n",
    "            final_score = interval_score + ease_score + success_score + time_score\n",
    "            \n",
    "            # Store results\n",
    "            scores[card_id] = {\n",
    "                'score': round(final_score, 3),\n",
    "                'components': {\n",
    "                    'interval_score': round(interval_score, 3),\n",
    "                    'ease_score': round(ease_score, 3),\n",
    "                    'success_score': round(success_score, 3),\n",
    "                    'time_score': round(time_score, 3)\n",
    "                },\n",
    "                'stats': {\n",
    "                    'current_interval': current_interval,\n",
    "                    'ease_factor': card.factor,\n",
    "                    'review_success_rate': round(success_count / total_reviews, 2) if total_reviews > 0 else 0,\n",
    "                    'total_reviews': total_reviews\n",
    "                },\n",
    "                'note_fields': dict(note.items())\n",
    "            }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for card_id, data in scores.items():\n",
    "        row = {\n",
    "            'card_id': card_id\n",
    "        }\n",
    "        \n",
    "        # Add note fields\n",
    "        row.update(data['note_fields'])\n",
    "        \n",
    "        # Add component scores\n",
    "        if 'components' in data:\n",
    "            row.update(data['components'])\n",
    "        \n",
    "        # Add statistics\n",
    "        if 'stats' in data:\n",
    "            row.update(data['stats'])\n",
    "        \n",
    "        # Add final score\n",
    "        row['knowledge_score'] = data['score']\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Reorder columns to have card_id first, then content fields, then scores\n",
    "    score_cols = ['interval_score', 'ease_score', 'success_score', 'time_score', 'knowledge_score']\n",
    "    stat_cols = ['current_interval', 'ease_factor', 'review_success_rate', 'total_reviews']\n",
    "    content_cols = [col for col in df.columns if col not in score_cols + stat_cols + ['card_id']]\n",
    "    \n",
    "    df = df[['card_id'] + content_cols + stat_cols + score_cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_knowledge_scores(scores: dict, num_examples: int = 5):\n",
    "    \"\"\"\n",
    "    Print formatted knowledge scores with examples of high and low scoring cards.\n",
    "    \n",
    "    Args:\n",
    "        scores: Dictionary of scores from calculate_knowledge_score()\n",
    "        num_examples: Number of high/low scoring examples to show\n",
    "    \"\"\"\n",
    "    # Calculate overall statistics\n",
    "    all_scores = [s['score'] for s in scores.values()]\n",
    "    avg_score = sum(all_scores) / len(all_scores) if all_scores else 0\n",
    "    \n",
    "    print(f\"Analysis of {len(scores)} cards:\")\n",
    "    print(f\"Average knowledge score: {avg_score:.3f}\")\n",
    "    print(f\"Score distribution:\")\n",
    "    \n",
    "    # Show score distribution\n",
    "    ranges = [(0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]\n",
    "    for low, high in ranges:\n",
    "        count = sum(1 for s in all_scores if low <= s < high)\n",
    "        print(f\"{low:.1f}-{high:.1f}: {count} cards ({count/len(all_scores)*100:.1f}%)\")\n",
    "    \n",
    "    # Show examples of highest and lowest scoring cards\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1]['score'])\n",
    "    \n",
    "    print(f\"\\nLowest {num_examples} scoring cards:\")\n",
    "    for card_id, data in sorted_scores[:num_examples]:\n",
    "        print(f\"\\nCard ID: {card_id}\")\n",
    "        print(f\"Score: {data['score']:.3f}\")\n",
    "        if 'components' in data:\n",
    "            print(\"Score components:\")\n",
    "            for component, value in data['components'].items():\n",
    "                print(f\"  {component}: {value:.3f}\")\n",
    "        print(\"Content:\")\n",
    "        for field, content in data['note_fields'].items():\n",
    "            # Truncate long content for display\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"  {field}: {content_preview}\")\n",
    "    \n",
    "    print(f\"\\nHighest {num_examples} scoring cards:\")\n",
    "    for card_id, data in sorted_scores[-num_examples:]:\n",
    "        print(f\"\\nCard ID: {card_id}\")\n",
    "        print(f\"Score: {data['score']:.3f}\")\n",
    "        if 'components' in data:\n",
    "            print(\"Score components:\")\n",
    "            for component, value in data['components'].items():\n",
    "                print(f\"  {component}: {value:.3f}\")\n",
    "        print(\"Content:\")\n",
    "        for field, content in data['note_fields'].items():\n",
    "            # Truncate long content for display\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"  {field}: {content_preview}\")\n",
    "\n",
    "# Example usage:\n",
    "# scores = calculate_knowledge_score(\"path/to/collection.anki2\", \"My Deck\")\n",
    "# print_knowledge_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_knowledge_score(ANKI_PATH, \"RapidRetention - Swedish - LM1000\")\n",
    "\n",
    "# View summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort by knowledge score\n",
    "df.groupby(\"EnglishText\").agg({\"knowledge_score\" : \"mean\"}).sort_values(by=\"knowledge_score\", ascending=False).head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_phrase = \"4-5 words long, but treat common lexical chunks (I'm going to.., Do you.., Let us.. etc) as a single word\"\n",
    "verbs_per_phrase = \"one verb (but OK for an additional adverb if required)\"\n",
    "gcse_phrases = generate_phrases_from_vocab_dict(vocab_dict=gcse_vocab, max_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcse_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm._lazy_load_voices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_text = \"سلام دنیا\"\n",
    "aud = slow_text_to_speech(persian_text, config_language=\"target\", gender=\"MALE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_remove_within_brackets(\"falling (over)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from src.phrase import generate_phrases_from_vocab_dict\n",
    "\n",
    "@dataclass\n",
    "class ProcessingResult:\n",
    "    pairs: List[Tuple[str, str]]\n",
    "    problem_lines: Dict[int, str]  # line number -> original line content\n",
    "    skipped_lines: Dict[int, str]  # line number -> reason for skipping\n",
    "\n",
    "def process_anki_file(file_path: str) -> ProcessingResult:\n",
    "    \"\"\"\n",
    "    Process tab-separated Anki export data from a file and pair up variations.\n",
    "    If there are more English variations than French, repeat the French term.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the tab-separated text file\n",
    "        \n",
    "    Returns:\n",
    "        ProcessingResult containing:\n",
    "        - processed pairs\n",
    "        - dictionary of problem lines\n",
    "        - dictionary of skipped lines\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    problem_lines = {}\n",
    "    skipped_lines = {}\n",
    "    \n",
    "    # Read and process the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            # Skip empty lines and comments\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                skipped_lines[line_num] = \"Empty line\"\n",
    "                continue\n",
    "            if line.startswith('#'):\n",
    "                skipped_lines[line_num] = \"Comment line\"\n",
    "                continue\n",
    "                \n",
    "            # Split line into columns\n",
    "            columns = line.split('\\t')\n",
    "            if len(columns) < 2:\n",
    "                skipped_lines[line_num] = \"Insufficient columns\"\n",
    "                continue\n",
    "                \n",
    "            french_terms = columns[0].strip()\n",
    "            english_terms = columns[1].strip()\n",
    "            \n",
    "            # Split variations\n",
    "            french_variations = [term.strip() for term in french_terms.split('/')]\n",
    "            english_variations = [term.strip() for term in english_terms.replace('/', ',').split(',')]\n",
    "            \n",
    "            # If there are more English variations than French ones\n",
    "            if len(french_variations) < len(english_variations):\n",
    "                if len(french_variations) == 1:  # If there's only one French term, repeat it\n",
    "                    french_term = french_variations[0]\n",
    "                    # Filter out 'i.e.' from English variations\n",
    "                    english_variations = [eng for eng in english_variations if eng.lower() != 'i.e.']\n",
    "                    # Create pairs with repeated French term\n",
    "                    for eng in english_variations:\n",
    "                        if eng:  # Only add if English term is non-empty\n",
    "                            result.append((french_term, eng))\n",
    "                else:\n",
    "                    # If multiple French terms but still fewer than English, record problem\n",
    "                    problem_lines[line_num] = line\n",
    "                    print(f\"\\nWARNING - Complex mismatch on line {line_num}:\")\n",
    "                    print(f\"French variations ({len(french_variations)}): {french_variations}\")\n",
    "                    print(f\"English variations ({len(english_variations)}): {english_variations}\")\n",
    "            else:\n",
    "                # Normal case where French variations >= English variations\n",
    "                for fr, eng in zip(french_variations, english_variations):\n",
    "                    if fr and eng:  # Only add if both terms are non-empty\n",
    "                        result.append((fr, eng))\n",
    "    \n",
    "    return ProcessingResult(pairs=result, problem_lines=problem_lines, skipped_lines=skipped_lines)\n",
    "\n",
    "\n",
    "processed_data = process_anki_file(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_word_pairs(pairs: List[Tuple[str, str]]) -> Dict[str, Set[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Categorize word pairs into verbs and vocab based on English 'to ' prefix.\n",
    "    \n",
    "    Args:\n",
    "        pairs: List of tuples containing (french_word, english_word)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'verbs' and 'vocab' keys containing sets of (french, english) pairs\n",
    "    \"\"\"\n",
    "    vocab_dict = {\n",
    "        'verbs': set(),\n",
    "        'vocab': set()\n",
    "    }\n",
    "    \n",
    "    for french, english in pairs:\n",
    "        # Clean up any trailing/leading whitespace\n",
    "        french = french.strip()\n",
    "        english = english.strip()\n",
    "        \n",
    "        # Check if it's a verb (starts with 'to ')\n",
    "        if english.lower().startswith('to ') | english.lower().startswith('to be '):\n",
    "            # Remove 'to ' and add to verbs\n",
    "            english_cleaned = english[3:].strip()  # Remove 'to ' prefix\n",
    "            vocab_dict['verbs'].add(english_cleaned)\n",
    "        else:\n",
    "            # Add to vocab\n",
    "            vocab_dict['vocab'].add(english)\n",
    "    \n",
    "    vocab_dict['vocab'] = list(vocab_dict['vocab'])\n",
    "    vocab_dict['verbs'] = list(vocab_dict['verbs'])\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcse_dict = categorize_word_pairs(processed_data.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_json\n",
    "\n",
    "\n",
    "save_json(gcse_dict, \"../outputs/gcse_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_phrases = generate_phrases_from_vocab_dict(gcse_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Configure speech service\n",
    "speech_key = os.getenv(\"AZURE_API_KEY\")\n",
    "service_region = \"eastus\"  # Default region\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# Get available voices\n",
    "result = speech_synthesizer.get_voices_async().get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voice in result.voices:\n",
    "    print(voice._short_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for voice in result.voices:\n",
    "    print(voice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from IPython.display import Audio\n",
    "import io\n",
    "\n",
    "# Configure speech service\n",
    "speech_key = os.getenv(\"AZURE_API_KEY\")\n",
    "service_region = \"eastus\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "\n",
    "# Note: the voice setting will not overwrite the voice element in input SSML.\n",
    "speech_config.speech_synthesis_voice_name = \"en-US-AmberNeural\"\n",
    "\n",
    "text = \"Hello World!\"\n",
    "\n",
    "# use the default speaker as audio output.\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "result = speech_synthesizer.speak_text_async(text).get()\n",
    "# Check result\n",
    "if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesized for text [{}]\".format(text))\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.audio_generation import play_audio\n",
    "\n",
    "display(Audio(result.audio_data, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# #from src.config_loader import config\n",
    "# from src.utils import create_html_story, create_test_story_dict, load_json\n",
    "# from src.audio_generation import text_to_speech\n",
    "# from src.translation import tokenize_text\n",
    "# from src.anki_tools import generate_wiktionary_links\n",
    "from src.nlp import filter_matching_phrases\n",
    "from src.utils import load_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"../data/gcse_vocab_list_cambridge.json\", \"r\") as gcse:\n",
    "    gcse_dict = json.load(gcse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/longman_phrases_convo_1000.txt\", \"r\") as core:\n",
    "    core_phrases = [line.strip(\"\\n\") for line in core.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcse_phrases = filter_matching_phrases(core_phrases, gcse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcse_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict = load_json(\"..\\data\\longman_phrase_images\\phrase_image_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "for key in image_dict:\n",
    "    if isinstance(image_dict.get(key), str):\n",
    "        continue\n",
    "    else:\n",
    "        phrases.append(image_dict[key]['phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "test_phrases = random.sample(phrases, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def merge_json_files(directory_path):\n",
    "    \"\"\"\n",
    "    Merge multiple JSON files containing transformed phrases into a single JSON file,\n",
    "    removing duplicates based on both original and conversational fields.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Merged dictionary containing unique transformed phrases\n",
    "    \"\"\"\n",
    "    # Use a set to track unique phrases (as tuples of original and conversational)\n",
    "    unique_phrases = set()\n",
    "    all_phrases = []\n",
    "    \n",
    "    # Convert directory path to Path object\n",
    "    dir_path = Path(directory_path)\n",
    "    \n",
    "    # Counter for tracking statistics\n",
    "    total_phrases = 0\n",
    "    \n",
    "    # Iterate through all JSON files in the directory\n",
    "    for json_file in dir_path.glob('*.json'):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Extract transformed_phrases from each file\n",
    "                if 'transformed_phrases' in data:\n",
    "                    phrases = data['transformed_phrases']\n",
    "                    total_phrases += len(phrases)\n",
    "                    \n",
    "                    # Process each phrase\n",
    "                    for phrase in phrases:\n",
    "                        # Create tuple of the phrase fields for uniqueness checking\n",
    "                        phrase_tuple = (phrase['original'], phrase['conversational'])\n",
    "                        \n",
    "                        # Only add if we haven't seen this combination before\n",
    "                        if phrase_tuple not in unique_phrases:\n",
    "                            unique_phrases.add(phrase_tuple)\n",
    "                            all_phrases.append(phrase)\n",
    "                else:\n",
    "                    print(f\"Warning: 'transformed_phrases' not found in {json_file}\")\n",
    "                    \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Could not parse JSON from {json_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {str(e)}\")\n",
    "    \n",
    "    # Create merged dictionary\n",
    "    merged_data = {\n",
    "        \"transformed_phrases\": all_phrases\n",
    "    }\n",
    "    \n",
    "    # Save merged data to a new JSON file\n",
    "    output_path = dir_path / 'merged_phrases.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Print statistics\n",
    "    duplicates = total_phrases - len(all_phrases)\n",
    "    print(f\"Processing complete:\")\n",
    "    print(f\"- Total phrases processed: {total_phrases}\")\n",
    "    print(f\"- Unique phrases: {len(all_phrases)}\")\n",
    "    print(f\"- Duplicates removed: {duplicates}\")\n",
    "    print(f\"Output saved to: {output_path}\")\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dat = merge_json_files(\"../outputs/phrase_changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conversational_phrases(transformed_phrases, output_file:str =\"../data/longman_phrases_convo_1000.txt\") -> None:\n",
    "    \"\"\"\n",
    "    Extract and save conversational phrases to a text file.\n",
    "    \n",
    "    Args:\n",
    "        transformed_phrases: List of dictionaries containing 'original' and 'conversational' phrases\n",
    "        output_file: Path to the output text file\n",
    "        \n",
    "    Prints summary of the operation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for phrase_pair in transformed_phrases:\n",
    "                f.write(phrase_pair['conversational'] + '\\n')\n",
    "                \n",
    "        # Count phrases saved\n",
    "        phrase_count = len(transformed_phrases)\n",
    "        print(f\"\\nOperation Summary:\")\n",
    "        print(f\"Successfully saved {phrase_count} phrases to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving phrases to {output_file}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_conversational_phrases(merged_dat[\"transformed_phrases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Dict, List\n",
    "from src.utils import clean_filename\n",
    "\n",
    "def copy_rename_phrase_images(transformed_phrases: List[Dict[str, str]], image_dir: str=\"..\\data\\longman_phrase_images\") -> None:\n",
    "    \"\"\"\n",
    "    Copy and rename image files based on transformed phrases.\n",
    "    \n",
    "    Args:\n",
    "        transformed_phrases: List of dictionaries containing 'original' and 'conversational' phrases\n",
    "        image_dir: Directory containing the image files\n",
    "        \n",
    "    Prints summary of operations and lists any missing original images.\n",
    "    \"\"\"\n",
    "    successful_copies = 0\n",
    "    missing_originals = []\n",
    "    \n",
    "    # Ensure image directory exists\n",
    "    if not os.path.exists(image_dir):\n",
    "        raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
    "        \n",
    "    for phrase_pair in transformed_phrases:\n",
    "        original_phrase = phrase_pair['original']\n",
    "        conversational_phrase = phrase_pair['conversational']\n",
    "        \n",
    "        # Generate filenames\n",
    "        original_filename = clean_filename(original_phrase) + '.png'\n",
    "        new_filename = clean_filename(conversational_phrase) + '.png'\n",
    "        \n",
    "        original_path = os.path.join(image_dir, original_filename)\n",
    "        new_path = os.path.join(image_dir, new_filename)\n",
    "        \n",
    "        # Check if original file exists\n",
    "        if not os.path.exists(original_path):\n",
    "            missing_originals.append(original_phrase)\n",
    "            continue\n",
    "            \n",
    "        # Skip if destination file already exists\n",
    "        if os.path.exists(new_path):\n",
    "            print(f\"Warning: Destination file already exists, skipping: {new_filename}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Copy the file\n",
    "            shutil.copy2(original_path, new_path)\n",
    "            successful_copies += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error copying {original_filename} to {new_filename}: {str(e)}\")\n",
    "            \n",
    "    # Print summary\n",
    "    print(\"\\nOperation Summary:\")\n",
    "    print(f\"Successfully copied and renamed: {successful_copies} images\")\n",
    "    print(f\"Missing original images: {len(missing_originals)}\")\n",
    "    \n",
    "    if missing_originals:\n",
    "        print(\"\\nMissing original images for these phrases:\")\n",
    "        for phrase in missing_originals:\n",
    "            print(f\"- {phrase}\")\n",
    "            \n",
    "    return successful_copies, missing_originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_copies, missing_originals = copy_rename_phrase_images(merged_dat['transformed_phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_phrase_chunk():\n",
    "    for index in range(0,len(phrases), 50):\n",
    "        yield phrases[index:index+50]\n",
    "\n",
    "phrase_iter= next_phrase_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(phrase_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Transform each sentence into a more natural, conversational phrase. Each original phrase has an associated teaching image, so the core situation and meaning MUST remain identical. Your response should be a valid JSON object with this structure:\n",
    "\n",
    "{{\n",
    "  \"transformed_phrases\": [\n",
    "    {{\n",
    "      \"original\": \"<original phrase>\",\n",
    "      \"conversational\": \"<transformed phrase>\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Rules for transformation:\n",
    "\n",
    "1. Keep ALL key vocabulary from the original (nouns, verbs, adjectives) - as these phrases are for learning that vocab\n",
    "\n",
    "2. Maintain the EXACT same situation/scene as the original, since it matches an existing image\n",
    "\n",
    "3. Transform into the most natural way someone would express this idea in conversation, mainly replace phrases that are narrative and passive. Replace with:\n",
    "\n",
    "   - Imperative (\"Don't be late...\")\n",
    "   - Simple statements (\"The traffic was terrible...\")\n",
    "   - First-person expressions (\"I enjoy...\")\n",
    "   - Questions (\"Shall we...?\", \"Do you ...?\", \"Did they...?\" etc)\n",
    "4. Keep the original phrase if a change isn't required or modifying the phrase would result in something unnatural sounding\n",
    "\n",
    "Examples:\n",
    "\n",
    "{{\n",
    "\"original\": \"We watched the sunrise over the mountains\",\n",
    "\"conversational\": \"We watched the sunrise over the mountains\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"She will call you as soon as possible\",\n",
    "\"conversational\": \"She will call you as soon as possible\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"They joined the protest against high taxes\",\n",
    "\"conversational\": \"Did you join the protest against high taxes?\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"He will receive an award for his work\",\n",
    "\"conversational\": \"I'm receiving an award for my work\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"She likes to play with her pet cat\",\n",
    "\"conversational\": \"Do you like playing with your pet cat?\"\n",
    "}},\n",
    "{{\n",
    "\"original\": \"He covered his face with his hands\",\n",
    "\"conversational\": \"Cover your face with your hands\"\n",
    "}},\n",
    "\n",
    "Here are the phrases to transform:\n",
    "\n",
    "{next(phrase_iter)}\n",
    "\n",
    "Output only valid JSON following the structure above.\"\"\"\n",
    "\n",
    "pyperclip.copy(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "pyperclip.copy(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[image_dict.get(key).get('phrase', '') for key in image_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = tokenize_text(\"hello world\", language_code=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = generate_wiktionary_links(\"こんにちは世界\", \"Japanese\", \"ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segment = slow_text_to_speech(\n",
    "        text=\"let's speak slowly and clearly\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.audio_generation import slow_text_to_speech\n",
    "from IPython.display import Audio\n",
    "import io\n",
    "\n",
    "# Test cases with languages, voice names, and potential issues\n",
    "test_phrases = [\n",
    "    # English with apostrophes\n",
    "    (\"I can't believe it!\", \"en-GB\", \"en-GB-Wavenet-D\"),\n",
    "    (\"Don't you'll I'm they're\", \"en-GB\", \"en-GB-Wavenet-D\"),\n",
    "    \n",
    "    # HTML entities that might appear\n",
    "    (\"Let&#39;s go &amp; have fun!\", \"en-US\", \"en-US-Wavenet-D\"),\n",
    "    \n",
    "    # Italian with apostrophes\n",
    "    (\"L'italiano è bellissimo\", \"it-IT\", \"it-IT-Wavenet-A\"),\n",
    "    \n",
    "    # Japanese (no apostrophes but needs tokenization)\n",
    "    (\"私は日本語を勉強しています\", \"ja-JP\", \"ja-JP-Wavenet-B\"),\n",
    "    \n",
    "    # Chinese test\n",
    "    (\"我正在学习中文\", \"zh-CN\", \"cmn-CN-Wavenet-A\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# Test each phrase and play the audio\n",
    "for text, lang_code, voice in test_phrases[0:2]:\n",
    "    print(f\"\\nTesting: {text}\")\n",
    "    print(f\"Language: {lang_code}\")\n",
    "    print(f\"Voice: {voice}\")\n",
    "    \n",
    "    audio_segment = slow_text_to_speech(\n",
    "        text, \n",
    "        language_code=lang_code,\n",
    "        voice_name=voice\n",
    "    )\n",
    "    \n",
    "    # Convert to format playable in notebook\n",
    "    buffer = io.BytesIO()\n",
    "    audio_segment.export(buffer, format=\"wav\")\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Display audio player\n",
    "    display(Audio(buffer.read(), rate=audio_segment.frame_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper = EnhancedSSMLClipper(\n",
    "    word_rate=\"0.85\",    # Very slow\n",
    "    word_pitch=\"-1st\",   # Slightly lower\n",
    "    break_time=\"300ms\",  # Longer breaks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Russian text: \"Hello! How are you? I am learning Russian.\"\n",
    "russian_text = \"Здравствуйте! Как дела? Я изучаю русский язык.\"\n",
    "\n",
    "# Russian neural voice name\n",
    "russian_voice = \"ru-RU-Standard-B\"  # Female voice\n",
    "clipper.synthesize_speech(russian_text, \"enhanced.mp3\", russian_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = text_to_speech(russian_text, \"ru\", russian_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_voice = \"it-IT-Neural2-C\"\n",
    "italian_text1 = \"Buongiorno! Come stai? Oggi vado in spiaggia.\"\n",
    "clipper.synthesize_speech(italian_text1, \"enhanced.mp3\", italian_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.anki_tools import generate_wiktionary_links_non_english\n",
    "\n",
    "\n",
    "def test_wiktionary_links():\n",
    "    \"\"\"Test function to demonstrate usage.\"\"\"\n",
    "    test_cases = [\n",
    "        (\"goodbye England\", \"uk\"),  # Ukrainian\n",
    "        (\"book reading\", \"sv\"),  # Swedish\n",
    "        (\"coffee shop\", \"ja\"),  # Japanese\n",
    "    ]\n",
    "    \n",
    "    for phrase, lang_code in test_cases:\n",
    "        print(f\"\\nTesting {lang_code} Wiktionary links for: {phrase}\")\n",
    "        result = generate_wiktionary_links_non_english(phrase, lang_code)\n",
    "        print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.target_language_voice_models[\"language_code\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
