{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# Add the parent directory of 'src' to the Python path\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.config_loader import config\n",
    "from src.utils import anthropic_generate, extract_json_from_llm_response\n",
    "from src.chat import (\n",
    "    get_challenge_generation_prompt,\n",
    "    create_html_challenges,\n",
    "    get_html_challenge_inputs,\n",
    ")\n",
    "\n",
    "COLLECTION = \"LM2000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gcs_storage import (\n",
    "    get_stories_from_collection,\n",
    "    get_story_dialogue_path,\n",
    "    get_story_challenges_path,\n",
    "    upload_to_gcs,\n",
    "    read_from_gcs,\n",
    ")\n",
    "\n",
    "all_stories = get_stories_from_collection(collection=COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n",
      "Function that called this one: <module>\n",
      "anthropic_generate: service=anthropic, model=None, max_tokens=5000, tools=no\n"
     ]
    }
   ],
   "source": [
    "for story_name in all_stories:\n",
    "    story_dialogue = read_from_gcs(\n",
    "        bucket_name=config.GCS_PRIVATE_BUCKET,\n",
    "        file_path=get_story_dialogue_path(story_name=story_name, collection=COLLECTION),\n",
    "    )\n",
    "    prompt = get_challenge_generation_prompt(story_dialogue)\n",
    "    scenario_dicts = extract_json_from_llm_response(\n",
    "        anthropic_generate(prompt, max_tokens=5000)\n",
    "    )\n",
    "\n",
    "    upload_to_gcs(\n",
    "        obj=scenario_dicts,\n",
    "        bucket_name=config.GCS_PRIVATE_BUCKET,\n",
    "        file_name=get_story_challenges_path(\n",
    "            story_name=story_name, collection=COLLECTION\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If scenarios already exist start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_dicts = read_from_gcs(\n",
    "    bucket_name=config.GCS_PRIVATE_BUCKET,\n",
    "    file_path=get_story_challenges_path(\n",
    "        story_name=\"story_midnight_garden_mystery\", collection=\"LM1000\"\n",
    "    ),\n",
    ")\n",
    "challenges = get_html_challenge_inputs(\n",
    "    scenario_dicts\n",
    ")  # this is the first time config.TARGET_LANGUAGE_NAME is used\n",
    "chat_webpage_file = create_html_challenges(\n",
    "    challenges, story_name=\"story_midnight_garden_mystery\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_gcs(\n",
    "    file_path=chat_webpage_file,\n",
    "    bucket_prefix=f\"{config.TARGET_LANGUAGE_NAME}/{STORY_NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
