{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory of 'src' to the Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longman phrase and anki deck generation\n",
    "A more memorable way to learn the core vocabulary as defined in Longman Communications vocab lists, we take the vocab and use an LLM\n",
    "to generate phrases using it.\n",
    "\n",
    "## Longman 1000, 2000 and 3000 already provided\n",
    "Enlish phrases for the longman vocab have already been created and can be found in the 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_longman_verb_vocab_dict\n",
    "from src.phrase import generate_phrases_from_vocab_dict\n",
    "\n",
    "file_path = '../data/longman-communication-3000.json'\n",
    "vocab_dict = get_longman_verb_vocab_dict(file_path, \"S3\") #S1 = 1st 1000 words used in Speech, options are S1-3 and W1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/15\n",
      "Generated 101 phrases\n",
      "We have 127 verbs and 728 vocab words left\n",
      "Iteration 2/15\n",
      "Generated 106 phrases\n",
      "We have 58 verbs and 660 vocab words left\n",
      "Iteration 3/15\n",
      "Generated 105 phrases\n",
      "We have 3 verbs and 594 vocab words left\n",
      "Iteration 4/15\n",
      "Generated 98 phrases\n",
      "We have 195 verbs and 520 vocab words left\n",
      "Iteration 5/15\n",
      "Generated 105 phrases\n",
      "We have 112 verbs and 434 vocab words left\n",
      "Iteration 6/15\n",
      "Generated 103 phrases\n",
      "We have 43 verbs and 344 vocab words left\n",
      "Iteration 7/15\n",
      "Generated 87 phrases\n",
      "We have 2 verbs and 307 vocab words left\n",
      "Iteration 8/15\n",
      "Generated 99 phrases\n",
      "We have 250 verbs and 272 vocab words left\n",
      "Iteration 9/15\n",
      "Generated 101 phrases\n",
      "We have 173 verbs and 232 vocab words left\n",
      "Iteration 10/15\n",
      "Generated 109 phrases\n",
      "We have 95 verbs and 161 vocab words left\n",
      "Iteration 11/15\n",
      "Generated 100 phrases\n",
      "We have 37 verbs and 136 vocab words left\n",
      "Iteration 12/15\n",
      "Generated 60 phrases - with minimal phrase prompt\n",
      "We have 30 verbs and 25 vocab words left\n",
      "Iteration 13/15\n",
      "Generated 7 phrases - with minimal phrase prompt\n",
      "We have 29 verbs and 0 vocab words left\n",
      "All words have been used. Phrase generation complete.\n"
     ]
    }
   ],
   "source": [
    "#uses LLM calls - it sometimes generates phrases terminated too early (e.g. Mind the pot on the), so advise you scan through and check\n",
    "\n",
    "#english phrases only initially\n",
    "longman_phrases = generate_phrases_from_vocab_dict(vocab_dict, max_iterations=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../outputs/longman/longman_3000_phrases.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for phrase in longman_phrases:\n",
    "        f.write(phrase + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Anki deck from thoses phrases\n",
    "\n",
    "Or, load one of the english Longman phrase lists in data/ already created and apply to your language\n",
    "\n",
    "This function:\n",
    "1. translates\n",
    "2. generates audio using text to speech\n",
    "3. packages up the text and audio into several anki decks (in batches), that can be imported into Anki.\n",
    "\n",
    "The deck_name will is used to derive the deck_id and so despite there being several *.apkg files created, these will all merge successfully into the same deck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning translation for anki\n",
      "Translated phrases\n",
      "\n",
      "Text-to-speech for phrases done\n",
      "\n",
      "Anki deck exported to ../outputs/longman\\longman_3000_0_anki_deck.apkg\n",
      "Cleanup of temporary MP3 files completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.anki import create_anki_deck_from_english_phrase_list\n",
    "\n",
    "_ = await create_anki_deck_from_english_phrase_list(longman_phrases[:3], deck_name=\"Longman 3000 - Swedish\", anki_filename_prefix=\"longman_3000_swedish\", batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
