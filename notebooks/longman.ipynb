{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory of 'src' to the Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longman phrase and anki deck generation\n",
    "A more memorable way to learn the core vocabulary as defined in Longman Communications vocab lists, we take the vocab and use an LLM\n",
    "to generate phrases using it.\n",
    "\n",
    "## Longman 1000, 2000 and 3000 already provided\n",
    "Enlish phrases for the longman vocab have already been created and can be found in the 'data' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_longman_verb_vocab_dict\n",
    "from src.phrase import generate_phrases_from_vocab_dict\n",
    "\n",
    "file_path = '../data/longman-communication-3000.json'\n",
    "vocab_dict = get_longman_verb_vocab_dict(file_path, \"S3\") #S1 = 1st 1000 words used in Speech, options are S1-3 and W1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses LLM calls - it sometimes generates phrases terminated too early (e.g. Mind the pot on the), so advise you scan through and check\n",
    "\n",
    "#english phrases only initially\n",
    "longman_phrases = generate_phrases_from_vocab_dict(vocab_dict, max_iterations=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../outputs/longman/longman_3000_phrases.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for phrase in longman_phrases:\n",
    "        f.write(phrase + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Anki deck from thoses phrases\n",
    "\n",
    "Or, load one of the english Longman phrase lists in data/ already created and apply to your language\n",
    "\n",
    "This function:\n",
    "1. translates\n",
    "2. generates audio using text to speech\n",
    "3. packages up the text and audio into several anki decks (in batches), that can be imported into Anki.\n",
    "\n",
    "The deck_name will is used to derive the deck_id and so despite there being several *.apkg files created, these will all merge successfully into the same deck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.anki import create_anki_deck_from_english_phrase_list\n",
    "\n",
    "_ = await create_anki_deck_from_english_phrase_list(longman_phrases[:3], deck_name=\"Longman 3000 - Swedish\", anki_filename_prefix=\"longman_3000_swedish\", batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import create_image_generation_prompt\n",
    "\n",
    "\n",
    "prompt = create_image_generation_prompt(longman_phrases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longman_phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd6b994d969468199a40824c0c88c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"TORCH_HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face cache directory: Y:/ai_caches/torch\n"
     ]
    }
   ],
   "source": [
    "torch.hub.set_dir(\"Y:/ai_caches/torch\")\n",
    "\n",
    "print(f\"Hugging Face cache directory: {torch.hub.get_dir()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if using torch < 2.0\n",
    "# pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "prompt = \"An exhausted engineer on a couch with a half-eaten pizza nearby. Next to the couch is a broken cooker with its parts scattered around\"\n",
    "\n",
    "images = pipe(prompt=prompt).images[0]\n",
    "images.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, EulerDiscreteScheduler\n",
    "\n",
    "# Use a smaller, faster model\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"  # Smaller than XL\n",
    "\n",
    "# Load the pipeline with optimizations\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "# Move to GPU and enable memory efficient attention\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.enable_attention_slicing()\n",
    "\n",
    "# Use the Euler scheduler, which is faster than the default\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# Optional: Enable torch.compile() for potential speedup (requires PyTorch 2.0 or later)\n",
    "# Uncomment the next line if you're using PyTorch 2.0+\n",
    "# pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate an image\n",
    "prompt = \"\"\"An exhausted engineer on a couch with a half-eaten pizza nearby. Next to the couch is a broken cooker with its parts scattered around. Hand-painted appearance, soft pastel colors, \n",
    "simple shapes, whimsical details. \"\"\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=50,  # Reduced from default 50\n",
    "    guidance_scale=7,        # Slightly reduced from default 7.5\n",
    "    height=512,              # Reduced from default 1024 for SDXL\n",
    "    width=512,               # Reduced from default 1024 for SDXL\n",
    ").images[0]\n",
    "\n",
    "# Save the image\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00440fcd257428e83dd31391326f6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c59229d41274b268165eb145dd6f3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  19%|#9        | 94.4M/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295a5552ede1488ab0a15bc81f18acab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   3%|2         | 94.4M/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc3b7ed38f045dba7fae9cac99d5630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:  34%|###4      | 115M/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df48a6f214e43439ea2ccd48183677f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   8%|7         | 94.4M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "y:\\Python Scripts\\audio-language-trainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\i5\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "def benchmark_model(model_id, num_runs=5):\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "    prompt = \"A bride on a balcony looking at a sunset.\"\n",
    "    \n",
    "    total_time = 0\n",
    "    for run in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        image = pipe(prompt, num_inference_steps=30, height=512, width=512).images[0]\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        total_time += inference_time\n",
    "\n",
    "        # Save the image\n",
    "        image_filename = f\"{model_id.split('/')[-1]}_{run+1}.png\"\n",
    "        image.save(image_filename)\n",
    "        \n",
    "        print(f\"Run {run+1}:\")\n",
    "        print(f\"  Model: {model_id}\")\n",
    "        print(f\"  Inference time: {inference_time:.2f} seconds\")\n",
    "        print(f\"  Image saved as: {image_filename}\")\n",
    "        print()\n",
    "\n",
    "    avg_time = total_time / num_runs\n",
    "    print(f\"{model_id}: Average inference time: {avg_time:.2f} seconds\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "# List of models to benchmark\n",
    "models = [\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    \"stabilityai/stable-diffusion-2-1\",\n",
    "    \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    benchmark_model(model, num_runs=1)\n",
    "\n",
    "print(\"Benchmark complete. Check the current directory for the generated images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate an image\n",
    "prompt = \"An exhausted engineer on a couch with a half-eaten pizza nearby. Next to the couch is a broken cooker with its parts scattered around\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
