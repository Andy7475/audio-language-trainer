{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "PAY_FOR_API = True #change to True to run cells that cost money via API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Card Generation 03\n",
    "\n",
    "## Generate flash cards\n",
    "\n",
    "The english phrases (01 notebook) and images (02 notebook) can now be re-used on whatever language you want.\n",
    "\n",
    "The translation and audio generation gets done at the same time as exporting to our unique RapidRetain flash card format.\n",
    "\n",
    "_IMPORTANT_\n",
    "\n",
    "If you are learning more than one language, to prevent memory interference, you should use a different set of images with each language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load phrases\n",
    "\n",
    "I've already generated some phrases using the longman corpus from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg path added to system PATH: C:\\Program Files\\ffmpeg-7.0-essentials_build\\bin\n",
      "(\"First few phrases ['Do you want to become a famous writer?', 'Let me show \"\n",
      " \"you around the city', 'We need to handle this situation carefully', 'Stop \"\n",
      " 'wasting time on this\\', \\'Do you like playing the guitar at night?\\', \"I\\'m '\n",
      " 'taking a vacation next month\", \"Don\\'t forget to wear a helmet while '\n",
      " 'cycling\", \"Let\\'s cut unnecessary expenses this year\", \"We\\'re producing a '\n",
      " 'new product soon\", \\'Did you remember to turn off the stove?\\']')\n"
     ]
    }
   ],
   "source": [
    "from src.anki_tools import create_anki_deck_from_english_phrase_list, export_to_anki_with_images\n",
    "from src.utils import load_text_file, save_json, load_json\n",
    "from src.config_loader import config\n",
    "from pprint import pprint\n",
    "\n",
    "filepath = \"../data/longman_1000_phrases.txt\"\n",
    "phrases = load_text_file(filepath)\n",
    "pprint(f\"First few phrases {phrases[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the flash cards\n",
    "\n",
    "The main function (under the hood) is export_to_anki_with_images()\n",
    "An earlier version of the code created flashcards without images (export_to_anki)\n",
    "\n",
    "Assuming you ran notebook 02 against your phrases, then this next step is a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file has been modified. Reloading...\n",
      "['Are they hoping to graduate with honors next year?',\n",
      " 'We just moved to a new house last month',\n",
      " \"I wonder what's for dinner tonight\",\n",
      " 'We really need to cut back on spending this month',\n",
      " \"Would you eat the fish if it's fresh?\",\n",
      " \"Look how the sun's clearing away the morning fog\",\n",
      " 'How do you maintain work-life balance?',\n",
      " 'Remember to speak softly in the library',\n",
      " 'Have you heard the old building is closed for renovation?',\n",
      " 'Shall we drink our coffee on the ground?']\n",
      "Voice models selected : (VoiceInfo(name='en-GB-Studio-B', provider=<VoiceProvider.GOOGLE: 'google'>, voice_type=<VoiceType.STUDIO: 'studio'>, gender='MALE', language_code='en-GB', country_code='GB', voice_id='en-GB-Studio-B'), VoiceInfo(name='دلارا', provider=<VoiceProvider.AZURE: 'azure'>, voice_type=<VoiceType.NEURAL: 'neural'>, gender='FEMALE', language_code='fa-IR', country_code='IR', voice_id='fa-IR-DilaraNeural'), VoiceInfo(name='فرید', provider=<VoiceProvider.AZURE: 'azure'>, voice_type=<VoiceType.NEURAL: 'neural'>, gender='MALE', language_code='fa-IR', country_code='IR', voice_id='fa-IR-FaridNeural'))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "config._load_config() #worth doing if you are switching between languages etc\n",
    "image_dir = \"../data/longman_phrase_images/longman1000\"\n",
    "anki_output_dir = f\"../outputs/flashcards/{config.TARGET_LANGUAGE_NAME.lower()}\"\n",
    "deck_name = f\"RapidRetention - {config.TARGET_LANGUAGE_NAME} - LM1000\" #this is used to genearte the Deck ID in Anki\n",
    "anki_filename_prefix = f\"longman_1000_{config.TARGET_LANGUAGE_NAME.lower()}\"\n",
    "sample_phrases = random.sample(phrases, k=10)\n",
    "pprint(sample_phrases)\n",
    "print(\"Voice models selected :\", config.get_voice_models())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg path added to system PATH: C:\\Program Files\\ffmpeg-7.0-essentials_build\\bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding translations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning translation for anki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding translations: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated phrases\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating audio:   0%|          | 0/2 [00:02<?, ?it/s]\n",
      "adding audio:   0%|          | 0/1 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Speech synthesis failed: CancellationReason.Error (Connection was closed by the remote host. Error code: 1007. Error details: Ssml should only contain one language. USP state: TurnStarted. Received audio size: 0 bytes.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PAY_FOR_API:\n\u001b[1;32m----> 2\u001b[0m   anki_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_anki_deck_from_english_phrase_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_phrases\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mdeck_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeck_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43manki_filename_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manki_filename_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#saves in batches of 50 notes per apkg file - useful for very large decks to split up\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manki_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#this is where our images are stored\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\anki_tools.py:540\u001b[0m, in \u001b[0;36mcreate_anki_deck_from_english_phrase_list\u001b[1;34m(phrase_list, deck_name, anki_filename_prefix, batch_size, output_dir, image_dir)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m from_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(phrase_list), batch_size):\n\u001b[0;32m    534\u001b[0m     partial_dict \u001b[38;5;241m=\u001b[39m create_test_story_dict(\n\u001b[0;32m    535\u001b[0m         translated_phrases_dict,\n\u001b[0;32m    536\u001b[0m         story_parts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    537\u001b[0m         phrases\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    538\u001b[0m         from_index\u001b[38;5;241m=\u001b[39mfrom_index,\n\u001b[0;32m    539\u001b[0m     )\n\u001b[1;32m--> 540\u001b[0m     translated_phrases_dict_audio \u001b[38;5;241m=\u001b[39m \u001b[43madd_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_dir:\n\u001b[0;32m    543\u001b[0m         translated_phrases_dict_audio \u001b[38;5;241m=\u001b[39m add_image_paths(\n\u001b[0;32m    544\u001b[0m             translated_phrases_dict_audio, image_dir\n\u001b[0;32m    545\u001b[0m         )\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\generate.py:192\u001b[0m, in \u001b[0;36madd_audio\u001b[1;34m(story_data_dict)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# now do phrases asynchronoulsy (still unsure if Google API allows this, not getting huge speed up)\u001b[39;00m\n\u001b[0;32m    191\u001b[0m translated_phrases \u001b[38;5;241m=\u001b[39m story_data_dict[story_part][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslated_phrase_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 192\u001b[0m translated_phrases_audio \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_translated_phrase_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslated_phrases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m story_data_dict[story_part][\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslated_phrase_list_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m ] \u001b[38;5;241m=\u001b[39m translated_phrases_audio\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText-to-speech for phrases done\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\audio_generation.py:59\u001b[0m, in \u001b[0;36mgenerate_translated_phrase_audio\u001b[1;34m(translated_phrases)\u001b[0m\n\u001b[0;32m     51\u001b[0m english_audio \u001b[38;5;241m=\u001b[39m text_to_speech(\n\u001b[0;32m     52\u001b[0m     text\u001b[38;5;241m=\u001b[39mcleaned_eng,\n\u001b[0;32m     53\u001b[0m     config_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     gender\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMALE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m     speaking_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Generate slow target language audio with word breaks\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m target_slow \u001b[38;5;241m=\u001b[39m \u001b[43mslow_text_to_speech\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFEMALE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaking_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSPEAKING_RATE_SLOW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_break_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWORD_BREAK_MS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Generate normal target language audio\u001b[39;00m\n\u001b[0;32m     68\u001b[0m target_normal \u001b[38;5;241m=\u001b[39m text_to_speech(\n\u001b[0;32m     69\u001b[0m     text\u001b[38;5;241m=\u001b[39mcleaned_target,\n\u001b[0;32m     70\u001b[0m     config_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m     gender\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFEMALE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     72\u001b[0m     speaking_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m     73\u001b[0m )\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\audio_generation.py:389\u001b[0m, in \u001b[0;36mslow_text_to_speech\u001b[1;34m(text, config_language, gender, speaking_rate, word_break_ms)\u001b[0m\n\u001b[0;32m    386\u001b[0m ssml_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ssml_parts)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# Use the main text_to_speech function with SSML\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext_to_speech\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssml_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaking_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaking_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ssml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\audio_generation.py:337\u001b[0m, in \u001b[0;36mtext_to_speech\u001b[1;34m(text, config_language, gender, speaking_rate, is_ssml)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_to_speech_google(text, voice_model, speaking_rate, is_ssml)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m voice_model\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;241m==\u001b[39m VoiceProvider\u001b[38;5;241m.\u001b[39mAZURE:\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext_to_speech_azure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoice_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaking_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_ssml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported voice provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvoice_model\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\audio_generation.py:295\u001b[0m, in \u001b[0;36mtext_to_speech_azure\u001b[1;34m(text, voice_model, speaking_rate, is_ssml)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_mp3(audio_buffer)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeech synthesis failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mcancellation_details\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mcancellation_details\u001b[38;5;241m.\u001b[39merror_details\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m     )\n",
      "\u001b[1;31mException\u001b[0m: Speech synthesis failed: CancellationReason.Error (Connection was closed by the remote host. Error code: 1007. Error details: Ssml should only contain one language. USP state: TurnStarted. Received audio size: 0 bytes.)"
     ]
    }
   ],
   "source": [
    "\n",
    "if PAY_FOR_API:\n",
    "  anki_data = create_anki_deck_from_english_phrase_list(phrase_list=sample_phrases[:2],\n",
    "                                                      deck_name = deck_name,\n",
    "                                                      anki_filename_prefix=anki_filename_prefix,\n",
    "                                                      batch_size=50, #saves in batches of 50 notes per apkg file - useful for very large decks to split up\n",
    "                                                      output_dir=anki_output_dir,\n",
    "                                                      image_dir=image_dir #this is where our images are stored\n",
    "                                                    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing / Development\n",
    "The anki_data is a dictionary which contains all translations, and audio, you can re-use this to save on API costs if iterating over flashcard design / testing, or if you want to save the data, and create the flashcards later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing story parts: 100%|██████████| 1/1 [00:00<00:00, 125.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.images import add_image_paths\n",
    "\n",
    "anki_data = add_image_paths(anki_data, image_dir=image_dir) #adds the image directory filepaths to the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating image and sound files: 10it [00:17,  1.76s/it]\n",
      "adding notes to deck: 100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anki deck exported to ../outputs/flashcards/french\\longman_1000_french_anki_deck.apkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deleting temp files: 100%|██████████| 30/30 [00:00<00:00, 3748.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup of temporary files completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "export_to_anki_with_images(anki_data,\n",
    "                           output_dir= anki_output_dir,\n",
    "                           story_name=anki_filename_prefix,\n",
    "                           deck_name=deck_name )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
