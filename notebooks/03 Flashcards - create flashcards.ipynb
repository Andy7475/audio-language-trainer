{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "PAY_FOR_API = True #change to True to run cells that cost money via API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Card Generation 03\n",
    "\n",
    "## Generate flash cards\n",
    "\n",
    "The english phrases (01 notebook) and images (02 notebook) can now be re-used on whatever language you want.\n",
    "\n",
    "The translation and audio generation gets done at the same time as exporting to our unique RapidRetain flash card format.\n",
    "\n",
    "_IMPORTANT_\n",
    "\n",
    "If you are learning more than one language, to prevent memory interference, you should use a different set of images with each language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load phrases\n",
    "\n",
    "I've already generated some phrases using the longman corpus from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting voice override: it-IT-Wavenet-E\n",
      "setting voice override: it-IT-Wavenet-F\n",
      "FFmpeg path added to system PATH: C:\\Program Files\\ffmpeg-7.0-essentials_build\\bin\n",
      "(\"First few phrases ['Three white cats on Monday', 'Seven red books on \"\n",
      " \"Tuesday', 'Twelve silver spoons at bedtime', 'Twenty gold stars in \"\n",
      " \"December', 'Five brown dogs in January', 'Eight grey birds in February', \"\n",
      " \"'Two pink cakes on Wednesday', 'Nine yellow bananas on Thursday', 'Sixteen \"\n",
      " \"purple grapes for breakfast', 'Four green apples in March']\")\n"
     ]
    }
   ],
   "source": [
    "from src.anki_tools import create_anki_deck_from_english_phrase_list, export_to_anki_with_images\n",
    "from src.utils import load_text_file, save_json, load_json\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "filepath = \"../data/numbers_days.txt\"\n",
    "phrases = load_text_file(filepath)\n",
    "pprint(f\"First few phrases {phrases[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the flash cards\n",
    "\n",
    "The main function (under the hood) is export_to_anki_with_images()\n",
    "An earlier version of the code created flashcards without images (export_to_anki)\n",
    "\n",
    "Assuming you ran notebook 02 against your phrases, then this next step is a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting voice override: it-IT-Wavenet-E\n",
      "setting voice override: it-IT-Wavenet-F\n",
      "setting voice override: it-IT-Wavenet-E\n",
      "setting voice override: it-IT-Wavenet-F\n",
      "['Three white cats on Monday',\n",
      " 'Seven red books on Tuesday',\n",
      " 'Twelve silver spoons at bedtime',\n",
      " 'Twenty gold stars in December',\n",
      " 'Five brown dogs in January',\n",
      " 'Eight grey birds in February',\n",
      " 'Two pink cakes on Wednesday',\n",
      " 'Nine yellow bananas on Thursday',\n",
      " 'Sixteen purple grapes for breakfast',\n",
      " 'Four green apples in March']\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "VoiceInfo(name='en-GB-Studio-B', provider=<VoiceProvider.GOOGLE: 'google'>, voice_type=<VoiceType.STUDIO: 'studio'>, gender='MALE', language_code='en-GB', country_code='GB', voice_id='en-GB-Studio-B')\n",
      "VoiceInfo(name='it-IT-Wavenet-E', provider=<VoiceProvider.GOOGLE: 'google'>, voice_type=<VoiceType.WAVENET: 'wavenet'>, gender='FEMALE', language_code='it-IT', country_code='IT', voice_id='it-IT-Wavenet-E')\n",
      "VoiceInfo(name='it-IT-Wavenet-F', provider=<VoiceProvider.GOOGLE: 'google'>, voice_type=<VoiceType.WAVENET: 'wavenet'>, gender='MALE', language_code='it-IT', country_code='IT', voice_id='it-IT-Wavenet-F')\n",
      "Italian\n",
      "RapidRetention - Italian - NumbersDays\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from src.config_loader import config\n",
    "\n",
    "config._load_config() #worth doing if you are switching between languages etc\n",
    "image_dir = \"../outputs/images/numbers_days\"\n",
    "assert os.path.exists(image_dir)\n",
    "anki_output_dir = f\"../outputs/flashcards/{config.TARGET_LANGUAGE_NAME.lower()}\"\n",
    "deck_name = f\"RapidRetention - {config.TARGET_LANGUAGE_NAME} - NumbersDays\" #this is used to genearte the Deck ID in Anki\n",
    "anki_filename_prefix = f\"longman_numberDays_{config.TARGET_LANGUAGE_NAME.lower()}\"\n",
    "pprint(phrases[:10])\n",
    "\n",
    "#may need to override voices to suppport speaking rate adjustments, do this\n",
    "# in the config.json file by setting the relevent voice ID:\n",
    "# \"TARGET_LANGUAGE_FEMALE_VOICE\": \"it-IT-WaveNet-E\", etc or null if you don't want to override\n",
    "\n",
    "voice_models = config.get_voice_models()\n",
    "for vm in voice_models:\n",
    "    print(vm)\n",
    "print(config.TARGET_LANGUAGE_NAME)\n",
    "print(deck_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(VoiceInfo(name='en-GB-Studio-B', provider=<VoiceProvider.GOOGLE: 'google'>, voice_type=<VoiceType.STUDIO: 'studio'>, gender='MALE', language_code='en-GB', country_code='GB', voice_id='en-GB-Studio-B'),\n",
       " VoiceInfo(name='it-IT-Wavenet-E', provider=<VoiceProvider.GOOGLE: 'google'>, voice_type=<VoiceType.WAVENET: 'wavenet'>, gender='FEMALE', language_code='it-IT', country_code='IT', voice_id='it-IT-Wavenet-E'),\n",
       " VoiceInfo(name='it-IT-Wavenet-F', provider=<VoiceProvider.GOOGLE: 'google'>, voice_type=<VoiceType.WAVENET: 'wavenet'>, gender='MALE', language_code='it-IT', country_code='IT', voice_id='it-IT-Wavenet-F'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get_voice_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding translations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning translation for anki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding translations: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated phrases\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding audio:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n",
      "Using override voice: it-IT-Wavenet-E\n",
      "Using override voice: it-IT-Wavenet-F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating audio:  62%|██████▎   | 15/24 [04:48<02:52, 19.22s/it]\n",
      "adding audio:   0%|          | 0/1 [04:48<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PAY_FOR_API:\n\u001b[1;32m----> 2\u001b[0m   anki_data \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_anki_deck_from_english_phrase_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphrases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mdeck_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeck_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43manki_filename_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manki_filename_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#saves in batches of 50 notes per apkg file - useful for very large decks to split up\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manki_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#this is where our images are stored\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\anki_tools.py:541\u001b[0m, in \u001b[0;36mcreate_anki_deck_from_english_phrase_list\u001b[1;34m(phrase_list, deck_name, anki_filename_prefix, batch_size, output_dir, image_dir)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m from_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(phrase_list), batch_size):\n\u001b[0;32m    535\u001b[0m     partial_dict \u001b[38;5;241m=\u001b[39m create_test_story_dict(\n\u001b[0;32m    536\u001b[0m         translated_phrases_dict,\n\u001b[0;32m    537\u001b[0m         story_parts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    538\u001b[0m         phrases\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    539\u001b[0m         from_index\u001b[38;5;241m=\u001b[39mfrom_index,\n\u001b[0;32m    540\u001b[0m     )\n\u001b[1;32m--> 541\u001b[0m     translated_phrases_dict_audio \u001b[38;5;241m=\u001b[39m \u001b[43madd_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_dir:\n\u001b[0;32m    544\u001b[0m         translated_phrases_dict_audio \u001b[38;5;241m=\u001b[39m add_image_paths(\n\u001b[0;32m    545\u001b[0m             translated_phrases_dict_audio, image_dir\n\u001b[0;32m    546\u001b[0m         )\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\generate.py:193\u001b[0m, in \u001b[0;36madd_audio\u001b[1;34m(story_data_dict, source_language_audio)\u001b[0m\n\u001b[0;32m    191\u001b[0m translated_phrases \u001b[38;5;241m=\u001b[39m story_data_dict[story_part]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslated_phrase_list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m translated_phrases:\n\u001b[1;32m--> 193\u001b[0m     translated_phrases_audio \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_translated_phrase_audio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtranslated_phrases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_language_audio\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     story_data_dict[story_part][\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslated_phrase_list_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m     ] \u001b[38;5;241m=\u001b[39m translated_phrases_audio\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText-to-speech for phrases done\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\audio_generation.py:62\u001b[0m, in \u001b[0;36mgenerate_translated_phrase_audio\u001b[1;34m(translated_phrases, source_language_audio)\u001b[0m\n\u001b[0;32m     59\u001b[0m     english_audio \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39msilent(\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Generate slow target language audio with word breaks\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m target_slow \u001b[38;5;241m=\u001b[39m \u001b[43mslow_text_to_speech\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFEMALE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaking_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSPEAKING_RATE_SLOW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword_break_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWORD_BREAK_MS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Generate normal target language audio\u001b[39;00m\n\u001b[0;32m     71\u001b[0m target_normal \u001b[38;5;241m=\u001b[39m text_to_speech(\n\u001b[0;32m     72\u001b[0m     text\u001b[38;5;241m=\u001b[39mcleaned_target,\n\u001b[0;32m     73\u001b[0m     config_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     74\u001b[0m     gender\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFEMALE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     75\u001b[0m     speaking_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m     76\u001b[0m )\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\audio_generation.py:411\u001b[0m, in \u001b[0;36mslow_text_to_speech\u001b[1;34m(text, config_language, gender, speaking_rate, word_break_ms)\u001b[0m\n\u001b[0;32m    408\u001b[0m ssml_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ssml_parts)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Use the main text_to_speech function with SSML\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext_to_speech\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssml_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_language\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeaking_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeaking_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ssml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\audio_generation.py:346\u001b[0m, in \u001b[0;36mtext_to_speech\u001b[1;34m(text, config_language, gender, speaking_rate, is_ssml)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03mWrapper that handles diveriting to Azure or Google depending on the settings in the config file\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03mfor language, which then cause the voice models to be either Azure or Google ones.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m    AudioSegment containing the generated speech\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# Use config values if parameters are not provided\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m voice_models \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_voice_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    349\u001b[0m     voice_model \u001b[38;5;241m=\u001b[39m voice_models[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\config_loader.py:396\u001b[0m, in \u001b[0;36mget_voice_models\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\config_loader.py:196\u001b[0m, in \u001b[0;36mget_voice\u001b[1;34m(self, language_code, gender)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_voice\u001b[39m(\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    185\u001b[0m     language_code: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    186\u001b[0m     gender: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFEMALE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    187\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[VoiceInfo]:\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get best available voice with fallback options.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    Will first check for override voices set via set_voice_override().\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    If no override found, will select best voice based on ranking.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m        language_code (str): Language code in format \"fr-FR\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m        gender (str): \"MALE\" or \"FEMALE\"\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m        VoiceInfo object or None if no voice found\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_voices(language_code)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# Check for override first\u001b[39;00m\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\config_loader.py:93\u001b[0m, in \u001b[0;36m_lazy_load_voices\u001b[1;34m(self, language_code)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\src\\config_loader.py:153\u001b[0m, in \u001b[0;36m_load_azure_voices\u001b[1;34m(self, locale)\u001b[0m\n\u001b[0;32m    149\u001b[0m service_region \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_REGION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meastus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m speech_config \u001b[38;5;241m=\u001b[39m speechsdk\u001b[38;5;241m.\u001b[39mSpeechConfig(\n\u001b[0;32m    151\u001b[0m     subscription\u001b[38;5;241m=\u001b[39mspeech_key, region\u001b[38;5;241m=\u001b[39mservice_region\n\u001b[0;32m    152\u001b[0m )\n\u001b[1;32m--> 153\u001b[0m speech_synthesizer \u001b[38;5;241m=\u001b[39m speechsdk\u001b[38;5;241m.\u001b[39mSpeechSynthesizer(\n\u001b[0;32m    154\u001b[0m     speech_config\u001b[38;5;241m=\u001b[39mspeech_config\n\u001b[0;32m    155\u001b[0m )\n\u001b[0;32m    157\u001b[0m result \u001b[38;5;241m=\u001b[39m speech_synthesizer\u001b[38;5;241m.\u001b[39mget_voices_async(locale\u001b[38;5;241m=\u001b[39mlocale)\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m voice \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mvoices:\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\.venv\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:576\u001b[0m, in \u001b[0;36mResultFuture.get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03mWaits until the result is available, and returns it.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__resolved:\n\u001b[1;32m--> 576\u001b[0m     result_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped_type(result_handle)\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\.venv\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:2481\u001b[0m, in \u001b[0;36mSpeechSynthesizer.get_voices_async.<locals>.resolve_future\u001b[1;34m(handle)\u001b[0m\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve_future\u001b[39m(handle: _spx_handle):\n\u001b[0;32m   2480\u001b[0m     result_handle \u001b[38;5;241m=\u001b[39m _spx_handle(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 2481\u001b[0m     \u001b[43m_call_hr_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_sdk_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesizer_get_voices_list_async_wait_for\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_uint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_handle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2482\u001b[0m     _sdk_lib\u001b[38;5;241m.\u001b[39msynthesizer_async_handle_release(handle)\n\u001b[0;32m   2483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_handle\n",
      "File \u001b[1;32my:\\Python Scripts\\audio-language-trainer\\.venv\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\interop.py:61\u001b[0m, in \u001b[0;36m_call_hr_fn\u001b[1;34m(fn, *args)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_hr_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, fn):\n\u001b[0;32m     60\u001b[0m     fn\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m _spx_hr\n\u001b[1;32m---> 61\u001b[0m     hr \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m fn()\n\u001b[0;32m     62\u001b[0m     _raise_if_failed(hr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if PAY_FOR_API:\n",
    "  anki_data = create_anki_deck_from_english_phrase_list(phrase_list=phrases,\n",
    "                                                      deck_name = deck_name,\n",
    "                                                      anki_filename_prefix=anki_filename_prefix,\n",
    "                                                      batch_size=50, #saves in batches of 50 notes per apkg file - useful for very large decks to split up\n",
    "                                                      output_dir=anki_output_dir,\n",
    "                                                      image_dir=image_dir #this is where our images are stored\n",
    "                                                    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing / Development\n",
    "The anki_data is a dictionary which contains all translations, and audio, you can re-use this to save on API costs if iterating over flashcard design / testing, or if you want to save the data, and create the flashcards later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.images import add_image_paths\n",
    "\n",
    "anki_data = add_image_paths(anki_data, image_dir=image_dir) #adds the image directory filepaths to the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anki_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_to_anki_with_images(anki_data,\n",
    "                           output_dir= anki_output_dir,\n",
    "                           story_name=anki_filename_prefix,\n",
    "                           deck_name=deck_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
