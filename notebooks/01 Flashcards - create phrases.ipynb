{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth import default\n",
    "credentials, project = default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.utils import load_json, save_text_file, load_text_file  # noqa: E402\n",
    "from src.config_loader import config\n",
    "src_dir = Path().absolute().parent\n",
    "phrase_dir = src_dir / \"data\" / \"phrases\"\n",
    "PAY_FOR_API = True #change to True to run cells that cost money via API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Card Generation 01\n",
    "\n",
    "## Generate english phrases\n",
    "\n",
    "The core way we store vocabularly for generating phrases, and then flashcards, is in a dictionary with two keys. 'verbs' (for verbs in the infinitive form, like 'be', 'run') and 'vocab' (for everything else).\n",
    "\n",
    "The intent is that a vocab list is a core learning requirement (e.g. for an exam), and that it is easier to remember words in the context of common phrases. i.e. learning the phrase 'I want', and separtely learning the noun 'cake' is less efficient than learning the phrase 'I want some cake, please'.\n",
    "\n",
    "Even better if we link that phrase to an image and associated audio. This is the dual-encoding theory of langauge learning and leads to retention and recall benefits.\n",
    "\n",
    "The first step is generating your english phrases from your vocab list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longman corpus\n",
    "\n",
    "A common 'starter' corpus containing core words you should learn in terms of the 1st 1000 words, 2nd 1000 words etc\n",
    "\n",
    "You can replace vocab_dict with any custom made python dictionary with 'verbs' and 'vocab' keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_longman_verb_vocab_dict, save_text_file, load_text_file\n",
    "from src.phrase import generate_phrases_from_vocab_dict, generate_scenario_phrases, generate_scenario_vocab_building_phrases\n",
    "\n",
    "file_path = '../data/longman-communication-3000.json' # a specifc format\n",
    "vocab_dict = get_longman_verb_vocab_dict(file_path, \"S2\") #S1 = 1st 1000 words used in Speech, W2 = 2nd 1000 words used in written etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" first 10 verbs: {vocab_dict['verbs'][:10]}, \\nand first 10 other words: {vocab_dict['vocab'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating conversational phrases from a vocabulary dictionary\n",
    "\n",
    "This function will iterate through (by sampling) the vocabularly dictionary, until it is exhausted.\n",
    "We run a check against generated phrases so we can 'tick off' words already used.\n",
    "\n",
    "Phrases are generated using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or for GCSE vocab:\n",
    "vocab_dict = load_json(\"..\\data\\gcse_vocab_list_cambridge.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PAY_FOR_API:\n",
    "    #comment out the below two lines to go for the default of 6 - 9 word phrases and no more than 2 verbs\n",
    "    length_phrase = \"4-5 words long, for beginner GCSE level, but treat common lexical chunks (I'm going to.., Do you.., Let us.. etc) as a single 'word'\"\n",
    "    verbs_per_phrase = \"one verb (but OK for an additional auxillary verb if necessary)\"\n",
    "    localise = False # whether to tweak the prompt to set phrases within the target country\n",
    "    generated_phrases = generate_phrases_from_vocab_dict(   \n",
    "        vocab_dict, max_iterations=1,\n",
    "         length_phrase=length_phrase,\n",
    "           verbs_per_phrase=verbs_per_phrase,\n",
    "           localise=localise)\n",
    "    #It takes about 15 iterations to go through 200 verbs, 800 vocab (1000 words total)\n",
    "    #You will end up with about 1000 phrases, so get practice of the same verb etc in different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an existing phrase list if you want\n",
    "generated_phrases = load_text_file(phrase_dir / \"longman_1000_phrases.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimise the phrase ordering\n",
    "from src.nlp import optimise_phrase_list, plot_vocabulary_growth\n",
    "\n",
    "ordered_phrase_list = optimise_phrase_list(generated_phrases, window_size=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vocabulary_growth(ordered_phrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text_file(lines=generated_phrases, file_path=phrase_dir / \"eating_out.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload phrases and index to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gcs_storage import upload_to_gcs, get_phrase_path, get_phrase_index_path\n",
    "    #comment out the below two lines to go for the default of 6 - 9 word phrases and no more than 2 verbs\n",
    "upload_to_gcs(obj=generated_phrases, bucket_name=config.GCS_PRIVATE_BUCKET, file_name = get_phrase_path(collection=\"LM1000\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing phrases...: 100%|██████████| 841/841 [17:31<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.nlp import create_flashcard_index\n",
    "\n",
    "LM1000_index = create_flashcard_index(generated_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://audio-language-trainer-private-content/collections/LM1000/index.json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_to_gcs(obj=LM1000_index, bucket_name=config.GCS_PRIVATE_BUCKET, file_name = get_phrase_index_path(collection=\"LM1000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove redundant phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.phrase import remove_phrases_with_no_new_words\n",
    "old_list = load_text_file(phrase_dir / \"longman_1000_phrases.txt\")\n",
    "new_list = load_text_file(phrase_dir / \"longman_2000_phrases.txt\")\n",
    "new_list_2 = remove_phrases_with_no_new_words(known_phrases=old_list, new_phrases=new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text_file(new_list_2, phrase_dir / \"longman_2000_phrases.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text_file(line=ordered_phrase_list, file_path = phrase_dir / \"gcse_phrases.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vocabulary_growth(new_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate phrases and vocab for a scenario\n",
    "Use an LLM to come up with typical phrases for a scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"meeting new swedish people - language learning community - talking about sweden - hiking, wild swimming, nature\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "speaking_phrases = generate_scenario_phrases(scenario, num_phrases=\"20 - 25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_text_file(lines=speaking_phrases, file_path=phrase_dir / \"swedish_language_learning.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk out this scenarios with some vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_phrases = generate_scenario_vocab_building_phrases(scenario=scenario)\n",
    "save_text_file(lines=vocab_phrases, file_path=phrase_dir / \"swedish_lanuage_learning_vocab2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
