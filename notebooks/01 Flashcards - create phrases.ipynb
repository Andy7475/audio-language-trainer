{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth import default\n",
    "credentials, project = default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Swedish'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.utils import load_json, save_text_file, load_text_file\n",
    "from src.nlp import get_vocab_dictionary_from_phrases\n",
    "from src.utils import get_longman_verb_vocab_dict, save_text_file, load_text_file\n",
    "from src.phrase import generate_phrases_from_vocab_dict, generate_scenario_phrases, generate_scenario_vocab_building_phrases\n",
    "from src.config_loader import config\n",
    "from src.gcs_storage import get_phrase_index_path\n",
    "config.TARGET_LANGUAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Card Generation 01\n",
    "\n",
    "## Generate english phrases\n",
    "\n",
    "The core way we store vocabularly for generating phrases, and then flashcards, is in a dictionary with two keys. 'verbs' (for verbs in the infinitive form, like 'be', 'run') and 'vocab' (for everything else).\n",
    "\n",
    "The intent is that a vocab list is a core learning requirement (e.g. for an exam), and that it is easier to remember words in the context of common phrases. i.e. learning the phrase 'I want', and separtely learning the noun 'cake' is less efficient than learning the phrase 'I want some cake, please'.\n",
    "\n",
    "Even better if we link that phrase to an image and associated audio. This is the dual-encoding theory of langauge learning and leads to retention and recall benefits.\n",
    "\n",
    "The first step is generating your english phrases from your vocab list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longman corpus\n",
    "\n",
    "A common 'starter' corpus containing core words you should learn in terms of the 1st 1000 words, 2nd 1000 words etc\n",
    "\n",
    "You can replace vocab_dict with any custom made python dictionary with 'verbs' and 'vocab' keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gcs_storage import get_phrase_path, read_from_gcs, get_story_collection_path, upload_to_gcs\n",
    "\n",
    "story_collection = read_from_gcs(config.GCS_PRIVATE_BUCKET, get_story_collection_path(collection=\"LM1000\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first 200 phrases\n",
    "all_phrases = []\n",
    "for story_name in story_collection:\n",
    "    all_phrases.extend([item['phrase'] for item in story_collection[story_name]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m all_vocab_dict = \u001b[43mget_vocab_dictionary_from_phrases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_phrases\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyl\\Python\\audio-language-trainer\\src\\nlp.py:155\u001b[39m, in \u001b[36mget_vocab_dictionary_from_phrases\u001b[39m\u001b[34m(english_phrases)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_vocab_dictionary_from_phrases\u001b[39m(\n\u001b[32m    145\u001b[39m     english_phrases: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    146\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m    147\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Processes the english phrases to extract a vocabulary dictionary with keys\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    'verbs' and 'vocab'. This is so we can, for a given chunk of phrases we are learning (in\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[33;03m    flash cards), extract the vocab, and then re-use that vocab to create a story to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m \u001b[33;03m    Returns: vocab_dict: {'verbs' : ['try', 'care', ...], 'vocab' : ['really', 'hello', ...]}\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     vocab_pos_tuples = \u001b[43mextract_vocab_and_pos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43menglish_phrases\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [('trying', 'VERB'), etc]\u001b[39;00m\n\u001b[32m    158\u001b[39m     vocab_dict = get_verb_and_vocab_lists(vocab_pos_tuples)\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vocab_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyl\\Python\\audio-language-trainer\\src\\nlp.py:171\u001b[39m, in \u001b[36mextract_vocab_and_pos\u001b[39m\u001b[34m(english_phrases)\u001b[39m\n\u001b[32m    168\u001b[39m excluded_names = {\u001b[33m\"\u001b[39m\u001b[33msam\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33malex\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m english_phrases:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    175\u001b[39m             token.pos_ != \u001b[33m\"\u001b[39m\u001b[33mPUNCT\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    176\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m token.ent_type_ != \u001b[33m\"\u001b[39m\u001b[33mPERSON\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    177\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m token.text.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m excluded_names\n\u001b[32m    178\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyl\\Python\\audio-language-trainer\\.venv\\Lib\\site-packages\\spacy\\language.py:1052\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1050\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1054\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_vocab_dict = get_vocab_dictionary_from_phrases(all_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vocab_dict['verbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get warmUp vocab dict\n",
    "first200_vocab_dict = get_vocab_dictionary_from_phrases(all_phrases[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first200_vocab_dict['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_path = '../data/longman-communication-3000.json' # a specifc format\n",
    "vocab_dict = get_longman_verb_vocab_dict(file_path, \"S2\") #S1 = 1st 1000 words used in Speech, W2 = 2nd 1000 words used in written etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating conversational phrases from a vocabulary dictionary\n",
    "\n",
    "This function will iterate through (by sampling) the vocabularly dictionary, until it is exhausted.\n",
    "We run a check against generated phrases so we can 'tick off' words already used.\n",
    "\n",
    "Phrases are generated using an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or for GCSE vocab:\n",
    "vocab_dict = load_json(\"..\\data\\gcse_vocab_list_cambridge.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/12 - Generating 100 phrases\n",
      "Function that called this one: generate_phrases_with_llm. Sleeping for 20 seconds\n",
      "Generated 99 phrases\n",
      "We have 78 verbs and 214 vocab words left\n",
      "Iteration 2/12 - Generating 100 phrases\n",
      "Function that called this one: generate_phrases_with_llm. Sleeping for 20 seconds\n",
      "Generated 101 phrases\n",
      "We have 6 verbs and 129 vocab words left\n",
      "Iteration 3/12 - Using minimal phrase generation\n",
      "Function that called this one: generate_minimal_phrases_with_llm. Sleeping for 20 seconds\n",
      "Generated 50 phrases - with minimal phrase prompt\n",
      "We have 1 verbs and 5 vocab words left\n",
      "Iteration 4/12 - Using minimal phrase generation\n",
      "Function that called this one: generate_minimal_phrases_with_llm. Sleeping for 20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for API cooldown: 100%|\u001b[34m██████████████\u001b[0m| 2/2 [00:02<00:00,  1.01s/it]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 phrases - with minimal phrase prompt\n",
      "We have 0 verbs and 0 vocab words left\n",
      "All words have been used. Phrase generation complete. Generated 253 phrases.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#comment out the below two lines to go for the default of 6 - 9 word phrases and no more than 2 verbs\n",
    "length_phrase = \"4-5 words long, for beginner GCSE level, but treat common lexical chunks (I'm going to.., Do you.., Let us.. etc) as a single 'word'\"\n",
    "verbs_per_phrase = \"one verb (but OK for an additional auxillary verb if necessary)\"\n",
    "localise = False # whether to tweak the prompt to set phrases within the target country\n",
    "generated_phrases = generate_phrases_from_vocab_dict(   \n",
    "    first200_vocab_dict, max_iterations=12,\n",
    "      length_phrase=length_phrase,\n",
    "        verbs_per_phrase=verbs_per_phrase,\n",
    "        localise=localise)\n",
    "#It takes about 15 iterations to go through 200 verbs, 800 vocab (1000 words total)\n",
    "#You will end up with about 1000 phrases, so get practice of the same verb etc in different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_phrases = load_text_file('../data/phrases/longman_2000_phrases.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://audio-language-trainer-private-content/collections/LM2000/phrases.json'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLLECTION = \"LM2000\"\n",
    "upload_to_gcs(obj=generated_phrases, bucket_name = config.GCS_PRIVATE_BUCKET, file_name = get_phrase_path(collection = COLLECTION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_phrases = read_from_gcs(config.GCS_PRIVATE_BUCKET,\n",
    "                                  file_path=get_phrase_path(collection=COLLECTION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(generated_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing phrases...: 100%|██████████| 249/249 [06:36<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.nlp import create_flashcard_index\n",
    "\n",
    "WarmUp150_Index = create_flashcard_index(generated_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://audio-language-trainer-private-content/collections/WarmUp150/index.json'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_to_gcs(obj=WarmUp150_Index, bucket_name=config.GCS_PRIVATE_BUCKET, file_name = get_phrase_index_path(collection=COLLECTION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove redundant phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.phrase import remove_phrases_with_no_new_words\n",
    "old_list = load_text_file(phrase_dir / \"longman_1000_phrases.txt\")\n",
    "new_list = load_text_file(phrase_dir / \"longman_2000_phrases.txt\")\n",
    "new_list_2 = remove_phrases_with_no_new_words(known_phrases=old_list, new_phrases=new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text_file(new_list_2, phrase_dir / \"longman_2000_phrases.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text_file(line=ordered_phrase_list, file_path = phrase_dir / \"gcse_phrases.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate phrases and vocab for a scenario\n",
    "Use an LLM to come up with typical phrases for a scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"meeting new swedish people - language learning community - talking about sweden - hiking, wild swimming, nature\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "speaking_phrases = generate_scenario_phrases(scenario, num_phrases=\"20 - 25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_text_file(lines=speaking_phrases, file_path=phrase_dir / \"swedish_language_learning.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk out this scenarios with some vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_phrases = generate_scenario_vocab_building_phrases(scenario=scenario)\n",
    "save_text_file(lines=vocab_phrases, file_path=phrase_dir / \"swedish_lanuage_learning_vocab2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
